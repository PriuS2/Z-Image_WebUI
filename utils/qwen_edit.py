"""Qwen-Image-Edit íŒŒì´í”„ë¼ì¸ ê´€ë¦¬

ovedrive/Qwen-Image-Edit-2511-4bit ëª¨ë¸ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ í¸ì§‘
- 1~3ì¥ì˜ ì´ë¯¸ì§€ ì…ë ¥ ì§€ì›
- 4bit NF4 ì–‘ìí™” (ë³„ë„ ì–‘ìí™” ì˜µì…˜ ë¶ˆí•„ìš”)
- true_cfg_scale íŒŒë¼ë¯¸í„° ì§€ì›
"""

import gc
import asyncio
from typing import Optional, Tuple, Callable, Any, List

import torch
from PIL import Image

from config.defaults import (
    QWEN_EDIT_MODEL,
    DEFAULT_QWEN_EDIT_SETTINGS,
    DEFAULT_GPU_SETTINGS,
)
from utils.gpu_monitor import gpu_monitor


class QwenEditManager:
    """Qwen-Image-Edit ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.pipe = None
        self.current_model: Optional[str] = None
        self.device: Optional[str] = None
        self.cpu_offload_enabled: bool = False
        self._lock = asyncio.Lock()
        self._original_progress_bar = None
    
    @property
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€"""
        return self.pipe is not None
    
    def get_device(self, target_device: str = "auto") -> str:
        """
        ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ë°˜í™˜
        
        Args:
            target_device: ëª©í‘œ ë””ë°”ì´ìŠ¤ ("auto", "cuda:0", "cuda:1", "cpu", "mps")
        
        Returns:
            ì‹¤ì œ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        return gpu_monitor.resolve_device(target_device, prefer_empty=True)
    
    def _get_cuda_index_from_device(self, device: str) -> int:
        """'cuda', 'cuda:0' í˜•íƒœì—ì„œ index ì¶”ì¶œ (ê¸°ë³¸ 0)"""
        if not device or not device.startswith("cuda"):
            return 0
        if ":" not in device:
            return 0
        try:
            return int(device.split(":", 1)[1])
        except Exception:
            return 0

    def _get_preferred_dtype(self, device: Optional[str]) -> torch.dtype:
        """
        GPU/í”Œë«í¼ì— ë§ëŠ” dtype ì„ íƒ
        - Ampere(8.x)+: bf16 ìš°ì„ 
        - ê·¸ ì™¸ CUDA: fp16
        - CPU: fp32
        """
        if device and device.startswith("cuda") and torch.cuda.is_available():
            idx = self._get_cuda_index_from_device(device)
            try:
                major, _minor = torch.cuda.get_device_capability(idx)
            except Exception:
                major = 0
            return torch.bfloat16 if major >= 8 else torch.float16
        return torch.float32
    
    async def load_model(
        self,
        cpu_offload: bool = True,
        model_path: Optional[str] = None,
        target_device: str = "auto",
        progress_callback: Optional[Callable[[int, str, str], Any]] = None
    ) -> Tuple[bool, str]:
        """
        Qwen-Image-Edit ëª¨ë¸ ë¡œë“œ
        
        Args:
            cpu_offload: CPU ì˜¤í”„ë¡œë”© ì‚¬ìš© ì—¬ë¶€ (VRAM ì ˆì•½)
            model_path: ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ
            target_device: ëª©í‘œ ë””ë°”ì´ìŠ¤ ("auto", "cuda:0", "cuda:1", "cpu", "mps")
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (percent, label, detail)
        
        Returns:
            (success, message)
        """
        async with self._lock:
            if self.pipe is not None:
                return False, "ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ì–¸ë¡œë“œí•˜ì„¸ìš”."
            
            try:
                self.device = self.get_device(target_device)
                preferred_dtype = self._get_preferred_dtype(self.device)
                
                # ì§„í–‰ ìƒí™© ì½œë°±
                def report_progress(percent: int, label: str, detail: str = ""):
                    if progress_callback:
                        if asyncio.iscoroutinefunction(progress_callback):
                            asyncio.create_task(progress_callback(percent, label, detail))
                        else:
                            asyncio.create_task(
                                asyncio.to_thread(progress_callback, percent, label, detail)
                            )
                
                report_progress(5, "ğŸ”§ Qwen-Image-Edit ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...", f"ë””ë°”ì´ìŠ¤: {self.device}, ì–‘ìí™”: NF4 (4bit)")
                
                # diffusersì—ì„œ íŒŒì´í”„ë¼ì¸ ì„í¬íŠ¸
                from diffusers import QwenImageEditPlusPipeline
                
                checkpoint_dir = model_path if model_path else QWEN_EDIT_MODEL
                
                report_progress(10, "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"ì €ì¥ì†Œ: {checkpoint_dir}")
                
                # íŒŒì´í”„ë¼ì¸ ë¡œë“œ (4bit ì–‘ìí™” ëª¨ë¸)
                report_progress(30, "ğŸ”„ íŒŒì´í”„ë¼ì¸ ë¡œë”© ì¤‘...", "ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¡œë“œ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                
                self.pipe = await asyncio.to_thread(
                    QwenImageEditPlusPipeline.from_pretrained,
                    checkpoint_dir,
                    torch_dtype=preferred_dtype,
                )
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                report_progress(80, f"ğŸš€ {self.device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...", "")
                
                if cpu_offload:
                    await asyncio.to_thread(self.pipe.enable_model_cpu_offload)
                    report_progress(95, "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì •ë¨", "VRAM ë¶€ì¡± ì‹œ RAM ì‚¬ìš©")
                    self.cpu_offload_enabled = True
                else:
                    await asyncio.to_thread(self.pipe.to, self.device)
                    self.cpu_offload_enabled = False
                
                # progress bar ì„¤ì •
                self.pipe.set_progress_bar_config(disable=None)
                
                self.current_model = QWEN_EDIT_MODEL
                
                # ì›ë³¸ progress_bar ë©”ì„œë“œ ì €ì¥ (í›„í‚¹ ë³µì›ìš©)
                self._original_progress_bar = self.pipe.progress_bar.__func__
                
                # GPU ëª¨ë‹ˆí„°ì— ëª¨ë¸ ë“±ë¡
                gpu_monitor.register_model("Qwen-Image-Edit", self.device)
                
                report_progress(100, "âœ… Qwen-Image-Edit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!", self._get_vram_info())
                
                return True, f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_dir} (NF4 4bit)"
                
            except ImportError as e:
                error_msg = str(e)
                if "QwenImageEditPlusPipeline" in error_msg:
                    return False, "diffusers ìµœì‹  ë²„ì „ì´ í•„ìš”í•©ë‹ˆë‹¤. 'pip install git+https://github.com/huggingface/diffusers'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
                return False, f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            except Exception as e:
                self._cleanup()
                return False, f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    async def unload_model(self) -> Tuple[bool, str]:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        async with self._lock:
            if self.pipe is None:
                return True, "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            try:
                self._cleanup()
                return True, "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"
            except Exception as e:
                return False, f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def _cleanup(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        # GPU ëª¨ë‹ˆí„°ì—ì„œ ëª¨ë¸ ë“±ë¡ í•´ì œ
        gpu_monitor.unregister_model("Qwen-Image-Edit")
        
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
        
        self.current_model = None
        self.cpu_offload_enabled = False
        self._original_progress_bar = None
        
        # GPU ìºì‹œ ì •ë¦¬
        gpu_monitor.clear_cache(self.device)
        gc.collect()
    
    def _get_vram_info(self) -> str:
        """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
        return gpu_monitor.get_vram_summary()
    
    def _hook_progress_bar(self, step_callback):
        """íŒŒì´í”„ë¼ì¸ì˜ progress_barë¥¼ í›„í‚¹í•˜ì—¬ ìŠ¤í…ë³„ ì½œë°± í˜¸ì¶œ"""
        pipe = self.pipe
        
        if self._original_progress_bar is None:
            print("[ê²½ê³ ] ì›ë³¸ progress_barê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ, í˜„ì¬ ë©”ì„œë“œ ì‚¬ìš©")
            original_progress_bar = pipe.progress_bar.__func__
        else:
            original_progress_bar = self._original_progress_bar
        
        def hooked_progress_bar(self_pipe, *args, **kwargs):
            pbar = original_progress_bar(self_pipe, *args, **kwargs)
            
            original_update = pbar.update
            
            def hooked_update(n=1):
                result = original_update(n)
                if step_callback and pbar.total:
                    step_callback(pbar.n, pbar.total)
                return result
            
            pbar.update = hooked_update
            return pbar
        
        import types
        pipe.progress_bar = types.MethodType(hooked_progress_bar, pipe)
    
    def _restore_progress_bar(self):
        """ì›ë˜ progress_bar ë³µì›"""
        import types
        if self.pipe and self._original_progress_bar:
            self.pipe.progress_bar = types.MethodType(self._original_progress_bar, self.pipe)
    
    async def edit_image(
        self,
        images: List[Image.Image],
        prompt: str,
        negative_prompt: str = " ",
        num_inference_steps: int = 20,
        true_cfg_scale: float = 4.0,
        guidance_scale: float = 1.0,
        seed: int = -1,
        num_images: int = 1,
        progress_callback: Optional[Callable[[int, int, int, int], Any]] = None,
        status_callback: Optional[Callable[[str], Any]] = None
    ) -> Tuple[bool, list, str]:
        """
        ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰
        
        Args:
            images: í¸ì§‘í•  ì›ë³¸ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ (1~3ì¥)
            prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_inference_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜ (ê¸°ë³¸ 20)
            true_cfg_scale: True CFG ìŠ¤ì¼€ì¼ (ê¸°ë³¸ 4.0, í”„ë¡¬í”„íŠ¸ ì¶©ì‹¤ë„)
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ (ê¸°ë³¸ 1.0)
            seed: ì‹œë“œ (-1ì´ë©´ ëœë¤)
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (current_image, total_images, current_step, total_steps)
            status_callback: ìƒíƒœ ë©”ì‹œì§€ ì½œë°± (message)
        
        Returns:
            (success, images, message)
        """
        if self.pipe is None:
            return False, [], "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        if not images or len(images) == 0:
            return False, [], "ìµœì†Œ 1ì¥ì˜ ì´ë¯¸ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        if len(images) > 3:
            return False, [], "ìµœëŒ€ 3ì¥ì˜ ì´ë¯¸ì§€ë§Œ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        try:
            import random
            
            # í¸ì§‘ ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            
            # RGBë¡œ ë³€í™˜
            processed_images = []
            for img in images:
                if img.mode != "RGB":
                    img = img.convert("RGB")
                processed_images.append(img)
            
            # ì‹œë“œ ì„¤ì •
            if seed == -1:
                seed = random.randint(0, 2147483647)
            
            # ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ìº¡ì²˜
            main_loop = asyncio.get_running_loop()
            
            results = []
            for i in range(num_images):
                current_seed = seed + i
                generator = torch.Generator("cpu").manual_seed(current_seed)
                
                # ìŠ¤í…ë³„ ì½œë°± í•¨ìˆ˜ ìƒì„±
                current_image_idx = i
                total_images = num_images
                
                def create_step_callback(img_idx, total_imgs):
                    def step_callback(current_step, total_steps):
                        if progress_callback:
                            asyncio.run_coroutine_threadsafe(
                                progress_callback(img_idx + 1, total_imgs, current_step, total_steps),
                                main_loop
                            )
                    return step_callback
                
                step_cb = create_step_callback(current_image_idx, total_images)
                
                # progress_bar í›„í‚¹
                self._hook_progress_bar(step_cb)
                
                try:
                    def run_edit():
                        return self.pipe(
                            image=processed_images,
                            prompt=prompt,
                            negative_prompt=negative_prompt,
                            num_inference_steps=num_inference_steps,
                            true_cfg_scale=true_cfg_scale,
                            guidance_scale=guidance_scale,
                            generator=generator,
                            num_images_per_prompt=1,
                        ).images[0]
                    
                    result_image = await asyncio.to_thread(run_edit)
                finally:
                    self._restore_progress_bar()
                
                results.append({
                    "image": result_image,
                    "seed": current_seed
                })
            
            # í¸ì§‘ ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            
            return True, results, f"í¸ì§‘ ì™„ë£Œ (ì‹œë“œ: {seed})"
            
        except Exception as e:
            self._restore_progress_bar()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            return False, [], f"í¸ì§‘ ì‹¤íŒ¨: {str(e)}"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
qwen_edit_manager = QwenEditManager()
