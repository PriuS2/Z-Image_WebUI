"""LongCat-Image-Edit íŒŒì´í”„ë¼ì¸ ê´€ë¦¬"""

import gc
import asyncio
from typing import Optional, Tuple, Callable, Any

import torch
from PIL import Image

from config.defaults import (
    LONGCAT_EDIT_MODEL,
    EDIT_QUANTIZATION_OPTIONS,
    DEFAULT_EDIT_SETTINGS,
    DEFAULT_GPU_SETTINGS,
)
from utils.llm_client import llm_client
from utils.gpu_monitor import gpu_monitor


# ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í¸ì§‘ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ ìš”ì†Œë§Œ ì¶”ì¶œ)
REFERENCE_IMAGE_ANALYSIS_TEMPLATE = """You are an expert at analyzing images for AI image editing tasks.

The user wants to edit an image with this instruction: "{edit_prompt}"

Your task: Look at the reference image and extract ONLY the specific elements mentioned or implied in the edit instruction.

Rules:
1. Focus ONLY on elements relevant to the edit instruction
2. If the instruction mentions "flower pot" or "plant", describe ONLY the pot/plant from the reference image
3. If the instruction mentions "style" or "atmosphere", describe ONLY the artistic style
4. If the instruction mentions a specific object, describe ONLY that object
5. Do NOT describe the entire image - be selective and focused
6. Keep description brief (max 50 words) - just the essential visual details

Examples:
- Edit instruction: "Make them hold this flower pot" â†’ Describe only the pot (color, material, plant type)
- Edit instruction: "Apply this painting style" â†’ Describe only the art style (brushwork, colors, technique)
- Edit instruction: "Add this hat to the person" â†’ Describe only the hat (shape, color, material)

Output ONLY a brief, focused description of the relevant element(s). No explanation or preamble.
Output in English."""


class LongCatEditManager:
    """LongCat-Image-Edit ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.pipe = None
        self.transformer = None
        self.text_processor = None
        self.text_encoder = None  # ì–‘ìí™”ëœ text_encoder ë³„ë„ ê´€ë¦¬
        self.current_model: Optional[str] = None
        self.current_quantization: Optional[str] = None  # í˜„ì¬ ì–‘ìí™” íƒ€ì…
        self.device: Optional[str] = None
        self.cpu_offload_enabled: bool = False  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™” ì—¬ë¶€
        self._lock = asyncio.Lock()
        self._original_progress_bar = None  # ì›ë³¸ progress_bar ë©”ì„œë“œ ì €ì¥
    
    @property
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€"""
        return self.pipe is not None
    
    def get_device(self, target_device: str = "auto") -> str:
        """
        ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ë°˜í™˜
        
        Args:
            target_device: ëª©í‘œ ë””ë°”ì´ìŠ¤ ("auto", "cuda:0", "cuda:1", "cpu", "mps")
        
        Returns:
            ì‹¤ì œ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        return gpu_monitor.resolve_device(target_device, prefer_empty=True)
    
    def _check_bitsandbytes_available(self) -> Tuple[bool, str]:
        """bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import bitsandbytes as bnb
            return True, ""
        except ImportError:
            return False, "bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install bitsandbytes'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
    
    def _get_quantization_config(self, quantization_type: str):
        """ì–‘ìí™” ì„¤ì • ë°˜í™˜"""
        if quantization_type == "int8":
            from transformers import BitsAndBytesConfig
            return BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_enable_fp32_cpu_offload=True,
            )
        elif quantization_type == "int4":
            from transformers import BitsAndBytesConfig
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
        return None
    
    async def load_model(
        self,
        quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)",
        cpu_offload: bool = True,
        model_path: Optional[str] = None,
        target_device: str = "auto",
        progress_callback: Optional[Callable[[int, str, str], Any]] = None
    ) -> Tuple[bool, str]:
        """
        LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ
        
        Args:
            quantization: ì–‘ìí™” ì˜µì…˜ (BF16, INT8, INT4)
            cpu_offload: CPU ì˜¤í”„ë¡œë”© ì‚¬ìš© ì—¬ë¶€ (VRAM ì ˆì•½)
            model_path: ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ
            target_device: ëª©í‘œ ë””ë°”ì´ìŠ¤ ("auto", "cuda:0", "cuda:1", "cpu", "mps")
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (percent, label, detail)
        
        Returns:
            (success, message)
        """
        async with self._lock:
            if self.pipe is not None:
                return False, "ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ì–¸ë¡œë“œí•˜ì„¸ìš”."
            
            try:
                self.device = self.get_device(target_device)
                quant_info = EDIT_QUANTIZATION_OPTIONS.get(quantization)
                
                if not quant_info:
                    return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {quantization}"
                
                repo_id = quant_info["repo"]
                quantization_type = quant_info.get("quantization")  # None, "int8", "int4"
                
                # ì–‘ìí™” ì‚¬ìš© ì‹œ bitsandbytes í™•ì¸
                if quantization_type in ("int8", "int4"):
                    bnb_available, bnb_error = self._check_bitsandbytes_available()
                    if not bnb_available:
                        return False, bnb_error
                
                # ì§„í–‰ ìƒí™© ì½œë°±
                def report_progress(percent: int, label: str, detail: str = ""):
                    if progress_callback:
                        # async í•¨ìˆ˜ì™€ sync í•¨ìˆ˜ ëª¨ë‘ ì§€ì›
                        if asyncio.iscoroutinefunction(progress_callback):
                            asyncio.create_task(progress_callback(percent, label, detail))
                        else:
                            asyncio.create_task(
                                asyncio.to_thread(progress_callback, percent, label, detail)
                            )
                
                quant_label = quantization_type.upper() if quantization_type else "BF16"
                report_progress(5, f"ğŸ”§ LongCat-Image-Edit ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...", f"ë””ë°”ì´ìŠ¤: {self.device}, ì–‘ìí™”: {quant_label}")
                
                # LongCat-Image íŒ¨í‚¤ì§€ì—ì„œ ì„í¬íŠ¸
                from transformers import AutoProcessor, AutoModel
                from longcat_image.models import LongCatImageTransformer2DModel
                from longcat_image.pipelines import LongCatImageEditPipeline
                
                checkpoint_dir = model_path if model_path else repo_id
                
                report_progress(10, "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"ì €ì¥ì†Œ: {checkpoint_dir}")
                
                # Text Processor ë¡œë“œ
                report_progress(15, "ğŸ”„ Text Processor ë¡œë”© ì¤‘...", "")
                self.text_processor = await asyncio.to_thread(
                    AutoProcessor.from_pretrained,
                    checkpoint_dir,
                    subfolder="tokenizer"
                )
                
                # Text Encoder ë¡œë“œ (ì–‘ìí™” ì ìš©)
                if quantization_type in ("int8", "int4"):
                    report_progress(25, f"ğŸ”„ Text Encoder ë¡œë”© ì¤‘ ({quant_label} ì–‘ìí™”)...", "VRAM ì ˆì•½ ëª¨ë“œ")
                    
                    quant_config = self._get_quantization_config(quantization_type)
                    
                    # ì–‘ìí™”ëœ text_encoder ë¡œë“œ
                    def load_quantized_encoder():
                        from transformers import Qwen2VLForConditionalGeneration
                        return Qwen2VLForConditionalGeneration.from_pretrained(
                            checkpoint_dir,
                            subfolder="text_encoder",
                            quantization_config=quant_config,
                            torch_dtype=torch.bfloat16,
                            device_map="auto" if not cpu_offload else None,
                        )
                    
                    self.text_encoder = await asyncio.to_thread(load_quantized_encoder)
                else:
                    self.text_encoder = None
                
                # Transformer ë¡œë“œ
                report_progress(45, "ğŸ”„ Transformer ë¡œë”© ì¤‘...", "ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¡œë“œ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                self.transformer = await asyncio.to_thread(
                    LongCatImageTransformer2DModel.from_pretrained,
                    checkpoint_dir,
                    subfolder="transformer",
                    torch_dtype=torch.bfloat16,
                    use_safetensors=True
                )
                
                # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                report_progress(70, "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...", "")
                
                pipeline_kwargs = {
                    "transformer": self.transformer,
                    "text_processor": self.text_processor,
                    "torch_dtype": torch.bfloat16,
                }
                
                # ì–‘ìí™”ëœ text_encoderê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                if self.text_encoder is not None:
                    pipeline_kwargs["text_encoder"] = self.text_encoder
                
                self.pipe = await asyncio.to_thread(
                    LongCatImageEditPipeline.from_pretrained,
                    checkpoint_dir,
                    **pipeline_kwargs
                )
                
                # VAE ë©”ëª¨ë¦¬ ìµœì í™” í™œì„±í™”
                report_progress(80, "ğŸ”§ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì¤‘...", "VAE slicing/tiling í™œì„±í™”")
                await asyncio.to_thread(self.pipe.enable_vae_slicing)
                await asyncio.to_thread(self.pipe.enable_vae_tiling)
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                report_progress(85, f"ğŸš€ {self.device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...", "")
                
                if cpu_offload:
                    # ì–‘ìí™”ëœ ëª¨ë¸ì€ device_mapì´ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
                    if self.text_encoder is None:
                        await asyncio.to_thread(self.pipe.enable_model_cpu_offload)
                    else:
                        # ì–‘ìí™” + CPU ì˜¤í”„ë¡œë”©: transformerì™€ VAEë§Œ ì˜¤í”„ë¡œë”©
                        await asyncio.to_thread(self.pipe.enable_model_cpu_offload)
                    report_progress(95, "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì •ë¨", "VRAM ë¶€ì¡± ì‹œ RAM ì‚¬ìš©")
                    self.cpu_offload_enabled = True
                else:
                    # ì–‘ìí™”ëœ ëª¨ë¸ì€ ì´ë¯¸ device_mapìœ¼ë¡œ ë°°ì¹˜ë¨
                    if self.text_encoder is None:
                        await asyncio.to_thread(self.pipe.to, self.device, torch.bfloat16)
                    self.cpu_offload_enabled = False
                
                self.current_model = quantization
                self.current_quantization = quantization_type
                
                # GPU ëª¨ë‹ˆí„°ì— ëª¨ë¸ ë“±ë¡
                gpu_monitor.register_model("LongCat-Image-Edit", self.device)
                
                # ì›ë³¸ progress_bar ë©”ì„œë“œ ì €ì¥ (í›„í‚¹ ë³µì›ìš©)
                self._original_progress_bar = self.pipe.progress_bar.__func__
                
                report_progress(100, "âœ… LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!", self._get_vram_info())
                
                return True, f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_dir} ({quant_label})"
                
            except ImportError as e:
                error_msg = str(e)
                if "bitsandbytes" in error_msg:
                    return False, "bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install bitsandbytes'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
                return False, f"LongCat-Image íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install -e ./LongCat-Image'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}"
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
                self._cleanup()
                return False, f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    async def unload_model(self) -> Tuple[bool, str]:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        async with self._lock:
            if self.pipe is None:
                return True, "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            try:
                self._cleanup()
                return True, "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"
            except Exception as e:
                return False, f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def _cleanup(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        # GPU ëª¨ë‹ˆí„°ì—ì„œ ëª¨ë¸ ë“±ë¡ í•´ì œ
        gpu_monitor.unregister_model("LongCat-Image-Edit")
        
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
        
        if self.transformer is not None:
            del self.transformer
            self.transformer = None
        
        if self.text_processor is not None:
            del self.text_processor
            self.text_processor = None
        
        if self.text_encoder is not None:
            del self.text_encoder
            self.text_encoder = None
        
        self.current_model = None
        self.current_quantization = None
        self.cpu_offload_enabled = False
        self._original_progress_bar = None  # ì›ë³¸ progress_bar ì°¸ì¡°ë„ ì •ë¦¬
        
        # GPU ìºì‹œ ì •ë¦¬
        gpu_monitor.clear_cache(self.device)
        gc.collect()
    
    def _get_vram_info(self) -> str:
        """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
        return gpu_monitor.get_vram_summary()
    
    def _hook_progress_bar(self, step_callback):
        """íŒŒì´í”„ë¼ì¸ì˜ progress_barë¥¼ í›„í‚¹í•˜ì—¬ ìŠ¤í…ë³„ ì½œë°± í˜¸ì¶œ"""
        pipe = self.pipe
        
        # ë¡œë“œ ì‹œ ì €ì¥ëœ ì›ë³¸ progress_bar ì‚¬ìš©
        if self._original_progress_bar is None:
            print("[ê²½ê³ ] ì›ë³¸ progress_barê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ, í˜„ì¬ ë©”ì„œë“œ ì‚¬ìš©")
            original_progress_bar = pipe.progress_bar.__func__
        else:
            original_progress_bar = self._original_progress_bar
        
        def hooked_progress_bar(self_pipe, *args, **kwargs):
            # ì›ë˜ progress_bar í˜¸ì¶œ
            pbar = original_progress_bar(self_pipe, *args, **kwargs)
            
            # tqdmì˜ update ë©”ì„œë“œë¥¼ í›„í‚¹
            original_update = pbar.update
            
            def hooked_update(n=1):
                result = original_update(n)
                # ìŠ¤í… ì½œë°± í˜¸ì¶œ
                if step_callback and pbar.total:
                    step_callback(pbar.n, pbar.total)
                return result
            
            pbar.update = hooked_update
            return pbar
        
        # ë©”ì„œë“œ ë°”ì¸ë”©
        import types
        pipe.progress_bar = types.MethodType(hooked_progress_bar, pipe)
    
    def _restore_progress_bar(self):
        """ì›ë˜ progress_bar ë³µì›"""
        import types
        if self.pipe and self._original_progress_bar:
            self.pipe.progress_bar = types.MethodType(self._original_progress_bar, self.pipe)
    
    async def analyze_reference_image(
        self,
        reference_image: Image.Image,
        original_prompt: str
    ) -> Tuple[str, bool]:
        """
        ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ Vision LLMìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í†µí•©
        
        Args:
            reference_image: ì°¸ì¡° ì´ë¯¸ì§€
            original_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
        
        Returns:
            (ê²°í•©ëœ í”„ë¡¬í”„íŠ¸, ì„±ê³µ ì—¬ë¶€)
        """
        if not llm_client.is_available:
            print("LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©.")
            return original_prompt, False
        
        try:
            # í¸ì§‘ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ ìš”ì†Œë§Œ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
            analysis_prompt = REFERENCE_IMAGE_ANALYSIS_TEMPLATE.format(
                edit_prompt=original_prompt
            )
            
            # ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ (í¸ì§‘ í”„ë¡¬í”„íŠ¸ì™€ ê´€ë ¨ëœ ìš”ì†Œë§Œ)
            analysis = await asyncio.to_thread(
                llm_client.analyze_image,
                reference_image,
                analysis_prompt,
                temperature=0.3,  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì¶¤
                max_tokens=100   # ê°„ê²°í•œ ì„¤ëª…ë§Œ í•„ìš”
            )
            
            if not analysis:
                print("ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©.")
                return original_prompt, False
            
            # í”„ë¡¬í”„íŠ¸ ê²°í•© (ì›ë³¸ ì§€ì‹œ + ì°¸ì¡° ìš”ì†Œ ì„¤ëª…)
            # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë¨¼ì € ë‘ê³ , ì°¸ì¡° ìš”ì†ŒëŠ” ë³´ì¡° ì •ë³´ë¡œ ì¶”ê°€
            combined_prompt = f"{original_prompt}. Reference element: {analysis}"
            
            print(f"[í¸ì§‘ í”„ë¡¬í”„íŠ¸] {original_prompt}")
            print(f"[ì¶”ì¶œëœ ì°¸ì¡° ìš”ì†Œ] {analysis}")
            print(f"[ìµœì¢… í”„ë¡¬í”„íŠ¸] {combined_prompt}")
            
            return combined_prompt, True
            
        except Exception as e:
            print(f"ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return original_prompt, False
    
    async def edit_image(
        self,
        image: Image.Image,
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = 50,
        guidance_scale: float = 4.5,
        seed: int = -1,
        num_images: int = 1,
        reference_image: Optional[Image.Image] = None,
        progress_callback: Optional[Callable[[int, int, int, int], Any]] = None,
        status_callback: Optional[Callable[[str], Any]] = None
    ) -> Tuple[bool, list, str]:
        """
        ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰
        
        Args:
            image: í¸ì§‘í•  ì›ë³¸ ì´ë¯¸ì§€
            prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_inference_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            seed: ì‹œë“œ (-1ì´ë©´ ëœë¤)
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
            reference_image: ì°¸ì¡° ì´ë¯¸ì§€ (ìŠ¤íƒ€ì¼ ì°¸ì¡°ìš©, Vision LLMìœ¼ë¡œ ë¶„ì„)
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (current_image, total_images, current_step, total_steps)
            status_callback: ìƒíƒœ ë©”ì‹œì§€ ì½œë°± (message)
        
        Returns:
            (success, images, message)
        """
        if self.pipe is None:
            return False, [], "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            import random
            
            # í¸ì§‘ ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            
            # RGBë¡œ ë³€í™˜
            if image.mode != "RGB":
                image = image.convert("RGB")
            
            # ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ë° í”„ë¡¬í”„íŠ¸ ê²°í•©
            final_prompt = prompt
            if reference_image is not None:
                if status_callback:
                    await status_callback("ğŸ” ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
                
                combined_prompt, success = await self.analyze_reference_image(
                    reference_image, prompt
                )
                
                if success:
                    final_prompt = combined_prompt
                    if status_callback:
                        await status_callback("âœ… ì°¸ì¡° ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ì ìš©ë¨")
                        await status_callback(f"ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸: {final_prompt}")
                else:
                    if status_callback:
                        await status_callback("âš ï¸ ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨, ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
            
            # ì‹œë“œ ì„¤ì •
            if seed == -1:
                seed = random.randint(0, 2147483647)
            
            generator = torch.Generator("cpu").manual_seed(seed)
            
            # ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ìº¡ì²˜ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
            main_loop = asyncio.get_running_loop()
            
            results = []
            for i in range(num_images):
                current_seed = seed + i
                if i > 0:
                    generator = torch.Generator("cpu").manual_seed(current_seed)
                
                # ìŠ¤í… ì½œë°±ì„ ìœ„í•œ ìƒíƒœ ì €ì¥ (í´ë¡œì € ë¬¸ì œ ë°©ì§€)
                current_image_idx = i
                total_images = num_images
                
                # ìŠ¤í…ë³„ ì½œë°± í•¨ìˆ˜ ìƒì„±
                def create_step_callback(img_idx, total_imgs):
                    def step_callback(current_step, total_steps):
                        if progress_callback:
                            # ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ì— ì½”ë£¨í‹´ ìŠ¤ì¼€ì¤„ë§
                            asyncio.run_coroutine_threadsafe(
                                progress_callback(img_idx + 1, total_imgs, current_step, total_steps),
                                main_loop
                            )
                    return step_callback
                
                step_cb = create_step_callback(current_image_idx, total_images)
                
                # progress_bar í›„í‚¹
                self._hook_progress_bar(step_cb)
                
                try:
                    # í¸ì§‘ ì‹¤í–‰ (final_prompt ì‚¬ìš© - ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ í¬í•¨)
                    def run_edit():
                        return self.pipe(
                            image,
                            final_prompt,
                            negative_prompt=negative_prompt,
                            guidance_scale=guidance_scale,
                            num_inference_steps=num_inference_steps,
                            num_images_per_prompt=1,
                            generator=generator
                        ).images[0]
                    
                    result_image = await asyncio.to_thread(run_edit)
                finally:
                    # progress_bar ë³µì›
                    self._restore_progress_bar()
                
                results.append({
                    "image": result_image,
                    "seed": current_seed
                })
            
            # í¸ì§‘ ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            
            return True, results, f"í¸ì§‘ ì™„ë£Œ (ì‹œë“œ: {seed})"
            
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ progress_bar ë³µì› ë° ë©”ëª¨ë¦¬ ì •ë¦¬
            self._restore_progress_bar()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            gc.collect()
            return False, [], f"í¸ì§‘ ì‹¤íŒ¨: {str(e)}"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
longcat_edit_manager = LongCatEditManager()

