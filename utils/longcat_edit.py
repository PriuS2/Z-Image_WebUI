"""LongCat-Image-Edit íŒŒì´í”„ë¼ì¸ ê´€ë¦¬"""

import gc
import asyncio
from pathlib import Path
from typing import Optional, Tuple, Callable, Any

import torch
from PIL import Image

from config.defaults import (
    LONGCAT_EDIT_MODEL,
    LONGCAT_EDIT_GGUF_REPO,
    EDIT_QUANTIZATION_OPTIONS,
    DEFAULT_EDIT_SETTINGS,
)


class LongCatEditManager:
    """LongCat-Image-Edit ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.pipe = None
        self.transformer = None
        self.text_processor = None
        self.current_model: Optional[str] = None
        self.device: Optional[str] = None
        self._lock = asyncio.Lock()
    
    @property
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€"""
        return self.pipe is not None
    
    def get_device(self) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    
    async def load_model(
        self,
        quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)",
        cpu_offload: bool = True,
        model_path: Optional[str] = None,
        progress_callback: Optional[Callable[[int, str, str], Any]] = None
    ) -> Tuple[bool, str]:
        """
        LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ
        
        Args:
            quantization: ì–‘ìí™” ì˜µì…˜
            cpu_offload: CPU ì˜¤í”„ë¡œë”© ì‚¬ìš© ì—¬ë¶€ (VRAM ì ˆì•½)
            model_path: ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (percent, label, detail)
        
        Returns:
            (success, message)
        """
        async with self._lock:
            if self.pipe is not None:
                return False, "ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ì–¸ë¡œë“œí•˜ì„¸ìš”."
            
            try:
                self.device = self.get_device()
                quant_info = EDIT_QUANTIZATION_OPTIONS.get(quantization)
                
                if not quant_info:
                    return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {quantization}"
                
                repo_id = quant_info["repo"]
                is_gguf = quant_info.get("is_gguf", False)
                
                # ì§„í–‰ ìƒí™© ì½œë°±
                def report_progress(percent: int, label: str, detail: str = ""):
                    if progress_callback:
                        # async í•¨ìˆ˜ì™€ sync í•¨ìˆ˜ ëª¨ë‘ ì§€ì›
                        if asyncio.iscoroutinefunction(progress_callback):
                            asyncio.create_task(progress_callback(percent, label, detail))
                        else:
                            asyncio.create_task(
                                asyncio.to_thread(progress_callback, percent, label, detail)
                            )
                
                report_progress(5, "ğŸ”§ LongCat-Image-Edit ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...", f"ë””ë°”ì´ìŠ¤: {self.device}")
                
                # LongCat-Image íŒ¨í‚¤ì§€ì—ì„œ ì„í¬íŠ¸
                from transformers import AutoProcessor
                from longcat_image.models import LongCatImageTransformer2DModel
                from longcat_image.pipelines import LongCatImageEditPipeline
                
                checkpoint_dir = model_path if model_path else repo_id
                
                if is_gguf:
                    # GGUF ì–‘ìí™” ëª¨ë¸ ë¡œë“œ
                    from diffusers import FluxTransformer2DModel, GGUFQuantizationConfig
                    from huggingface_hub import hf_hub_download
                    
                    filename = quant_info["filename"]
                    gguf_repo = quant_info["repo"]
                    
                    # GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                    report_progress(10, "ğŸ“¥ GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"íŒŒì¼: {filename}")
                    gguf_path = await asyncio.to_thread(
                        hf_hub_download,
                        repo_id=gguf_repo,
                        filename=filename
                    )
                    
                    # GGUF Transformer ë¡œë“œ (FluxTransformer2DModel ì‚¬ìš©)
                    report_progress(30, "ğŸ”„ GGUF Transformer ë¡œë”© ì¤‘...", "ì–‘ìí™” ëª¨ë¸ ë¡œë“œ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                    self.transformer = await asyncio.to_thread(
                        FluxTransformer2DModel.from_single_file,
                        gguf_path,
                        quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
                        torch_dtype=torch.bfloat16
                    )
                    
                    # ê¸°ë³¸ ëª¨ë¸ì—ì„œ ë‚˜ë¨¸ì§€ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ
                    base_model = LONGCAT_EDIT_MODEL
                    report_progress(50, "ğŸ”„ Text Processor ë¡œë”© ì¤‘...", f"ê¸°ë³¸ ëª¨ë¸: {base_model}")
                    self.text_processor = await asyncio.to_thread(
                        AutoProcessor.from_pretrained,
                        base_model,
                        subfolder="tokenizer"
                    )
                    
                    # íŒŒì´í”„ë¼ì¸ êµ¬ì„± (GGUF transformer ì‚¬ìš©)
                    report_progress(70, "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...", "GGUF Transformerì™€ ê¸°ë³¸ ëª¨ë¸ ê²°í•©")
                    self.pipe = await asyncio.to_thread(
                        LongCatImageEditPipeline.from_pretrained,
                        base_model,
                        transformer=self.transformer,
                        text_processor=self.text_processor,
                        torch_dtype=torch.bfloat16
                    )
                else:
                    # BF16 ëª¨ë¸ ë¡œë“œ
                    report_progress(10, "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"ì €ì¥ì†Œ: {checkpoint_dir}")
                    
                    # Text Processor ë¡œë“œ
                    report_progress(20, "ğŸ”„ Text Processor ë¡œë”© ì¤‘...", "")
                    self.text_processor = await asyncio.to_thread(
                        AutoProcessor.from_pretrained,
                        checkpoint_dir,
                        subfolder="tokenizer"
                    )
                    
                    # Transformer ë¡œë“œ
                    report_progress(40, "ğŸ”„ Transformer ë¡œë”© ì¤‘...", "ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¡œë“œ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                    self.transformer = await asyncio.to_thread(
                        LongCatImageTransformer2DModel.from_pretrained,
                        checkpoint_dir,
                        subfolder="transformer",
                        torch_dtype=torch.bfloat16,
                        use_safetensors=True
                    )
                    
                    # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                    report_progress(70, "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...", "")
                    self.pipe = await asyncio.to_thread(
                        LongCatImageEditPipeline.from_pretrained,
                        checkpoint_dir,
                        transformer=self.transformer,
                        text_processor=self.text_processor,
                        torch_dtype=torch.bfloat16
                    )
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                report_progress(85, f"ğŸš€ {self.device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...", "")
                
                if cpu_offload:
                    await asyncio.to_thread(self.pipe.enable_model_cpu_offload)
                    report_progress(95, "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì •ë¨", "VRAM ë¶€ì¡± ì‹œ RAM ì‚¬ìš©")
                else:
                    await asyncio.to_thread(self.pipe.to, self.device, torch.bfloat16)
                
                self.current_model = quantization
                
                report_progress(100, "âœ… LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!", self._get_vram_info())
                
                return True, f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_dir}"
                
            except ImportError as e:
                return False, f"LongCat-Image íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install -e ./LongCat-Image'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}"
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
                self._cleanup()
                return False, f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    async def unload_model(self) -> Tuple[bool, str]:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        async with self._lock:
            if self.pipe is None:
                return True, "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            try:
                self._cleanup()
                return True, "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"
            except Exception as e:
                return False, f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def _cleanup(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
        
        if self.transformer is not None:
            del self.transformer
            self.transformer = None
        
        if self.text_processor is not None:
            del self.text_processor
            self.text_processor = None
        
        self.current_model = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
    
    def _get_vram_info(self) -> str:
        """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
        if torch.cuda.is_available():
            vram_used = torch.cuda.memory_allocated() / 1024**3
            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            return f"VRAM: {vram_used:.1f}GB / {vram_total:.1f}GB"
        return "N/A"
    
    async def edit_image(
        self,
        image: Image.Image,
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = 50,
        guidance_scale: float = 4.5,
        seed: int = -1,
        num_images: int = 1,
        reference_image: Optional[Image.Image] = None
    ) -> Tuple[bool, list, str]:
        """
        ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰
        
        Args:
            image: í¸ì§‘í•  ì›ë³¸ ì´ë¯¸ì§€
            prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_inference_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            seed: ì‹œë“œ (-1ì´ë©´ ëœë¤)
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
            reference_image: ì°¸ì¡° ì´ë¯¸ì§€ (ìŠ¤íƒ€ì¼ ì°¸ì¡°ìš©)
        
        Returns:
            (success, images, message)
        """
        if self.pipe is None:
            return False, [], "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            import random
            
            # RGBë¡œ ë³€í™˜
            if image.mode != "RGB":
                image = image.convert("RGB")
            
            # ì‹œë“œ ì„¤ì •
            if seed == -1:
                seed = random.randint(0, 2147483647)
            
            generator = torch.Generator("cpu").manual_seed(seed)
            
            results = []
            for i in range(num_images):
                current_seed = seed + i
                if i > 0:
                    generator = torch.Generator("cpu").manual_seed(current_seed)
                
                # í¸ì§‘ ì‹¤í–‰
                def run_edit():
                    return self.pipe(
                        image,
                        prompt,
                        negative_prompt=negative_prompt,
                        guidance_scale=guidance_scale,
                        num_inference_steps=num_inference_steps,
                        num_images_per_prompt=1,
                        generator=generator
                    ).images[0]
                
                result_image = await asyncio.to_thread(run_edit)
                results.append({
                    "image": result_image,
                    "seed": current_seed
                })
            
            return True, results, f"í¸ì§‘ ì™„ë£Œ (ì‹œë“œ: {seed})"
            
        except Exception as e:
            return False, [], f"í¸ì§‘ ì‹¤íŒ¨: {str(e)}"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
longcat_edit_manager = LongCatEditManager()

