"""LongCat-Image-Edit íŒŒì´í”„ë¼ì¸ ê´€ë¦¬"""

import gc
import asyncio
from typing import Optional, Tuple, Callable, Any

import torch
from PIL import Image

from config.defaults import (
    LONGCAT_EDIT_MODEL,
    EDIT_QUANTIZATION_OPTIONS,
    DEFAULT_EDIT_SETTINGS,
    EDIT_GPU_INDEX,
    EDIT_TEXT_ENCODER_GPU,
    EDIT_TRANSFORMER_GPU,
)
from utils.llm_client import llm_client


# ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í¸ì§‘ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ ìš”ì†Œë§Œ ì¶”ì¶œ)
REFERENCE_IMAGE_ANALYSIS_TEMPLATE = """You are an expert at analyzing images for AI image editing tasks.

The user wants to edit an image with this instruction: "{edit_prompt}"

Your task: Look at the reference image and extract ONLY the specific elements mentioned or implied in the edit instruction.

Rules:
1. Focus ONLY on elements relevant to the edit instruction
2. If the instruction mentions "flower pot" or "plant", describe ONLY the pot/plant from the reference image
3. If the instruction mentions "style" or "atmosphere", describe ONLY the artistic style
4. If the instruction mentions a specific object, describe ONLY that object
5. Do NOT describe the entire image - be selective and focused
6. Keep description brief (max 50 words) - just the essential visual details

Examples:
- Edit instruction: "Make them hold this flower pot" â†’ Describe only the pot (color, material, plant type)
- Edit instruction: "Apply this painting style" â†’ Describe only the art style (brushwork, colors, technique)
- Edit instruction: "Add this hat to the person" â†’ Describe only the hat (shape, color, material)

Output ONLY a brief, focused description of the relevant element(s). No explanation or preamble.
Output in English."""


class LongCatEditManager:
    """LongCat-Image-Edit ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.pipe = None
        self.transformer = None
        self.text_processor = None
        self.current_model: Optional[str] = None
        self.device: Optional[str] = None
        self.gpu_index: int = EDIT_GPU_INDEX
        # ì»´í¬ë„ŒíŠ¸ë³„ GPU ì„¤ì • (-1ì´ë©´ ë¶„ì‚° ë¹„í™œì„±í™”)
        self.text_encoder_gpu: int = EDIT_TEXT_ENCODER_GPU
        self.transformer_gpu: int = EDIT_TRANSFORMER_GPU  # VAEë„ í•¨ê»˜ ë°°ì¹˜
        self.distributed_mode: bool = False  # ë¶„ì‚° ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
        self._lock = asyncio.Lock()
    
    @property
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€"""
        return self.pipe is not None
    
    def get_device(self, gpu_index: Optional[int] = None) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜ (ë©€í‹° GPU ì§€ì›)"""
        if gpu_index is None:
            gpu_index = self.gpu_index
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            if gpu_index >= gpu_count:
                gpu_index = 0  # ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ í´ë°±
            return f"cuda:{gpu_index}"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    
    async def load_model(
        self,
        quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)",
        cpu_offload: bool = True,
        model_path: Optional[str] = None,
        gpu_index: Optional[int] = None,
        text_encoder_gpu: Optional[int] = None,
        transformer_gpu: Optional[int] = None,
        progress_callback: Optional[Callable[[int, str, str], Any]] = None
    ) -> Tuple[bool, str]:
        """
        LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ
        
        Args:
            quantization: ì–‘ìí™” ì˜µì…˜
            cpu_offload: CPU ì˜¤í”„ë¡œë”© ì‚¬ìš© ì—¬ë¶€ (VRAM ì ˆì•½)
            model_path: ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ
            gpu_index: ì‚¬ìš©í•  GPU ì¸ë±ìŠ¤ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©) - ë¶„ì‚° ë¹„í™œì„±í™” ì‹œ ì‚¬ìš©
            text_encoder_gpu: Text Encoder GPU ì¸ë±ìŠ¤ (-1 ë˜ëŠ” Noneì´ë©´ ë¶„ì‚° ì•ˆí•¨)
            transformer_gpu: Transformer + VAE GPU ì¸ë±ìŠ¤ (-1 ë˜ëŠ” Noneì´ë©´ ë¶„ì‚° ì•ˆí•¨)
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (percent, label, detail)
        
        Returns:
            (success, message)
        """
        async with self._lock:
            if self.pipe is not None:
                return False, "ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ì–¸ë¡œë“œí•˜ì„¸ìš”."
            
            try:
                # GPU ì¸ë±ìŠ¤ ì„¤ì •
                if gpu_index is not None:
                    self.gpu_index = gpu_index
                
                # ì»´í¬ë„ŒíŠ¸ë³„ GPU ì„¤ì • (-1 ë˜ëŠ” Noneì´ë©´ ë¶„ì‚° ë¹„í™œì„±í™”)
                if text_encoder_gpu is not None and text_encoder_gpu >= 0:
                    self.text_encoder_gpu = text_encoder_gpu
                if transformer_gpu is not None and transformer_gpu >= 0:
                    self.transformer_gpu = transformer_gpu
                
                # GPU ì¸ë±ìŠ¤ ìœ íš¨ì„± ê²€ì‚¬
                gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
                if gpu_count > 0 and self.gpu_index >= gpu_count:
                    self.gpu_index = 0  # ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ í´ë°±
                
                # ë¶„ì‚° ëª¨ë“œ í™œì„±í™” ì—¬ë¶€ í™•ì¸ (Text Encoderì™€ Transformer+VAEë¥¼ ë‹¤ë¥¸ GPUì— ë°°ì¹˜)
                self.distributed_mode = (
                    gpu_count > 1 and 
                    (self.text_encoder_gpu >= 0 or self.transformer_gpu >= 0)
                )
                
                self.device = self.get_device(self.gpu_index)
                quant_info = EDIT_QUANTIZATION_OPTIONS.get(quantization)
                
                if not quant_info:
                    return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {quantization}"
                
                repo_id = quant_info["repo"]
                
                # ì§„í–‰ ìƒí™© ì½œë°±
                def report_progress(percent: int, label: str, detail: str = ""):
                    if progress_callback:
                        # async í•¨ìˆ˜ì™€ sync í•¨ìˆ˜ ëª¨ë‘ ì§€ì›
                        if asyncio.iscoroutinefunction(progress_callback):
                            asyncio.create_task(progress_callback(percent, label, detail))
                        else:
                            asyncio.create_task(
                                asyncio.to_thread(progress_callback, percent, label, detail)
                            )
                
                # ë¶„ì‚° ëª¨ë“œ ë©”ì‹œì§€
                if self.distributed_mode:
                    dist_info = []
                    if self.text_encoder_gpu >= 0:
                        dist_info.append(f"TextEncâ†’GPU{self.text_encoder_gpu}")
                    if self.transformer_gpu >= 0:
                        dist_info.append(f"Trans+VAEâ†’GPU{self.transformer_gpu}")
                    report_progress(5, "ğŸ”§ ë¶„ì‚° ëª¨ë“œë¡œ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...", ", ".join(dist_info))
                else:
                    report_progress(5, "ğŸ”§ LongCat-Image-Edit ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...", f"ë””ë°”ì´ìŠ¤: {self.device}")
                
                # LongCat-Image íŒ¨í‚¤ì§€ì—ì„œ ì„í¬íŠ¸
                from transformers import AutoProcessor
                from longcat_image.models import LongCatImageTransformer2DModel
                from longcat_image.pipelines import LongCatImageEditPipeline
                
                checkpoint_dir = model_path if model_path else repo_id
                
                # BF16 ëª¨ë¸ ë¡œë“œ
                report_progress(10, "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"ì €ì¥ì†Œ: {checkpoint_dir}")
                
                # Text Processor ë¡œë“œ
                report_progress(20, "ğŸ”„ Text Processor ë¡œë”© ì¤‘...", "")
                self.text_processor = await asyncio.to_thread(
                    AutoProcessor.from_pretrained,
                    checkpoint_dir,
                    subfolder="tokenizer"
                )
                
                # Transformer ë¡œë“œ
                report_progress(40, "ğŸ”„ Transformer ë¡œë”© ì¤‘...", "ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¡œë“œ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                self.transformer = await asyncio.to_thread(
                    LongCatImageTransformer2DModel.from_pretrained,
                    checkpoint_dir,
                    subfolder="transformer",
                    torch_dtype=torch.bfloat16,
                    use_safetensors=True
                )
                
                # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                report_progress(70, "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...", "")
                self.pipe = await asyncio.to_thread(
                    LongCatImageEditPipeline.from_pretrained,
                    checkpoint_dir,
                    transformer=self.transformer,
                    text_processor=self.text_processor,
                    torch_dtype=torch.bfloat16
                )
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
                
                if self.distributed_mode and gpu_count > 1:
                    # ë¶„ì‚° ëª¨ë“œ: ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ì§€ì •ëœ GPUë¡œ ì´ë™ + accelerate hooks ì‚¬ìš©
                    report_progress(85, "ğŸ”€ ì»´í¬ë„ŒíŠ¸ë³„ GPU ë¶„ì‚° ì¤‘...", "")
                    
                    def distribute_components():
                        from accelerate.hooks import add_hook_to_module, AlignDevicesHook
                        
                        default_device = torch.device(f"cuda:{self.gpu_index}")
                        
                        # Text Encoder ë°°ì¹˜
                        te_device = default_device
                        if self.text_encoder_gpu >= 0 and self.text_encoder_gpu < gpu_count:
                            te_device = torch.device(f"cuda:{self.text_encoder_gpu}")
                        if hasattr(self.pipe, 'text_encoder') and self.pipe.text_encoder is not None:
                            self.pipe.text_encoder = self.pipe.text_encoder.to(te_device)
                            # Hook ì¶”ê°€: forward ì‹œ ì…ë ¥ì„ ìë™ìœ¼ë¡œ í•´ë‹¹ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                            add_hook_to_module(self.pipe.text_encoder, AlignDevicesHook(execution_device=te_device))
                            print(f"ğŸ“ Text Encoder â†’ {te_device}")
                        
                        # Transformer + VAE ë°°ì¹˜ (ê°™ì€ GPUì— í•¨ê»˜ ë°°ì¹˜)
                        tf_vae_device = default_device
                        if self.transformer_gpu >= 0 and self.transformer_gpu < gpu_count:
                            tf_vae_device = torch.device(f"cuda:{self.transformer_gpu}")
                        
                        if hasattr(self.pipe, 'transformer') and self.pipe.transformer is not None:
                            self.pipe.transformer = self.pipe.transformer.to(tf_vae_device)
                            add_hook_to_module(self.pipe.transformer, AlignDevicesHook(execution_device=tf_vae_device))
                            print(f"ğŸ“ Transformer â†’ {tf_vae_device}")
                        
                        # VAEëŠ” Transformerì™€ ê°™ì€ GPUì— ë°°ì¹˜
                        if hasattr(self.pipe, 'vae') and self.pipe.vae is not None:
                            self.pipe.vae = self.pipe.vae.to(tf_vae_device)
                            add_hook_to_module(self.pipe.vae, AlignDevicesHook(execution_device=tf_vae_device))
                            print(f"ğŸ“ VAE â†’ {tf_vae_device} (Transformerì™€ ë™ì¼)")
                    
                    await asyncio.to_thread(distribute_components)
                    
                    # ë¶„ì‚° ë°°ì¹˜ ì •ë³´ ìƒì„±
                    dist_info = []
                    if self.text_encoder_gpu >= 0:
                        dist_info.append(f"TextEncâ†’GPU{self.text_encoder_gpu}")
                    if self.transformer_gpu >= 0:
                        dist_info.append(f"Trans+VAEâ†’GPU{self.transformer_gpu}")
                    
                    report_progress(95, "âš™ï¸ ë¶„ì‚° ë°°ì¹˜ ì™„ë£Œ", ", ".join(dist_info) if dist_info else "ê¸°ë³¸ GPU ì‚¬ìš©")
                
                elif cpu_offload:
                    # CPU ì˜¤í”„ë¡œë”© ëª¨ë“œ
                    gpu_name = torch.cuda.get_device_properties(self.gpu_index).name if gpu_count > 0 else "N/A"
                    report_progress(85, f"ğŸš€ GPU{self.gpu_index} ({gpu_name})ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...", "")
                    await asyncio.to_thread(self.pipe.enable_model_cpu_offload, gpu_id=self.gpu_index)
                    report_progress(95, "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì •ë¨", f"GPU{self.gpu_index} ì‚¬ìš©, VRAM ë¶€ì¡± ì‹œ RAM ì‚¬ìš©")
                
                else:
                    # ë‹¨ì¼ GPU ëª¨ë“œ
                    gpu_name = torch.cuda.get_device_properties(self.gpu_index).name if gpu_count > 0 else "N/A"
                    report_progress(85, f"ğŸš€ GPU{self.gpu_index} ({gpu_name})ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...", "")
                    await asyncio.to_thread(self.pipe.to, self.device, torch.bfloat16)
                
                self.current_model = quantization
                
                # ì™„ë£Œ ë©”ì‹œì§€
                if self.distributed_mode:
                    report_progress(100, "âœ… ë¶„ì‚° ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!", self._get_all_vram_info())
                else:
                    report_progress(100, "âœ… LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!", self._get_vram_info())
                
                return True, f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_dir}"
                
            except ImportError as e:
                return False, f"LongCat-Image íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install -e ./LongCat-Image'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}"
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
                self._cleanup()
                return False, f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    async def unload_model(self) -> Tuple[bool, str]:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        async with self._lock:
            if self.pipe is None:
                return True, "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            try:
                self._cleanup()
                return True, "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"
            except Exception as e:
                return False, f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def _cleanup(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
        
        if self.transformer is not None:
            del self.transformer
            self.transformer = None
        
        if self.text_processor is not None:
            del self.text_processor
            self.text_processor = None
        
        self.current_model = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
    
    def _get_vram_info(self) -> str:
        """í˜„ì¬ GPUì˜ VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
        if torch.cuda.is_available():
            gpu_idx = self.gpu_index
            vram_used = torch.cuda.memory_allocated(gpu_idx) / 1024**3
            vram_total = torch.cuda.get_device_properties(gpu_idx).total_memory / 1024**3
            return f"GPU{gpu_idx} VRAM: {vram_used:.1f}GB / {vram_total:.1f}GB"
        return "N/A"
    
    def _get_all_vram_info(self) -> str:
        """ëª¨ë“  GPUì˜ VRAM ì‚¬ìš©ëŸ‰ ì •ë³´ (ë¶„ì‚° ëª¨ë“œìš©)"""
        if torch.cuda.is_available():
            infos = []
            gpu_count = torch.cuda.device_count()
            
            # ì‚¬ìš© ì¤‘ì¸ GPUë§Œ í‘œì‹œ
            used_gpus = set([self.gpu_index])
            if self.text_encoder_gpu >= 0:
                used_gpus.add(self.text_encoder_gpu)
            if self.transformer_gpu >= 0:
                used_gpus.add(self.transformer_gpu)  # VAEë„ ì—¬ê¸°ì— í¬í•¨
            
            for gpu_idx in sorted(used_gpus):
                if gpu_idx < gpu_count:
                    vram_used = torch.cuda.memory_allocated(gpu_idx) / 1024**3
                    vram_total = torch.cuda.get_device_properties(gpu_idx).total_memory / 1024**3
                    infos.append(f"GPU{gpu_idx}: {vram_used:.1f}/{vram_total:.1f}GB")
            
            return " | ".join(infos)
        return "N/A"
    
    def _hook_progress_bar(self, step_callback):
        """íŒŒì´í”„ë¼ì¸ì˜ progress_barë¥¼ í›„í‚¹í•˜ì—¬ ìŠ¤í…ë³„ ì½œë°± í˜¸ì¶œ"""
        pipe = self.pipe
        original_progress_bar = pipe.progress_bar.__func__  # ì–¸ë°”ìš´ë“œ ë©”ì„œë“œ ê°€ì ¸ì˜¤ê¸°
        
        def hooked_progress_bar(self_pipe, *args, **kwargs):
            # ì›ë˜ progress_bar í˜¸ì¶œ
            pbar = original_progress_bar(self_pipe, *args, **kwargs)
            
            # tqdmì˜ update ë©”ì„œë“œë¥¼ í›„í‚¹
            original_update = pbar.update
            
            def hooked_update(n=1):
                result = original_update(n)
                # ìŠ¤í… ì½œë°± í˜¸ì¶œ
                if step_callback and pbar.total:
                    step_callback(pbar.n, pbar.total)
                return result
            
            pbar.update = hooked_update
            return pbar
        
        # ë©”ì„œë“œ ë°”ì¸ë”©
        import types
        pipe.progress_bar = types.MethodType(hooked_progress_bar, pipe)
        
        return original_progress_bar
    
    def _restore_progress_bar(self, original_progress_bar):
        """ì›ë˜ progress_bar ë³µì›"""
        import types
        if self.pipe and original_progress_bar:
            self.pipe.progress_bar = types.MethodType(original_progress_bar, self.pipe)
    
    async def analyze_reference_image(
        self,
        reference_image: Image.Image,
        original_prompt: str
    ) -> Tuple[str, bool]:
        """
        ì°¸ì¡° ì´ë¯¸ì§€ë¥¼ Vision LLMìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í†µí•©
        
        Args:
            reference_image: ì°¸ì¡° ì´ë¯¸ì§€
            original_prompt: ì›ë³¸ í”„ë¡¬í”„íŠ¸
        
        Returns:
            (ê²°í•©ëœ í”„ë¡¬í”„íŠ¸, ì„±ê³µ ì—¬ë¶€)
        """
        if not llm_client.is_available:
            print("LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©.")
            return original_prompt, False
        
        try:
            # í¸ì§‘ í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ìœ¼ë¡œ í•„ìš”í•œ ìš”ì†Œë§Œ ì¶”ì¶œí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
            analysis_prompt = REFERENCE_IMAGE_ANALYSIS_TEMPLATE.format(
                edit_prompt=original_prompt
            )
            
            # ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ (í¸ì§‘ í”„ë¡¬í”„íŠ¸ì™€ ê´€ë ¨ëœ ìš”ì†Œë§Œ)
            analysis = await asyncio.to_thread(
                llm_client.analyze_image,
                reference_image,
                analysis_prompt,
                temperature=0.3,  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì¶¤
                max_tokens=100   # ê°„ê²°í•œ ì„¤ëª…ë§Œ í•„ìš”
            )
            
            if not analysis:
                print("ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨. ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©.")
                return original_prompt, False
            
            # í”„ë¡¬í”„íŠ¸ ê²°í•© (ì›ë³¸ ì§€ì‹œ + ì°¸ì¡° ìš”ì†Œ ì„¤ëª…)
            # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ë¨¼ì € ë‘ê³ , ì°¸ì¡° ìš”ì†ŒëŠ” ë³´ì¡° ì •ë³´ë¡œ ì¶”ê°€
            combined_prompt = f"{original_prompt}. Reference element: {analysis}"
            
            print(f"[í¸ì§‘ í”„ë¡¬í”„íŠ¸] {original_prompt}")
            print(f"[ì¶”ì¶œëœ ì°¸ì¡° ìš”ì†Œ] {analysis}")
            print(f"[ìµœì¢… í”„ë¡¬í”„íŠ¸] {combined_prompt}")
            
            return combined_prompt, True
            
        except Exception as e:
            print(f"ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return original_prompt, False
    
    async def edit_image(
        self,
        image: Image.Image,
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = 50,
        guidance_scale: float = 4.5,
        seed: int = -1,
        num_images: int = 1,
        reference_image: Optional[Image.Image] = None,
        progress_callback: Optional[Callable[[int, int, int, int], Any]] = None,
        status_callback: Optional[Callable[[str], Any]] = None
    ) -> Tuple[bool, list, str]:
        """
        ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰
        
        Args:
            image: í¸ì§‘í•  ì›ë³¸ ì´ë¯¸ì§€
            prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_inference_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            seed: ì‹œë“œ (-1ì´ë©´ ëœë¤)
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
            reference_image: ì°¸ì¡° ì´ë¯¸ì§€ (ìŠ¤íƒ€ì¼ ì°¸ì¡°ìš©, Vision LLMìœ¼ë¡œ ë¶„ì„)
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (current_image, total_images, current_step, total_steps)
            status_callback: ìƒíƒœ ë©”ì‹œì§€ ì½œë°± (message)
        
        Returns:
            (success, images, message)
        """
        if self.pipe is None:
            return False, [], "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            import random
            
            # RGBë¡œ ë³€í™˜
            if image.mode != "RGB":
                image = image.convert("RGB")
            
            # ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ë° í”„ë¡¬í”„íŠ¸ ê²°í•©
            final_prompt = prompt
            if reference_image is not None:
                if status_callback:
                    await status_callback("ğŸ” ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì¤‘...")
                
                combined_prompt, success = await self.analyze_reference_image(
                    reference_image, prompt
                )
                
                if success:
                    final_prompt = combined_prompt
                    if status_callback:
                        await status_callback("âœ… ì°¸ì¡° ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼ ì ìš©ë¨")
                        await status_callback(f"ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸: {final_prompt}")
                else:
                    if status_callback:
                        await status_callback("âš ï¸ ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨, ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©")
            
            # ì‹œë“œ ì„¤ì •
            if seed == -1:
                seed = random.randint(0, 2147483647)
            
            generator = torch.Generator("cpu").manual_seed(seed)
            
            # ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ìº¡ì²˜ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´)
            main_loop = asyncio.get_running_loop()
            
            results = []
            for i in range(num_images):
                current_seed = seed + i
                if i > 0:
                    generator = torch.Generator("cpu").manual_seed(current_seed)
                
                # ìŠ¤í… ì½œë°±ì„ ìœ„í•œ ìƒíƒœ ì €ì¥ (í´ë¡œì € ë¬¸ì œ ë°©ì§€)
                current_image_idx = i
                total_images = num_images
                
                # ìŠ¤í…ë³„ ì½œë°± í•¨ìˆ˜ ìƒì„±
                def create_step_callback(img_idx, total_imgs):
                    def step_callback(current_step, total_steps):
                        if progress_callback:
                            # ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ì— ì½”ë£¨í‹´ ìŠ¤ì¼€ì¤„ë§
                            asyncio.run_coroutine_threadsafe(
                                progress_callback(img_idx + 1, total_imgs, current_step, total_steps),
                                main_loop
                            )
                    return step_callback
                
                step_cb = create_step_callback(current_image_idx, total_images)
                
                # progress_bar í›„í‚¹
                original_progress_bar = self._hook_progress_bar(step_cb)
                
                try:
                    # í¸ì§‘ ì‹¤í–‰ (final_prompt ì‚¬ìš© - ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ í¬í•¨)
                    def run_edit():
                        return self.pipe(
                            image,
                            final_prompt,
                            negative_prompt=negative_prompt,
                            guidance_scale=guidance_scale,
                            num_inference_steps=num_inference_steps,
                            num_images_per_prompt=1,
                            generator=generator
                        ).images[0]
                    
                    result_image = await asyncio.to_thread(run_edit)
                finally:
                    # progress_bar ë³µì›
                    self._restore_progress_bar(original_progress_bar)
                
                results.append({
                    "image": result_image,
                    "seed": current_seed
                })
            
            return True, results, f"í¸ì§‘ ì™„ë£Œ (ì‹œë“œ: {seed})"
            
        except Exception as e:
            return False, [], f"í¸ì§‘ ì‹¤íŒ¨: {str(e)}"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
longcat_edit_manager = LongCatEditManager()

