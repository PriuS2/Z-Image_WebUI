"""LongCat-Image-Edit íŒŒì´í”„ë¼ì¸ ê´€ë¦¬"""

import gc
import asyncio
from typing import Optional, Tuple, Callable, Any

import torch
from PIL import Image

from config.defaults import (
    LONGCAT_EDIT_MODEL,
    EDIT_QUANTIZATION_OPTIONS,
    DEFAULT_EDIT_SETTINGS,
)


class LongCatEditManager:
    """LongCat-Image-Edit ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.pipe = None
        self.transformer = None
        self.text_processor = None
        self.current_model: Optional[str] = None
        self.device: Optional[str] = None
        self._lock = asyncio.Lock()
    
    @property
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ì—¬ë¶€"""
        return self.pipe is not None
    
    def get_device(self) -> str:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    
    async def load_model(
        self,
        quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)",
        cpu_offload: bool = True,
        model_path: Optional[str] = None,
        progress_callback: Optional[Callable[[int, str, str], Any]] = None
    ) -> Tuple[bool, str]:
        """
        LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ
        
        Args:
            quantization: ì–‘ìí™” ì˜µì…˜
            cpu_offload: CPU ì˜¤í”„ë¡œë”© ì‚¬ìš© ì—¬ë¶€ (VRAM ì ˆì•½)
            model_path: ì»¤ìŠ¤í…€ ëª¨ë¸ ê²½ë¡œ
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (percent, label, detail)
        
        Returns:
            (success, message)
        """
        async with self._lock:
            if self.pipe is not None:
                return False, "ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ì–¸ë¡œë“œí•˜ì„¸ìš”."
            
            try:
                self.device = self.get_device()
                quant_info = EDIT_QUANTIZATION_OPTIONS.get(quantization)
                
                if not quant_info:
                    return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {quantization}"
                
                repo_id = quant_info["repo"]
                
                # ì§„í–‰ ìƒí™© ì½œë°±
                def report_progress(percent: int, label: str, detail: str = ""):
                    if progress_callback:
                        # async í•¨ìˆ˜ì™€ sync í•¨ìˆ˜ ëª¨ë‘ ì§€ì›
                        if asyncio.iscoroutinefunction(progress_callback):
                            asyncio.create_task(progress_callback(percent, label, detail))
                        else:
                            asyncio.create_task(
                                asyncio.to_thread(progress_callback, percent, label, detail)
                            )
                
                report_progress(5, "ğŸ”§ LongCat-Image-Edit ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...", f"ë””ë°”ì´ìŠ¤: {self.device}")
                
                # LongCat-Image íŒ¨í‚¤ì§€ì—ì„œ ì„í¬íŠ¸
                from transformers import AutoProcessor
                from longcat_image.models import LongCatImageTransformer2DModel
                from longcat_image.pipelines import LongCatImageEditPipeline
                
                checkpoint_dir = model_path if model_path else repo_id
                
                # BF16 ëª¨ë¸ ë¡œë“œ
                report_progress(10, "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"ì €ì¥ì†Œ: {checkpoint_dir}")
                
                # Text Processor ë¡œë“œ
                report_progress(20, "ğŸ”„ Text Processor ë¡œë”© ì¤‘...", "")
                self.text_processor = await asyncio.to_thread(
                    AutoProcessor.from_pretrained,
                    checkpoint_dir,
                    subfolder="tokenizer"
                )
                
                # Transformer ë¡œë“œ
                report_progress(40, "ğŸ”„ Transformer ë¡œë”© ì¤‘...", "ëŒ€ìš©ëŸ‰ ëª¨ë¸ ë¡œë“œ ì¤‘ (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
                self.transformer = await asyncio.to_thread(
                    LongCatImageTransformer2DModel.from_pretrained,
                    checkpoint_dir,
                    subfolder="transformer",
                    torch_dtype=torch.bfloat16,
                    use_safetensors=True
                )
                
                # íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                report_progress(70, "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...", "")
                self.pipe = await asyncio.to_thread(
                    LongCatImageEditPipeline.from_pretrained,
                    checkpoint_dir,
                    transformer=self.transformer,
                    text_processor=self.text_processor,
                    torch_dtype=torch.bfloat16
                )
                
                # ë””ë°”ì´ìŠ¤ ì„¤ì •
                report_progress(85, f"ğŸš€ {self.device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...", "")
                
                if cpu_offload:
                    await asyncio.to_thread(self.pipe.enable_model_cpu_offload)
                    report_progress(95, "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì •ë¨", "VRAM ë¶€ì¡± ì‹œ RAM ì‚¬ìš©")
                else:
                    await asyncio.to_thread(self.pipe.to, self.device, torch.bfloat16)
                
                self.current_model = quantization
                
                report_progress(100, "âœ… LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!", self._get_vram_info())
                
                return True, f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {checkpoint_dir}"
                
            except ImportError as e:
                return False, f"LongCat-Image íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install -e ./LongCat-Image'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”. ì˜¤ë¥˜: {e}"
            except Exception as e:
                # ì‹¤íŒ¨ ì‹œ ì •ë¦¬
                self._cleanup()
                return False, f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    async def unload_model(self) -> Tuple[bool, str]:
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        async with self._lock:
            if self.pipe is None:
                return True, "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
            
            try:
                self._cleanup()
                return True, "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"
            except Exception as e:
                return False, f"ëª¨ë¸ ì–¸ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
    
    def _cleanup(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
        
        if self.transformer is not None:
            del self.transformer
            self.transformer = None
        
        if self.text_processor is not None:
            del self.text_processor
            self.text_processor = None
        
        self.current_model = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
    
    def _get_vram_info(self) -> str:
        """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
        if torch.cuda.is_available():
            vram_used = torch.cuda.memory_allocated() / 1024**3
            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            return f"VRAM: {vram_used:.1f}GB / {vram_total:.1f}GB"
        return "N/A"
    
    def _hook_progress_bar(self, step_callback):
        """íŒŒì´í”„ë¼ì¸ì˜ progress_barë¥¼ í›„í‚¹í•˜ì—¬ ìŠ¤í…ë³„ ì½œë°± í˜¸ì¶œ"""
        from tqdm import tqdm
        import types
        
        pipe = self.pipe
        original_progress_bar = pipe.progress_bar
        
        def hooked_progress_bar(*args, **kwargs):
            # ì›ë˜ progress_bar í˜¸ì¶œ
            pbar = original_progress_bar(*args, **kwargs)
            
            # tqdmì˜ update ë©”ì„œë“œë¥¼ í›„í‚¹
            original_update = pbar.update
            
            def hooked_update(n=1):
                result = original_update(n)
                # ìŠ¤í… ì½œë°± í˜¸ì¶œ
                if step_callback and pbar.total:
                    step_callback(pbar.n, pbar.total)
                return result
            
            pbar.update = hooked_update
            return pbar
        
        # ë©”ì„œë“œ ë°”ì¸ë”©
        pipe.progress_bar = types.MethodType(
            lambda self, *args, **kwargs: hooked_progress_bar(*args, **kwargs),
            pipe
        )
        
        return original_progress_bar
    
    def _restore_progress_bar(self, original_progress_bar):
        """ì›ë˜ progress_bar ë³µì›"""
        import types
        if self.pipe and original_progress_bar:
            self.pipe.progress_bar = types.MethodType(original_progress_bar, self.pipe)
    
    async def edit_image(
        self,
        image: Image.Image,
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = 50,
        guidance_scale: float = 4.5,
        seed: int = -1,
        num_images: int = 1,
        reference_image: Optional[Image.Image] = None,
        progress_callback: Optional[Callable[[int, int, int, int], Any]] = None
    ) -> Tuple[bool, list, str]:
        """
        ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰
        
        Args:
            image: í¸ì§‘í•  ì›ë³¸ ì´ë¯¸ì§€
            prompt: í¸ì§‘ í”„ë¡¬í”„íŠ¸
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_inference_steps: ì¶”ë¡  ìŠ¤í… ìˆ˜
            guidance_scale: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
            seed: ì‹œë“œ (-1ì´ë©´ ëœë¤)
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
            reference_image: ì°¸ì¡° ì´ë¯¸ì§€ (ìŠ¤íƒ€ì¼ ì°¸ì¡°ìš©)
            progress_callback: ì§„í–‰ìƒí™© ì½œë°± (current_image, total_images, current_step, total_steps)
        
        Returns:
            (success, images, message)
        """
        if self.pipe is None:
            return False, [], "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            import random
            
            # RGBë¡œ ë³€í™˜
            if image.mode != "RGB":
                image = image.convert("RGB")
            
            # ì‹œë“œ ì„¤ì •
            if seed == -1:
                seed = random.randint(0, 2147483647)
            
            generator = torch.Generator("cpu").manual_seed(seed)
            
            results = []
            for i in range(num_images):
                current_seed = seed + i
                if i > 0:
                    generator = torch.Generator("cpu").manual_seed(current_seed)
                
                # ìŠ¤í… ì½œë°±ì„ ìœ„í•œ ìƒíƒœ ì €ì¥
                current_image_idx = i
                total_images = num_images
                
                # ìŠ¤í…ë³„ ì½œë°± í•¨ìˆ˜
                def step_callback(current_step, total_steps):
                    if progress_callback:
                        # sync ì½œë°± í˜¸ì¶œ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ)
                        try:
                            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ asyncioë¡œ ì‹¤í–‰
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                asyncio.run_coroutine_threadsafe(
                                    progress_callback(current_image_idx + 1, total_images, current_step, total_steps),
                                    loop
                                )
                        except RuntimeError:
                            pass  # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
                
                # progress_bar í›„í‚¹
                original_progress_bar = self._hook_progress_bar(step_callback)
                
                try:
                    # í¸ì§‘ ì‹¤í–‰
                    def run_edit():
                        return self.pipe(
                            image,
                            prompt,
                            negative_prompt=negative_prompt,
                            guidance_scale=guidance_scale,
                            num_inference_steps=num_inference_steps,
                            num_images_per_prompt=1,
                            generator=generator
                        ).images[0]
                    
                    result_image = await asyncio.to_thread(run_edit)
                finally:
                    # progress_bar ë³µì›
                    self._restore_progress_bar(original_progress_bar)
                
                results.append({
                    "image": result_image,
                    "seed": current_seed
                })
            
            return True, results, f"í¸ì§‘ ì™„ë£Œ (ì‹œë“œ: {seed})"
            
        except Exception as e:
            return False, [], f"í¸ì§‘ ì‹¤íŒ¨: {str(e)}"


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
longcat_edit_manager = LongCatEditManager()

