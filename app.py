"""Z-Image WebUI - FastAPI ê¸°ë°˜ ëŒ€í™”í˜• ì´ë¯¸ì§€ ìƒì„± ì›¹ì•± (ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›)"""

import os
import sys
import json
import asyncio
import base64
import random
import gc
import time
import inspect
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
from io import BytesIO
from contextlib import asynccontextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent
sys.path.insert(0, str(ROOT_DIR))

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File, Form, Response, Cookie, Depends
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.requests import Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
import uvicorn

import torch
from PIL import Image

# ë¡œì»¬ ëª¨ë“ˆ
from config.defaults import (
    QUANTIZATION_OPTIONS,
    RESOLUTION_PRESETS,
    OUTPUTS_DIR,
    MODELS_DIR,
    SERVER_HOST,
    SERVER_PORT,
    SERVER_RELOAD,
    QWEN_EDIT_AUTO_UNLOAD_TIMEOUT,
    QWEN_EDIT_MODEL_VRAM,
    DEFAULT_QWEN_EDIT_SETTINGS,
    DEFAULT_GPU_SETTINGS,
)
from config.templates import PROMPT_TEMPLATES
from utils.settings import settings
from utils.translator import translator
from utils.prompt_enhancer import prompt_enhancer
from utils.metadata import ImageMetadata, filename_generator
from utils.history import get_history_manager_sync, HistoryManager, clear_history_manager_cache
from utils.favorites import get_favorites_manager_sync, FavoritesManager, clear_favorites_manager_cache
from utils.session import session_manager, is_localhost, SessionManager, SessionInfo
from utils.queue_manager import generation_queue, GenerationQueueManager
from utils.auth import auth_manager, User
from utils.api_keys import api_key_manager, APIKey
from utils.qwen_edit import qwen_edit_manager
from utils.edit_history import get_edit_history_manager_sync, EditHistoryManager, clear_edit_history_manager_cache
from utils.edit_llm import edit_translator, edit_enhancer, edit_suggester
from utils.gpu_monitor import gpu_monitor
from services.ws_manager import ws_manager


# ============= ì „ì—­ ë³€ìˆ˜ =============
pipe = None
current_model = None
device = None
last_activity_time = time.time()  # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
auto_unload_task = None  # ìë™ ì–¸ë¡œë“œ ì²´í¬ íƒœìŠ¤í¬
model_lock = asyncio.Lock()  # ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ì ê¸ˆ

# Qwen-Image-Edit ê´€ë ¨
edit_last_activity_time = time.time()  # í¸ì§‘ ëª¨ë¸ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
edit_auto_unload_task = None  # í¸ì§‘ ëª¨ë¸ ìë™ ì–¸ë¡œë“œ íƒœìŠ¤í¬
edit_model_lock = asyncio.Lock()  # í¸ì§‘ ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ì ê¸ˆ


# ============= ëª¨ë¸ë³„ ì˜ˆìƒ VRAM ì‚¬ìš©ëŸ‰ (GB) =============
# ìƒì„± ëª¨ë¸: ì–‘ìí™”ì— ë”°ë¼ ë‹¤ë¦„
GENERATION_MODEL_VRAM = {
    "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)": 14.0,
    "GGUF Q8_0 (7.22GB, ê³ í’ˆì§ˆ)": 7.5,
    "GGUF Q6_K (5.91GB, ê³ í’ˆì§ˆ)": 6.0,
    "GGUF Q5_K_M (5.52GB, ê· í˜•)": 5.8,
    "GGUF Q5_K_S (5.19GB, ê· í˜•)": 5.5,
    "GGUF Q4_K_M (4.98GB, ì¶”ì²œ)": 5.2,
    "GGUF Q4_K_S (4.66GB, ê²½ëŸ‰)": 4.9,
    "GGUF Q3_K_M (4.12GB, ì €ì‚¬ì–‘)": 4.4,
    "GGUF Q3_K_S (3.79GB, ìµœì €ì‚¬ì–‘)": 4.1,
}


# ============= ìë™ ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ í•¨ìˆ˜ =============
async def unload_generation_model_internal():
    """ìƒì„± ëª¨ë¸ ë‚´ë¶€ ì–¸ë¡œë“œ (lock ì—†ì´)"""
    global pipe, current_model, device
    
    if pipe is None:
        return
    
    print("[*] Auto-unloading generation model...")
    
    # GPU ëª¨ë‹ˆí„°ì—ì„œ ëª¨ë¸ ë“±ë¡ í•´ì œ
    gpu_monitor.unregister_model("Z-Image-Turbo")
    
    del pipe
    pipe = None
    old_model = current_model
    current_model = None
    
    # GPU ìºì‹œ ì •ë¦¬
    gpu_monitor.clear_cache(device)
    gc.collect()
    
    # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼
    await ws_manager.broadcast({
        "type": "model_status_change",
        "model_loaded": False,
        "current_model": None
    })
    await ws_manager.broadcast({
        "type": "system",
        "content": f"ğŸ”„ VRAM í™•ë³´ë¥¼ ìœ„í•´ ìƒì„± ëª¨ë¸({old_model})ì´ ìë™ ì–¸ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
    })
    
    print(f"[OK] Generation model auto-unloaded. VRAM: {get_vram_info()}")


async def unload_edit_model_internal():
    """í¸ì§‘ ëª¨ë¸ ë‚´ë¶€ ì–¸ë¡œë“œ (lock ì—†ì´)"""
    if not qwen_edit_manager.is_loaded:
        return
    
    print("[*] Auto-unloading edit model...")
    
    old_model = qwen_edit_manager.current_model
    
    # í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ (ë‚´ë¶€ lock ì‚¬ìš©)
    success, message = await qwen_edit_manager.unload_model()
    
    if success:
        # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼
        await ws_manager.broadcast({
            "type": "edit_model_status_change",
            "model_loaded": False,
            "current_model": None
        })
        await ws_manager.broadcast({
            "type": "system",
            "content": f"ğŸ”„ VRAM í™•ë³´ë¥¼ ìœ„í•´ í¸ì§‘ ëª¨ë¸({old_model})ì´ ìë™ ì–¸ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤."
        })
        
        print(f"[OK] Edit model auto-unloaded. VRAM: {get_vram_info()}")


async def ensure_generation_model_loaded(session_id: str = None) -> tuple[bool, str]:
    """
    ìƒì„± ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìë™ ë¡œë“œ
    VRAMì´ ë¶€ì¡±í•˜ë©´ í¸ì§‘ ëª¨ë¸ì„ ë¨¼ì € ì–¸ë¡œë“œ
    
    Args:
        session_id: ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ì„¸ì…˜ ID (Noneì´ë©´ broadcast)
    
    Returns:
        (success, message)
    """
    global pipe, current_model, device, model_lock
    
    # ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if pipe is not None:
        return True, "ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if model_lock.locked():
        return False, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    async with model_lock:
        # ë‹¤ì‹œ í™•ì¸ (lock ëŒ€ê¸° ì¤‘ ë¡œë“œë˜ì—ˆì„ ìˆ˜ ìˆìŒ)
        if pipe is not None:
            return True, "ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        
        # ì„¤ì •ì—ì„œ ì–‘ìí™” ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
        quantization = settings.get("quantization", "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)")
        cpu_offload = settings.get("cpu_offload", False)
        target_device_setting = settings.get("generation_gpu", DEFAULT_GPU_SETTINGS["generation_gpu"])
        
        # í•„ìš”í•œ VRAM ê³„ì‚°
        required_vram = GENERATION_MODEL_VRAM.get(quantization, 14.0)
        
        # í˜„ì¬ VRAM ì—¬ìœ  í™•ì¸
        resolved_device = get_device(target_device_setting)
        free_vram = gpu_monitor.get_free_vram_gb(resolved_device)
        
        async def send_message(msg: str, msg_type: str = "system"):
            if session_id:
                await ws_manager.send_to_session(session_id, {"type": msg_type, "content": msg})
            else:
                await ws_manager.broadcast({"type": msg_type, "content": msg})
        
        # VRAMì´ ë¶€ì¡±í•˜ë©´ í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ
        if not gpu_monitor.has_enough_vram(required_vram, resolved_device):
            if qwen_edit_manager.is_loaded:
                await send_message(f"âš ï¸ VRAM ë¶€ì¡± ({free_vram:.1f}GB < {required_vram:.1f}GB). í¸ì§‘ ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤...")
                await unload_edit_model_internal()
                
                # ì–¸ë¡œë“œ í›„ VRAM ì¬í™•ì¸
                await asyncio.sleep(0.5)  # GPU ìºì‹œ ì •ë¦¬ ëŒ€ê¸°
                free_vram = gpu_monitor.get_free_vram_gb(resolved_device)
        
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ê²½ê³ ë§Œ í•˜ê³  ì§„í–‰ (CPU ì˜¤í”„ë¡œë”© ê°€ëŠ¥)
        if not gpu_monitor.has_enough_vram(required_vram, resolved_device):
            await send_message(f"âš ï¸ VRAMì´ ì—¬ì „íˆ ë¶€ì¡±í•©ë‹ˆë‹¤ ({free_vram:.1f}GB). CPU ì˜¤í”„ë¡œë”©ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
            cpu_offload = True
        
        await send_message(f"ğŸ”„ ìƒì„± ëª¨ë¸ ìë™ ë¡œë“œ ì¤‘... ({quantization})")
        
        # ëª¨ë¸ ë¡œë“œ ì§„í–‰
        try:
            device = get_device(target_device_setting)
            
            quant_info = QUANTIZATION_OPTIONS.get(quantization)
            if not quant_info:
                return False, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {quantization}"
            
            repo_id = quant_info["repo"]
            dtype = quant_info["type"]
            is_gguf = quant_info.get("is_gguf", False)
            
            # ì§„í–‰ ìƒí™© ë¸Œë¡œë“œìºìŠ¤íŠ¸
            async def progress(percent, label, detail=""):
                await ws_manager.broadcast({
                    "type": "model_progress",
                    "progress": percent,
                    "label": label,
                    "detail": detail,
                    "stage": "loading" if percent < 100 else "complete"
                })
            
            await progress(5, "ğŸ”§ ëª¨ë¸ ìë™ ë¡œë“œ ì‹œì‘...", f"ì–‘ìí™”: {dtype}")
            
            from diffusers import ZImagePipeline
            
            if is_gguf:
                from diffusers import ZImageTransformer2DModel, GGUFQuantizationConfig
                from huggingface_hub import hf_hub_download
                
                filename = quant_info["filename"]
                
                await progress(15, "ğŸ“¥ GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"íŒŒì¼: {filename}")
                
                gguf_path = await asyncio.to_thread(
                    hf_hub_download,
                    repo_id=repo_id,
                    filename=filename,
                    cache_dir=str(MODELS_DIR)
                )
                
                await progress(35, "ğŸ”„ GGUF Transformer ë¡œë”© ì¤‘...", f"ì–‘ìí™” íƒ€ì…: {dtype}")
                
                transformer = await asyncio.to_thread(
                    ZImageTransformer2DModel.from_single_file,
                    gguf_path,
                    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
                    torch_dtype=torch.bfloat16,
                )
                
                await progress(60, "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...", "")
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    "Tongyi-MAI/Z-Image-Turbo",
                    transformer=transformer,
                    torch_dtype=torch.bfloat16,
                )
            else:
                await progress(15, "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...", f"ì €ì¥ì†Œ: {repo_id}")
                
                load_kwargs = {
                    "torch_dtype": torch.bfloat16,
                    "cache_dir": str(MODELS_DIR),
                }
                
                await progress(35, "ğŸ”„ ëª¨ë¸ íŒŒì¼ ë¡œë”© ì¤‘...", "")
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    repo_id,
                    **load_kwargs
                )
            
            await progress(80, f"ğŸš€ {device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...", "")
            
            if cpu_offload:
                await asyncio.to_thread(pipe.enable_model_cpu_offload)
            else:
                await asyncio.to_thread(pipe.to, device)
            
            current_model = quantization
            
            # GPU ëª¨ë‹ˆí„°ì— ëª¨ë¸ ë“±ë¡
            gpu_monitor.register_model("Z-Image-Turbo", device)
            
            await progress(100, "âœ… ëª¨ë¸ ìë™ ë¡œë“œ ì™„ë£Œ!", f"VRAM: {get_vram_info()}")
            
            await ws_manager.broadcast({
                "type": "model_status_change",
                "model_loaded": True,
                "current_model": current_model,
                "device": device
            })
            
            return True, f"ìƒì„± ëª¨ë¸ ìë™ ë¡œë“œ ì™„ë£Œ: {quantization}"
            
        except Exception as e:
            await ws_manager.broadcast({
                "type": "model_progress",
                "progress": 0,
                "label": "âŒ ìë™ ë¡œë“œ ì‹¤íŒ¨",
                "detail": str(e),
                "stage": "error"
            })
            return False, f"ìƒì„± ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"


async def ensure_edit_model_loaded(session_id: str = None) -> tuple[bool, str]:
    """
    í¸ì§‘ ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ìë™ ë¡œë“œ
    VRAMì´ ë¶€ì¡±í•˜ë©´ ìƒì„± ëª¨ë¸ì„ ë¨¼ì € ì–¸ë¡œë“œ
    
    Args:
        session_id: ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ì„¸ì…˜ ID (Noneì´ë©´ broadcast)
    
    Returns:
        (success, message)
    """
    global pipe, current_model, edit_model_lock
    
    # ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if qwen_edit_manager.is_loaded:
        return True, "í¸ì§‘ ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if edit_model_lock.locked():
        return False, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ í¸ì§‘ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    async with edit_model_lock:
        # ë‹¤ì‹œ í™•ì¸ (lock ëŒ€ê¸° ì¤‘ ë¡œë“œë˜ì—ˆì„ ìˆ˜ ìˆìŒ)
        if qwen_edit_manager.is_loaded:
            return True, "í¸ì§‘ ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        
        # ì„¤ì •ì—ì„œ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸° (Qwenì€ 4bit NF4 ê³ ì •)
        cpu_offload = settings.get("edit_cpu_offload", True)
        target_device_setting = settings.get("edit_gpu", DEFAULT_GPU_SETTINGS["edit_gpu"])

        # í•„ìš”í•œ VRAM ê³„ì‚° (Qwen-Image-Edit 4bit: ~16GB with CPU offload)
        required_vram = QWEN_EDIT_MODEL_VRAM
        
        # í˜„ì¬ VRAM ì—¬ìœ  í™•ì¸
        resolved_device = qwen_edit_manager.get_device(target_device_setting)
        free_vram = gpu_monitor.get_free_vram_gb(resolved_device)
        
        async def send_message(msg: str, msg_type: str = "edit_system"):
            if session_id:
                await ws_manager.send_to_session(session_id, {"type": msg_type, "content": msg})
            else:
                await ws_manager.broadcast({"type": msg_type, "content": msg})
        
        # VRAMì´ ë¶€ì¡±í•˜ë©´ ìƒì„± ëª¨ë¸ ì–¸ë¡œë“œ
        if not gpu_monitor.has_enough_vram(required_vram, resolved_device):
            if pipe is not None:
                # ìƒì„± ëª¨ë¸ lock í™•ì¸
                if model_lock.locked():
                    return False, "ìƒì„± ëª¨ë¸ì´ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                
                async with model_lock:
                    await send_message(f"âš ï¸ VRAM ë¶€ì¡± ({free_vram:.1f}GB < {required_vram:.1f}GB). ìƒì„± ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤...")
                    await unload_generation_model_internal()
                
                # ì–¸ë¡œë“œ í›„ VRAM ì¬í™•ì¸
                await asyncio.sleep(0.5)  # GPU ìºì‹œ ì •ë¦¬ ëŒ€ê¸°
                free_vram = gpu_monitor.get_free_vram_gb(resolved_device)
        
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ê²½ê³ ë§Œ í•˜ê³  ì§„í–‰ (CPU ì˜¤í”„ë¡œë”© í™œì„±í™”)
        if not gpu_monitor.has_enough_vram(required_vram, resolved_device):
            await send_message(f"âš ï¸ VRAMì´ ì—¬ì „íˆ ë¶€ì¡±í•©ë‹ˆë‹¤ ({free_vram:.1f}GB). CPU ì˜¤í”„ë¡œë”©ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
            cpu_offload = True
        
        await send_message("ğŸ”„ Qwen-Image-Edit ëª¨ë¸ ìë™ ë¡œë“œ ì¤‘... (NF4 4bit)")

        # ì§„í–‰ ìƒí™© ì½œë°±
        async def progress_callback(percent, label, detail):
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": percent,
                "label": label,
                "detail": detail,
                "stage": "loading" if percent < 100 else "complete"
            })

        # ëª¨ë¸ ë¡œë“œ
        success, message = await qwen_edit_manager.load_model(
            cpu_offload=cpu_offload,
            target_device=target_device_setting,
            progress_callback=progress_callback
        )
        
        if success:
            await ws_manager.broadcast({
                "type": "edit_model_status_change",
                "model_loaded": True,
                "current_model": qwen_edit_manager.current_model,
                "device": qwen_edit_manager.device
            })
        
        return success, message


# ============= ìë™ ì–¸ë¡œë“œ ê´€ë ¨ í•¨ìˆ˜ =============
def update_activity():
    """ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸"""
    global last_activity_time
    last_activity_time = time.time()


async def auto_unload_checker():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ ì–¸ë¡œë“œ ì²´í¬"""
    global pipe, current_model, last_activity_time
    
    while True:
        await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
        
        # ìë™ ì–¸ë¡œë“œ ì„¤ì • í™•ì¸
        if not settings.get("auto_unload_enabled", True):
            continue
        
        # ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
        if pipe is None:
            continue
        
        # ìƒì„± ì¤‘ì´ë©´ ìŠ¤í‚µ
        if generation_queue.is_processing():
            update_activity()  # ìƒì„± ì¤‘ì—ëŠ” í™œë™ìœ¼ë¡œ ê°„ì£¼
            continue
        
        # íƒ€ì„ì•„ì›ƒ ì²´í¬
        timeout_minutes = settings.get("auto_unload_timeout", 10)
        timeout_seconds = timeout_minutes * 60
        elapsed = time.time() - last_activity_time
        
        if elapsed >= timeout_seconds:
            print(f"â° ìë™ ì–¸ë¡œë“œ: {timeout_minutes}ë¶„ ë™ì•ˆ í™œë™ì´ ì—†ì–´ ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤.")
            
            try:
                # GPU ëª¨ë‹ˆí„°ì—ì„œ ëª¨ë¸ ë“±ë¡ í•´ì œ
                gpu_monitor.unregister_model("Z-Image-Turbo")
                
                # ëª¨ë¸ ì–¸ë¡œë“œ
                del pipe
                pipe = None
                current_model = None
                
                # GPU ìºì‹œ ì •ë¦¬ (ìƒì„± ëª¨ë¸ì´ ì˜¬ë ¤ì ¸ ìˆë˜ ë””ë°”ì´ìŠ¤ë§Œ ì •ë¦¬)
                # - í¸ì§‘ ëª¨ë¸ì´ ë‹¤ë¥¸ GPUì— ì˜¬ë¼ê°„ ê²½ìš°ê¹Œì§€ ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë²”ìœ„ë¥¼ ì œí•œí•œë‹¤.
                gpu_monitor.clear_cache(device)
                gc.collect()
                
                # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼
                await ws_manager.broadcast({
                    "type": "system",
                    "content": f"â° {timeout_minutes}ë¶„ ë™ì•ˆ í™œë™ì´ ì—†ì–´ ëª¨ë¸ì´ ìë™ ì–¸ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. VRAMì„ ì ˆì•½í•©ë‹ˆë‹¤."
                })
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 100, 
                    "label": "â° ìë™ ì–¸ë¡œë“œ ì™„ë£Œ",
                    "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}",
                    "stage": "complete"
                })
                
                print(f"[OK] Auto-unload complete. VRAM: {get_vram_info()}")
                
            except Exception as e:
                print(f"[ERR] Auto-unload failed: {e}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” lifespan í•¸ë“¤ëŸ¬"""
    global auto_unload_task
    
    # ì‹œì‘ ì‹œ: ìë™ ì–¸ë¡œë“œ ì²´í¬ íƒœìŠ¤í¬ ì‹œì‘
    auto_unload_task = asyncio.create_task(auto_unload_checker())
    print("[*] Auto unload checker started")
    
    # í ì›Œì»¤ ì‹œì‘
    await generation_queue.start_worker()
    print("[*] Image generation queue worker started")
    
    # í ì½œë°± ì„¤ì •
    generation_queue.set_callbacks(
        on_status_change=on_queue_status_change,
        on_broadcast=on_queue_broadcast,
        generate_func=execute_generation
    )
    
    yield
    
    # ì¢…ë£Œ ì‹œ: íƒœìŠ¤í¬ ì·¨ì†Œ
    if auto_unload_task:
        auto_unload_task.cancel()
        try:
            await auto_unload_task
        except asyncio.CancelledError:
            pass
    
    # í ì›Œì»¤ ì¤‘ì§€
    await generation_queue.stop_worker()


# ============= FastAPI ì•± ì„¤ì • =============
app = FastAPI(
    title="Z-Image WebUI", 
    version="2.0.0", 
    lifespan=lifespan,
    swagger_ui_parameters={"persistAuthorization": True}  # ì¸ì¦ ì •ë³´ ìœ ì§€
)


# OpenAPI ìŠ¤í‚¤ë§ˆì— API í‚¤ ì¸ì¦ ì¶”ê°€
def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    
    from fastapi.openapi.utils import get_openapi
    openapi_schema = get_openapi(
        title=app.title,
        version=app.version,
        routes=app.routes,
    )
    
    # securitySchemes ì¶”ê°€
    openapi_schema["components"]["securitySchemes"] = {
        "APIKeyAuth": {
            "type": "http",
            "scheme": "bearer",
            "bearerFormat": "API Key",
            "description": "API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (zimg_ë¡œ ì‹œì‘í•˜ëŠ” í‚¤). ì„¤ì • > API í‚¤ ê´€ë¦¬ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }
    }
    
    # API í‚¤ ì¸ì¦ì´ í•„ìš”í•œ ì—”ë“œí¬ì¸íŠ¸ì— security ì„¤ì • ì¶”ê°€
    api_key_endpoints = [
        "/api/instant-generate",
        "/api/generate",
        "/api/edit/generate",
    ]
    
    for path_key, path_item in openapi_schema.get("paths", {}).items():
        if path_key in api_key_endpoints:
            for method in path_item.values():
                if isinstance(method, dict):
                    method["security"] = [{"APIKeyAuth": []}]
    
    app.openapi_schema = openapi_schema
    return app.openapi_schema


app.openapi = custom_openapi

# ì •ì  íŒŒì¼ ë° í…œí”Œë¦¿
app.mount("/static", StaticFiles(directory=ROOT_DIR / "static"), name="static")
templates = Jinja2Templates(directory=ROOT_DIR / "templates")


# ============= ë¼ìš°í„° ë“±ë¡ =============
from routers import auth, history, gallery, settings_router, admin

app.include_router(auth.router, prefix="/api/auth", tags=["ì¸ì¦"])
app.include_router(history.router, prefix="/api", tags=["íˆìŠ¤í† ë¦¬"])
app.include_router(gallery.router, prefix="/api", tags=["ê°¤ëŸ¬ë¦¬"])
app.include_router(settings_router.router, prefix="/api", tags=["ì„¤ì •"])
app.include_router(admin.router, prefix="/api/admin", tags=["ê´€ë¦¬ì"])


# ============= Pydantic ëª¨ë¸ =============
class GenerateRequest(BaseModel):
    prompt: str
    korean_prompt: str = ""  # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ (ì›ë³¸)
    width: int = 512
    height: int = 512
    steps: int = 8
    guidance_scale: float = 0.0
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


class ModelLoadRequest(BaseModel):
    quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"
    cpu_offload: bool = False
    target_device: str = "auto"  # ê´€ë¦¬ì ì „ìš©: "auto", "cuda:0", "cuda:1", "cpu", "mps"


class SettingsRequest(BaseModel):
    openai_api_key: str = ""  # ë ˆê±°ì‹œ í˜¸í™˜
    output_path: str = ""
    filename_pattern: str = "{date}_{time}_{seed}"
    # LLM Provider ì„¤ì •
    llm_provider: str = ""
    llm_api_key: str = ""
    # NOTE:
    # - /api/settings ëŠ” ë‹¤ì–‘í•œ ì„¤ì •(ìë™ ì–¸ë¡œë“œ ë“±) ì €ì¥ì—ë„ ì¬ì‚¬ìš©ëœë‹¤.
    # - ì•„ë˜ ê°’ë“¤ì„ ê¸°ë³¸ê°’ ""ë¡œ ë‘ë©´, ìš”ì²­ ë°”ë””ì— í•´ë‹¹ í•„ë“œê°€ ì—†ì–´ë„ Pydanticì´ ""ë¥¼ ì±„ì›Œë„£ì–´
    #   ì €ì¥ ì‹œ ê¸°ì¡´ ê°’ì´ ""ë¡œ ë®ì—¬ì„œ "ì„¤ì •ì´ í’€ë¦¬ëŠ”" ë¬¸ì œê°€ ë°œìƒí•œë‹¤.
    # - ë”°ë¼ì„œ Optionalë¡œ ë‘ê³ , ì‹¤ì œë¡œ ê°’ì´ ì „ë‹¬ëœ ê²½ìš°ì—ë§Œ(= Noneì´ ì•„ë‹ ë•Œë§Œ) ì €ì¥í•œë‹¤.
    llm_base_url: Optional[str] = None
    llm_model: Optional[str] = None
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²ˆì—­/í–¥ìƒ)
    translate_system_prompt: Optional[str] = None
    enhance_system_prompt: Optional[str] = None
    # ìë™ ì–¸ë¡œë“œ ì„¤ì •
    auto_unload_enabled: Optional[bool] = None
    auto_unload_timeout: Optional[int] = None
    # í¸ì§‘ ëª¨ë¸ ìë™ ì–¸ë¡œë“œ ì„¤ì •
    edit_auto_unload_enabled: Optional[bool] = None
    edit_auto_unload_timeout: Optional[int] = None

    # ëª¨ë¸ ì„¤ì • (ê´€ë¦¬ì ì „ìš©)
    quantization: Optional[str] = None
    cpu_offload: Optional[bool] = None
    # í¸ì§‘ ëª¨ë¸ ì„¤ì • (ê´€ë¦¬ì ì „ìš©) - Qwenì€ 4bit NF4 ê³ ì •
    edit_cpu_offload: Optional[bool] = None


class FavoriteRequest(BaseModel):
    name: str
    prompt: str
    settings: dict = {}


class TranslateRequest(BaseModel):
    text: str


class EnhanceRequest(BaseModel):
    prompt: str
    style: str = "ê¸°ë³¸"


class ConversationUpdateRequest(BaseModel):
    conversation: List[Dict[str, Any]]


# ============= í¸ì§‘ ê´€ë ¨ Pydantic ëª¨ë¸ =============
class EditModelLoadRequest(BaseModel):
    model_path: str = ""
    cpu_offload: bool = True  # ê¸°ë³¸ í™œì„±í™” (VRAM ì ˆì•½, ~16GB)
    target_device: str = "auto"  # ê´€ë¦¬ì ì „ìš©: "auto", "cuda:0", "cuda:1", "cpu", "mps"


class EditGenerateRequest(BaseModel):
    prompt: str
    negative_prompt: str = " "  # Qwenì€ negative prompt ì§€ì›
    korean_prompt: str = ""
    steps: int = 20  # Qwen ê¸°ë³¸ê°’
    true_cfg_scale: float = 4.0  # Qwen ì „ìš©: í”„ë¡¬í”„íŠ¸ ì¶©ì‹¤ë„
    guidance_scale: float = 1.0  # Qwen ê¸°ë³¸ê°’
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


class EditTranslateRequest(BaseModel):
    text: str


class EditEnhanceRequest(BaseModel):
    instruction: str


class EditSuggestRequest(BaseModel):
    context: str = ""
    image_description: str = ""


class EditConversationUpdateRequest(BaseModel):
    conversation: List[Dict[str, Any]]


# ============= ì¸ì¦ ê´€ë ¨ Pydantic ëª¨ë¸ =============
class RegisterRequest(BaseModel):
    username: str
    password: str
    password_confirm: str


class LoginRequest(BaseModel):
    username: str
    password: str


class ChangePasswordRequest(BaseModel):
    current_password: str
    new_password: str
    new_password_confirm: str


class ResetPasswordRequest(BaseModel):
    new_password: Optional[str] = None  # Noneì´ë©´ ì„ì‹œ ë¹„ë°€ë²ˆí˜¸ ìë™ ìƒì„±


# ============= ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =============
def get_device(target_device: str = "auto") -> str:
    """
    ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ë°˜í™˜
    
    Args:
        target_device: ëª©í‘œ ë””ë°”ì´ìŠ¤ ("auto", "cuda:0", "cuda:1", "cpu", "mps")
    
    Returns:
        ì‹¤ì œ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    """
    return gpu_monitor.resolve_device(target_device, prefer_empty=True)


def image_to_base64(image: Image.Image) -> str:
    """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode()


def get_vram_info() -> str:
    """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
    return gpu_monitor.get_vram_summary()


async def get_session_from_request(request: Request, create_if_missing: bool = False) -> Optional[SessionInfo]:
    """
    ìš”ì²­ì—ì„œ ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°
    - ê¸°ë³¸: **ë¹„ë¡œê·¸ì¸(ì¿ í‚¤ ì—†ìŒ/ìœ íš¨í•˜ì§€ ì•ŠìŒ/ë©”ëª¨ë¦¬ì— ì—†ìŒ)ì—ì„œëŠ” ì„¸ì…˜ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ**
    - ë¡œê·¸ì¸/íšŒì›ê°€ì… ë“± ì¼ë¶€ ì—”ë“œí¬ì¸íŠ¸ì—ì„œë§Œ create_if_missing=Trueë¡œ ì„¸ì…˜ ìƒì„±
    """
    session_id = request.cookies.get(SessionManager.COOKIE_NAME)

    # ì¿ í‚¤ê°€ ìˆê³  ë©”ëª¨ë¦¬ì— ì‚´ì•„ìˆëŠ” ì„¸ì…˜ì´ë©´ ë°˜í™˜
    if session_id and session_manager.validate_session_id(session_id):
        session = session_manager.get_session(session_id)
        if session:
            session.update_activity()
            return session

    # í•„ìš”í•œ ê²½ìš°ì—ë§Œ ìƒˆ ì„¸ì…˜ ìƒì„±
    if create_if_missing:
        return await session_manager.get_or_create_session(session_id)

    return None


def require_auth(session: Optional[SessionInfo]) -> None:
    """ì¸ì¦ í•„ìˆ˜ ì²´í¬ - ë¡œê·¸ì¸í•˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ"""
    if not session or not session.is_authenticated:
        raise HTTPException(401, "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")


# API í‚¤ ì¸ì¦ì„ ìœ„í•œ ë³´ì•ˆ ìŠ¤í‚¤ë§ˆ (Swagger docsì—ì„œ ì‚¬ìš©)
api_key_scheme = HTTPBearer(
    scheme_name="API Key",
    description="API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (zimg_ë¡œ ì‹œì‘í•˜ëŠ” í‚¤)",
    auto_error=False  # ì¸ì¦ ì‹¤íŒ¨ ì‹œ ìë™ ì—ëŸ¬ ë°œìƒ ì•ˆ í•¨ (ì„¸ì…˜ ì¸ì¦ í´ë°± í—ˆìš©)
)


async def get_api_key_auth(
    credentials: Optional[HTTPAuthorizationCredentials] = Depends(api_key_scheme)
) -> Optional[str]:
    """Swagger docsì—ì„œ API í‚¤ ì¸ì¦ì„ ìœ„í•œ ì˜ì¡´ì„±"""
    if credentials:
        return credentials.credentials
    return None


def get_api_key_from_request(request: Request) -> Optional[str]:
    """ìš”ì²­ì—ì„œ API í‚¤ ì¶”ì¶œ (Authorization: Bearer <api_key>)"""
    auth_header = request.headers.get("Authorization")
    if auth_header and auth_header.startswith("Bearer "):
        return auth_header[7:]
    return None


async def get_auth_from_request(request: Request) -> Dict[str, Any]:
    """
    ìš”ì²­ì—ì„œ ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (API í‚¤ ë˜ëŠ” ì„¸ì…˜)
    
    Returns:
        {"type": "api_key", "api_key": APIKey} ë˜ëŠ”
        {"type": "session", "session": SessionInfo} ë˜ëŠ”
        ì˜ˆì™¸ ë°œìƒ
    """
    # 1. Authorization í—¤ë”ì—ì„œ API í‚¤ í™•ì¸
    api_key_str = get_api_key_from_request(request)
    if api_key_str:
        is_valid, api_key_obj = api_key_manager.validate_api_key(api_key_str)
        if is_valid and api_key_obj:
            return {"type": "api_key", "api_key": api_key_obj}
        raise HTTPException(401, "ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤.")
    
    # 2. ê¸°ì¡´ ì„¸ì…˜ ì¸ì¦ìœ¼ë¡œ í´ë°±
    session = await get_session_from_request(request)
    if session and session.is_authenticated:
        return {"type": "session", "session": session}
    
    raise HTTPException(401, "ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¡œê·¸ì¸í•˜ê±°ë‚˜ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")


async def require_auth_or_api_key(request: Request) -> Dict[str, Any]:
    """ì¸ì¦ í•„ìˆ˜ ì²´í¬ - ì„¸ì…˜ ë˜ëŠ” API í‚¤ ì¤‘ í•˜ë‚˜ê°€ ìˆì–´ì•¼ í•¨"""
    return await get_auth_from_request(request)


def require_admin(request: Request) -> None:
    """ê´€ë¦¬ì ê¶Œí•œ ì²´í¬ - localhostê°€ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ"""
    client_host = request.client.host if request.client else None
    if not is_localhost(client_host):
        raise HTTPException(403, "ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")


def set_session_cookie(response: Response, session: Optional[SessionInfo]):
    """ì‘ë‹µì— ì„¸ì…˜ ì¿ í‚¤ ì„¤ì •"""
    if not session:
        return
    response.set_cookie(
        key=SessionManager.COOKIE_NAME,
        value=session.session_id,
        max_age=SessionManager.COOKIE_MAX_AGE,
        httponly=True,
        samesite="lax"
    )


def clear_session_cookie(response: Response):
    """ì„¸ì…˜ ì¿ í‚¤ ì œê±°"""
    response.delete_cookie(key=SessionManager.COOKIE_NAME)


# ============= WebSocket ê´€ë¦¬ìëŠ” services.ws_managerì—ì„œ ê°€ì ¸ì˜´ =============
# from services.ws_manager import ws_manager (ìƒë‹¨ì—ì„œ import)


def _format_bytes(total_size: int) -> str:
    """ë°”ì´íŠ¸ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë‹¨ìœ„ë¡œ ë³€í™˜"""
    if total_size < 1024:
        return f"{total_size} B"
    if total_size < 1024 * 1024:
        return f"{total_size / 1024:.1f} KB"
    if total_size < 1024 * 1024 * 1024:
        return f"{total_size / (1024 * 1024):.1f} MB"
    return f"{total_size / (1024 * 1024 * 1024):.2f} GB"


def _get_data_size_by_data_id(data_id: str) -> str:
    """data_id(user_{id}) ê¸°ì¤€ ë°ì´í„° í¬ê¸° ê³„ì‚° (ì„¸ì…˜ í™”ë©´ìš©)"""
    from config.defaults import DATA_DIR, OUTPUTS_DIR
    total_size = 0
    sessions_dir = DATA_DIR / "sessions" / data_id
    outputs_dir = OUTPUTS_DIR / data_id

    for d in (sessions_dir, outputs_dir):
        if d.exists():
            for f in d.rglob("*"):
                if f.is_file():
                    try:
                        total_size += f.stat().st_size
                    except Exception:
                        pass
    return _format_bytes(total_size)


def _parse_user_id_from_data_id(data_id: str) -> Optional[int]:
    """user_123 -> 123"""
    if not isinstance(data_id, str):
        return None
    if not data_id.startswith("user_"):
        return None
    try:
        return int(data_id.split("_", 1)[1])
    except Exception:
        return None


# ============= í ì½œë°± í•¨ìˆ˜ë“¤ =============
async def on_queue_status_change(session_id: str, event_type: str, data: dict):
    """í ìƒíƒœ ë³€ê²½ ì‹œ ì„¸ì…˜ì— ì•Œë¦¼"""
    if event_type == "generation_start":
        await ws_manager.send_to_session(session_id, {
            "type": "queue_status",
            "status": "processing",
            "position": 0,
            "message": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
        })
    elif event_type == "queue_position":
        await ws_manager.send_to_session(session_id, {
            "type": "queue_status",
            "status": "waiting",
            "position": data["position"],
            "message": f"â³ ëŒ€ê¸° ì¤‘... (ìˆœì„œ: {data['position']})"
        })
    elif event_type == "generation_error":
        await ws_manager.send_to_session(session_id, {
            "type": "error",
            "content": f"âŒ ìƒì„± ì˜¤ë¥˜: {data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        })
    elif event_type == "generation_complete":
        # ê²°ê³¼ëŠ” execute_generationì—ì„œ ì§ì ‘ ì „ì†¡
        pass


async def on_queue_broadcast(data: dict):
    """í ìƒíƒœ ì „ì²´ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
    await ws_manager.broadcast(data)


async def execute_generation(request_data: dict) -> dict:
    """ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰"""
    global pipe, current_model
    
    session_id = request_data.get("session_id")
    # ê³„ì • ë‹¨ìœ„(data_id)ë¡œ ì‹¤í–‰ ì‹œ SessionInfoê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜µì…˜ ì²˜ë¦¬
    session = session_manager.get_session(session_id)
    
    if pipe is None:
        raise Exception("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    prompt = request_data.get("prompt", "")
    korean_prompt = request_data.get("korean_prompt", "")
    width = request_data.get("width", 512)
    height = request_data.get("height", 512)
    steps = request_data.get("steps", 8)
    guidance_scale = request_data.get("guidance_scale", 0.0)
    seed = request_data.get("seed", -1)
    num_images = request_data.get("num_images", 1)
    auto_translate = request_data.get("auto_translate", True)
    
    # ë²ˆì—­
    final_prompt = prompt
    if auto_translate and translator.is_korean(prompt):
        await ws_manager.send_to_session(session_id, {
            "type": "system",
            "content": "ğŸŒ í”„ë¡¬í”„íŠ¸ ë²ˆì—­ ì¤‘..."
        })
        final_prompt, success = translator.translate(prompt)
        if not success:
            await ws_manager.send_to_session(session_id, {
                "type": "warning",
                "content": "âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"
            })
    
    # ì‹œë“œ ì„¤ì •
    if seed == -1:
        seed = random.randint(0, 2147483647)
    
    # ìƒì„± ì‹œì‘ ë©”ì‹œì§€
    await ws_manager.send_to_session(session_id, {
        "type": "system",
        "content": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘..."
    })
    
    # ìƒì„± ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    
    # ì„¸ì…˜ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
    if session:
        outputs_dir = session.get_outputs_dir()
    else:
        # session_idê°€ user_{id} í˜•íƒœë©´ outputs/user_{id}ì— ì €ì¥
        if isinstance(session_id, str) and session_id.startswith("user_"):
            outputs_dir = OUTPUTS_DIR / session_id
        else:
            outputs_dir = OUTPUTS_DIR
    
    images = []
    for i in range(num_images):
        current_seed = seed + i
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
        percent = ((i) / num_images) * 100
        await ws_manager.send_to_session(session_id, {
            "type": "image_progress",
            "progress": percent,
            "current": i + 1,
            "total": num_images
        })
        await asyncio.sleep(0.05)
        
        generator = torch.Generator(device).manual_seed(current_seed)
        loop = asyncio.get_running_loop()
        last_sent_step = {"value": -1}  # í´ë¡œì €ì—ì„œ mutableë¡œ ì‚¬ìš©

        def _send_generation_progress_from_thread(current_step: int, total_steps: int):
            """diffusers ì½œë°±(ë³„ë„ ìŠ¤ë ˆë“œ)ì—ì„œ WebSocket ì§„í–‰ìƒí™© ì „ì†¡"""
            # ë„ˆë¬´ ì¦ì€ ì¤‘ë³µ ì „ì†¡ ë°©ì§€
            if current_step == last_sent_step["value"]:
                return
            last_sent_step["value"] = current_step

            # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚° (ì´ë¯¸ì§€ + ìŠ¤í… ê¸°ì¤€)
            image_progress = (i) / num_images
            step_progress = (current_step / max(total_steps, 1)) / num_images
            overall_progress = int((image_progress + step_progress) * 100)

            payload = {
                "type": "generation_progress",
                "current_image": i + 1,
                "total_images": num_images,
                "current_step": current_step,
                "total_steps": total_steps,
                "progress": overall_progress,
            }

            try:
                fut = asyncio.run_coroutine_threadsafe(
                    ws_manager.send_to_session(session_id, payload),
                    loop
                )
                # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì‘ì—…ì„ ê¹¨ì§€ ì•Šë„ë¡ í¡ìˆ˜
                fut.add_done_callback(lambda f: f.exception())
            except Exception:
                pass
        
        # ë™ê¸° pipe í˜¸ì¶œì„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        def run_pipe():
            call_sig = inspect.signature(pipe.__call__)

            # diffusers ìµœì‹ : callback_on_step_end ì§€ì›
            if "callback_on_step_end" in call_sig.parameters:
                def callback_on_step_end(_pipeline, step_index, _timestep, callback_kwargs):
                    # step_indexëŠ” 0-basedì¸ ê²½ìš°ê°€ ëŒ€ë¶€ë¶„
                    _send_generation_progress_from_thread(step_index + 1, steps)
                    return callback_kwargs

                return pipe(
                    prompt=final_prompt,
                    height=height,
                    width=width,
                    num_inference_steps=steps,
                    guidance_scale=guidance_scale,
                    generator=generator,
                    callback_on_step_end=callback_on_step_end,
                ).images[0]

            # diffusers êµ¬ë²„ì „: callback/callback_steps ì§€ì›
            if "callback" in call_sig.parameters:
                def callback(step_index, _timestep, _latents):
                    _send_generation_progress_from_thread(step_index + 1, steps)

                return pipe(
                    prompt=final_prompt,
                    height=height,
                    width=width,
                    num_inference_steps=steps,
                    guidance_scale=guidance_scale,
                    generator=generator,
                    callback=callback,
                    callback_steps=1,
                ).images[0]

            # ì½œë°± ë¯¸ì§€ì›(ì˜ˆì™¸ ì¼€ì´ìŠ¤): ê¸°ì¡´ ë™ì‘
            return pipe(
                prompt=final_prompt,
                height=height,
                width=width,
                num_inference_steps=steps,
                guidance_scale=guidance_scale,
                generator=generator,
            ).images[0]
        
        image = await asyncio.to_thread(run_pipe)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥
        metadata = ImageMetadata.create_metadata(
            prompt=final_prompt,
            seed=current_seed,
            width=width,
            height=height,
            steps=steps,
            guidance_scale=guidance_scale,
            model=current_model or "unknown",
        )
        
        outputs_dir.mkdir(parents=True, exist_ok=True)
        filename = filename_generator.generate(
            pattern=settings.get("filename_pattern", "{date}_{time}_{seed}"),
            prompt=final_prompt,
            seed=current_seed
        )
        output_path = outputs_dir / filename
        ImageMetadata.save_with_metadata(image, output_path, metadata)
        
        images.append({
            "base64": image_to_base64(image),
            "filename": filename,
            "seed": current_seed,
            "path": (
                f"/outputs/{session.data_id}/{filename}"
                if session
                else (f"/outputs/{session_id}/{filename}" if isinstance(session_id, str) and session_id.startswith("user_") else f"/outputs/{filename}")
            )
        })
        
        # ê° ì´ë¯¸ì§€ ìƒì„± í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì—¬ëŸ¬ ì¥ ìƒì„± ì‹œ OOM ë°©ì§€)
        if num_images > 1 and torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # ìƒì„± ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    
    # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ì‚¬ìš©ìë³„)
    if session:
        history_mgr = get_history_manager_sync(session.data_id)
    else:
        # ê³„ì • ë‹¨ìœ„ë¡œ ì‹¤í–‰ëœ ê²½ìš°(user_{id})ì—ëŠ” ê·¸ ê³„ì • íˆìŠ¤í† ë¦¬ì— ì €ì¥
        if isinstance(session_id, str) and session_id.startswith("user_"):
            history_mgr = get_history_manager_sync(session_id)
        else:
            from utils.history import history_manager
            history_mgr = history_manager
    
    history_entry = history_mgr.add(
        prompt=prompt,
        korean_prompt=korean_prompt,
        settings={
            "width": width,
            "height": height,
            "steps": steps,
            "guidance_scale": guidance_scale,
            "seed": seed,
        }
    )
    
    # ì™„ë£Œ ë©”ì‹œì§€
    await ws_manager.send_to_session(session_id, {
        "type": "complete",
        "content": f"âœ… {len(images)}ì¥ ìƒì„± ì™„ë£Œ! (ì‹œë“œ: {seed})"
    })
    
    # ì´ë¯¸ì§€ ê²°ê³¼ ì „ì†¡
    await ws_manager.send_to_session(session_id, {
        "type": "generation_result",
        "images": images,
        "seed": seed,
        "prompt": final_prompt,
        "history_id": history_entry.id
    })
    
    return {
        "success": True,
        "images": images,
        "seed": seed,
        "prompt": final_prompt,
        "history_id": history_entry.id
    }


# ============= ì„¸ì…˜ë³„ ì¶œë ¥ í´ë” ì •ì  íŒŒì¼ ì œê³µ =============
@app.get("/outputs/{session_id}/{filename:path}")
async def serve_session_output(session_id: str, filename: str, request: Request):
    """ì„¸ì…˜ë³„ ì¶œë ¥ íŒŒì¼ ì œê³µ"""
    # ì„¸ì…˜ ID ê²€ì¦
    if not session_manager.validate_session_id(session_id):
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    file_path = OUTPUTS_DIR / session_id / filename
    if not file_path.exists():
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(file_path)


# ë ˆê±°ì‹œ ì¶œë ¥ í´ë” (ì„¸ì…˜ ì—†ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ìš©)
@app.get("/outputs/{filename:path}")
async def serve_legacy_output(filename: str):
    """ë ˆê±°ì‹œ ì¶œë ¥ íŒŒì¼ ì œê³µ"""
    # ì„¸ì…˜ IDì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ í™•ì¸ (UUID í˜•ì‹)
    if "/" in filename or session_manager.validate_session_id(filename.split("/")[0] if "/" in filename else ""):
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    file_path = OUTPUTS_DIR / filename
    if not file_path.exists():
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(file_path)


# ============= API ì—”ë“œí¬ì¸íŠ¸ =============

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€ - ë¡œê·¸ì¸ í•„ìˆ˜"""
    update_activity()
    session = await get_session_from_request(request)
    
    # ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ê²½ìš° ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    if not session or not session.is_authenticated:
        from fastapi.responses import RedirectResponse
        response = RedirectResponse(url="/login", status_code=302)
        # ë¹„ë¡œê·¸ì¸ ì„¸ì…˜ì€ ìƒì„±/ìœ ì§€í•˜ì§€ ì•ŠìŒ (ì¿ í‚¤ë„ ì œê±°)
        clear_session_cookie(response)
        return response
    
    response = templates.TemplateResponse("index.html", {
        "request": request,
        "user": {
            "id": session.user_id,
            "username": session.username,
        },
        # ì •ì  íŒŒì¼ ìºì‹œë¡œ ì¸í•´ UI ë³€ê²½ì´ ë°˜ì˜ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ ë°©ì§€
        "cache_bust": int(time.time()),
    })
    set_session_cookie(response, session)
    
    return response


@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    """ë¡œê·¸ì¸ í˜ì´ì§€"""
    session = await get_session_from_request(request)
    
    # ì´ë¯¸ ë¡œê·¸ì¸ëœ ê²½ìš° ë©”ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    if session and session.is_authenticated:
        from fastapi.responses import RedirectResponse
        response = RedirectResponse(url="/", status_code=302)
        set_session_cookie(response, session)
        return response
    
    response = templates.TemplateResponse("login.html", {
        "request": request,
        "cache_bust": int(time.time()),
    })
    # ë¹„ë¡œê·¸ì¸ì—ì„œëŠ” ì„¸ì…˜/ì¿ í‚¤ë¥¼ ë§Œë“¤ì§€ ì•ŠìŒ
    clear_session_cookie(response)
    
    return response


@app.get("/api/status")
async def get_status(request: Request):
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    global pipe, current_model, device
    update_activity()
    
    session = await get_session_from_request(request)
    queue_status = generation_queue.get_queue_status()
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)
    
    status = {
        "model_loaded": pipe is not None,
        "current_model": current_model,
        "device": device or get_device(),
        "vram": get_vram_info(),
        "is_generating": queue_status["is_processing"],
        "queue_length": queue_status["queue_length"],
        "connected_users": ws_manager.get_session_count(),
        # í”„ë¡ íŠ¸ í˜¸í™˜ í•„ë“œ: ë¡œê·¸ì¸ ì‹œ ê³„ì • í‚¤(user_{id}), ë¹„ë¡œê·¸ì¸ ì‹œ None
        "session_id": (session.data_id if session else None),
        "is_admin": is_admin,
    }
    
    # ê´€ë¦¬ìì¸ ê²½ìš° GPU ì •ë³´ ì¶”ê°€
    if is_admin:
        status["gpu_info"] = {
            "gpu_count": gpu_monitor.gpu_count,
            "cuda_available": gpu_monitor.cuda_available,
            "available_devices": gpu_monitor.get_available_devices(),
            "gpus": gpu_monitor.get_all_gpu_info(),
        }
    
    return status


@app.post("/api/model/load")
async def load_model(request: Request, model_request: ModelLoadRequest):
    """ëª¨ë¸ ë¡œë“œ"""
    global pipe, current_model, device, model_lock
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    async with model_lock:
        # GPU ì„ íƒ (ê´€ë¦¬ìë§Œ íŠ¹ì • GPU ì§€ì • ê°€ëŠ¥)
        target_device = model_request.target_device
        client_host = request.client.host if request.client else None
        is_admin = is_localhost(client_host)

        # UIê°€ target_device="auto"ë¡œ ë³´ë‚´ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ,
        # ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê¸°ë³¸ GPU(ì„¤ì • -> GPU ì„¤ì •/ëª¨ë‹ˆí„°ë§)ë¥¼ ìë™ ì ìš©í•œë‹¤.
        if target_device == "auto":
            target_device = settings.get("generation_gpu", DEFAULT_GPU_SETTINGS["generation_gpu"])

        if not is_admin and target_device != "auto":
            # ê´€ë¦¬ìê°€ ì•„ë‹Œ ê²½ìš° autoë¡œ ê°•ì œ
            target_device = "auto"
        
        device = get_device(target_device)

        # ì–‘ìí™”/CPU ì˜¤í”„ë¡œë”©ì€ ê´€ë¦¬ìë§Œ ë³€ê²½ ê°€ëŠ¥
        requested_quantization = model_request.quantization
        requested_cpu_offload = model_request.cpu_offload
        if not is_admin:
            requested_quantization = settings.get("quantization", requested_quantization)
            requested_cpu_offload = settings.get("cpu_offload", requested_cpu_offload)

        quant_info = QUANTIZATION_OPTIONS.get(requested_quantization)
        
        if not quant_info:
            raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {requested_quantization}")
        
        repo_id = quant_info["repo"]
        dtype = quant_info["type"]
        is_gguf = quant_info.get("is_gguf", False)
        
        try:
            # 1ë‹¨ê³„: ë¡œë”© ì¤€ë¹„
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 5, 
                "label": "ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...",
                "detail": f"ì–‘ìí™”: {dtype}, ë””ë°”ì´ìŠ¤: {device}",
                "stage": "init"
            })
            await asyncio.sleep(0.1)
            
            from diffusers import ZImagePipeline
            
            if is_gguf:
                # GGUF ì–‘ìí™” ëª¨ë¸ ë¡œë“œ
                from diffusers import ZImageTransformer2DModel, GGUFQuantizationConfig
                from huggingface_hub import hf_hub_download
                
                filename = quant_info["filename"]
                
                # 2ë‹¨ê³„: GGUF ë‹¤ìš´ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 10, 
                    "label": "ğŸ“¥ GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                    "detail": f"íŒŒì¼: {filename} (ìºì‹œ í™•ì¸ ì¤‘...)",
                    "stage": "download"
                })
                await asyncio.sleep(0.1)
                
                gguf_path = await asyncio.to_thread(
                    hf_hub_download,
                    repo_id=repo_id, 
                    filename=filename,
                    cache_dir=str(MODELS_DIR)
                )
                
                # 3ë‹¨ê³„: GGUF Transformer ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 30, 
                    "label": "ğŸ”„ GGUF Transformer ë¡œë”© ì¤‘...",
                    "detail": f"ì–‘ìí™” íƒ€ì…: {dtype} (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)",
                    "stage": "load_transformer"
                })
                await asyncio.sleep(0.1)
                
                transformer = await asyncio.to_thread(
                    ZImageTransformer2DModel.from_single_file,
                    gguf_path,
                    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
                    torch_dtype=torch.bfloat16,
                )
                
                # 4ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 55, 
                    "label": "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...",
                    "detail": "ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ ë° GGUF Transformer ê²°í•©",
                    "stage": "load_pipeline"
                })
                await asyncio.sleep(0.1)
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    "Tongyi-MAI/Z-Image-Turbo",
                    transformer=transformer,
                    torch_dtype=torch.bfloat16,
                )
            else:
                # ê¸°ë³¸ BF16 ëª¨ë¸ ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 15, 
                    "label": "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                    "detail": f"ì €ì¥ì†Œ: {repo_id} (ìºì‹œì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤)",
                    "stage": "download"
                })
                await asyncio.sleep(0.1)
                
                load_kwargs = {
                    "torch_dtype": torch.bfloat16,
                    "cache_dir": str(MODELS_DIR),
                }
                
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 30, 
                    "label": "ğŸ”„ ëª¨ë¸ íŒŒì¼ ë¡œë”© ì¤‘...",
                    "detail": "ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ìºì‹œì—ì„œ ë¡œë“œ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª‡ ë¶„ ì†Œìš”)",
                    "stage": "load_model"
                })
                await asyncio.sleep(0.1)
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    repo_id,
                    **load_kwargs
                )
            
            # 5ë‹¨ê³„: ë””ë°”ì´ìŠ¤ ì „ì†¡
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 75, 
                "label": f"ğŸš€ {device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...",
                "detail": "VRAMìœ¼ë¡œ ëª¨ë¸ ë³µì‚¬ ì¤‘...",
                "stage": "to_device"
            })
            await asyncio.sleep(0.1)
            
            if requested_cpu_offload:
                await asyncio.to_thread(pipe.enable_model_cpu_offload)
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 95, 
                    "label": "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì • ì¤‘...",
                    "detail": "VRAM ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ RAM ì‚¬ìš©",
                    "stage": "cpu_offload"
                })
            else:
                await asyncio.to_thread(pipe.to, device)
            
            current_model = requested_quantization
            
            # GPU ëª¨ë‹ˆí„°ì— ëª¨ë¸ ë“±ë¡
            gpu_monitor.register_model("Z-Image-Turbo", device)
            
            # 6ë‹¨ê³„: ì™„ë£Œ
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 100, 
                "label": "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}",
                "stage": "complete"
            })
            
            await ws_manager.broadcast({
                "type": "model_status_change",
                "model_loaded": True,
                "current_model": current_model,
                "device": device
            })
            
            await ws_manager.broadcast({
                "type": "complete",
                "content": f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({dtype}, {device})"
            })
            
            return {"success": True, "message": f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {repo_id} ({dtype})", "device": device}
            
        except Exception as e:
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 0, 
                "label": "âŒ ë¡œë“œ ì‹¤íŒ¨",
                "detail": str(e),
                "stage": "error"
            })
            await ws_manager.broadcast({"type": "error", "content": f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"})
            raise HTTPException(500, str(e))


@app.post("/api/model/unload")
async def unload_model(request: Request):
    """ëª¨ë¸ ì–¸ë¡œë“œ"""
    global pipe, current_model, model_lock
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    async with model_lock:
        if pipe is None:
            return {"success": True, "message": "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 30, 
                "label": "ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...",
                "detail": ""
            })
            
            # GPU ëª¨ë‹ˆí„°ì—ì„œ ëª¨ë¸ ë“±ë¡ í•´ì œ
            gpu_monitor.unregister_model("Z-Image-Turbo")
            
            del pipe
            pipe = None
            current_model = None
            
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 60, 
                "label": "VRAM ì •ë¦¬ ì¤‘...",
                "detail": ""
            })
            
            # GPU ìºì‹œ ì •ë¦¬ (ìƒì„± ëª¨ë¸ ë””ë°”ì´ìŠ¤ë§Œ ì •ë¦¬)
            gpu_monitor.clear_cache(device)
            gc.collect()
            
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 100, 
                "label": "ì–¸ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}"
            })
            
            await ws_manager.broadcast({
                "type": "model_status_change",
                "model_loaded": False,
                "current_model": None
            })
            
            await ws_manager.broadcast({"type": "complete", "content": "âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!"})
            return {"success": True, "message": "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"}
            
        except Exception as e:
            raise HTTPException(500, str(e))


# ============= Instant Generate API (íœ˜ë°œì„± ì´ë¯¸ì§€ ìƒì„±) =============
class InstantGenerateRequest(BaseModel):
    """íœ˜ë°œì„± ì´ë¯¸ì§€ ìƒì„± ìš”ì²­"""
    prompt: str
    width: int = 512
    height: int = 512
    steps: int = 8
    guidance_scale: float = 0.0
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


@app.post("/api/instant-generate", summary="Instant Generate (No Save)", 
          description="íœ˜ë°œì„± ì´ë¯¸ì§€ ìƒì„± - íŒŒì¼ ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œ ë°”ë¡œ ë°˜í™˜ (API í‚¤ í•„ìˆ˜)")
async def instant_generate_image(
    request: Request,
    gen_request: InstantGenerateRequest,
    api_key: Optional[str] = Depends(get_api_key_auth)
):
    """
    íœ˜ë°œì„± ì´ë¯¸ì§€ ìƒì„± API
    
    - ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ
    - íˆìŠ¤í† ë¦¬ì— ê¸°ë¡í•˜ì§€ ì•ŠìŒ
    - base64ë¡œ ì§ì ‘ ë°˜í™˜ í›„ ë©”ëª¨ë¦¬ì—ì„œ ì‚­ì œ
    - API í‚¤ ì¸ì¦ í•„ìˆ˜
    """
    global pipe
    update_activity()
    
    # API í‚¤ ì¸ì¦ í•„ìˆ˜
    api_key_str = api_key or get_api_key_from_request(request)
    if not api_key_str:
        raise HTTPException(401, "API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. Authorization: Bearer <api_key> í—¤ë”ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    is_valid, api_key_obj = api_key_manager.validate_api_key(api_key_str)
    if not is_valid:
        raise HTTPException(401, "ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤.")
    
    # ëª¨ë¸ ì²´í¬
    if pipe is None:
        success, message = await ensure_generation_model_loaded()
        if not success:
            raise HTTPException(400, f"ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {message}")
    
    if not gen_request.prompt.strip():
        raise HTTPException(400, "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    try:
        # í”„ë¡¬í”„íŠ¸ ë²ˆì—­
        final_prompt = gen_request.prompt
        if gen_request.auto_translate and translator.is_korean(gen_request.prompt):
            final_prompt, success = translator.translate(gen_request.prompt)
        
        # ì‹œë“œ ì„¤ì •
        seed = gen_request.seed
        if seed == -1:
            seed = random.randint(0, 2147483647)
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        
        # ì´ë¯¸ì§€ ìƒì„±
        images_response = []
        current_seed = seed
        
        for i in range(gen_request.num_images):
            generator = torch.Generator(device=device).manual_seed(current_seed)
            
            result = pipe(
                prompt=final_prompt,
                width=gen_request.width,
                height=gen_request.height,
                num_inference_steps=gen_request.steps,
                guidance_scale=gen_request.guidance_scale,
                generator=generator,
            )
            
            result_image = result.images[0]
            
            # base64ë¡œ ë³€í™˜ (íŒŒì¼ ì €ì¥ ì—†ìŒ)
            buffered = BytesIO()
            result_image.save(buffered, format="PNG")
            img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
            buffered.close()
            
            images_response.append({
                "base64": img_base64,
                "seed": current_seed,
                "width": result_image.width,
                "height": result_image.height,
            })
            
            # ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ìœ„í•œ ì‹œë“œ ì¦ê°€
            current_seed += 1
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del result_image
            del result
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return {
            "success": True,
            "images": images_response,
            "prompt": final_prompt,
            "original_prompt": gen_request.prompt,
            "settings": {
                "width": gen_request.width,
                "height": gen_request.height,
                "steps": gen_request.steps,
                "guidance_scale": gen_request.guidance_scale,
                "seed": seed,
                "num_images": gen_request.num_images,
            }
        }
        
    except Exception as e:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        raise HTTPException(500, f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}")


@app.post("/api/generate", summary="Generate Image", description="ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ (íì— ì¶”ê°€ ë˜ëŠ” ì§ì ‘ ì‹¤í–‰)")
async def generate_image(
    request: Request, 
    gen_request: GenerateRequest,
    api_key: Optional[str] = Depends(get_api_key_auth)
):
    """ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ (íì— ì¶”ê°€ ë˜ëŠ” ì§ì ‘ ì‹¤í–‰)"""
    update_activity()
    
    # API í‚¤ ë˜ëŠ” ì„¸ì…˜ ì¸ì¦ í™•ì¸ (Swagger docsì˜ Authorize ë²„íŠ¼ ë˜ëŠ” í—¤ë”ì—ì„œ)
    api_key_str = api_key or get_api_key_from_request(request)
    
    if api_key_str:
        # API í‚¤ ì¸ì¦
        is_valid, api_key_obj = api_key_manager.validate_api_key(api_key_str)
        if not is_valid:
            raise HTTPException(401, "ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤.")
        
        # API í‚¤ë¡œ í˜¸ì¶œ ì‹œ: ì§ì ‘ ì‹¤í–‰ ëª¨ë“œ (ë™ê¸°ì ìœ¼ë¡œ ê²°ê³¼ ë°˜í™˜)
        if pipe is None:
            success, message = await ensure_generation_model_loaded()
            if not success:
                raise HTTPException(400, f"ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {message}")
        
        if not gen_request.prompt.strip():
            raise HTTPException(400, "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        # API í‚¤ ì‚¬ìš© ì‹œ ë³„ë„ data_id ì‚¬ìš©
        api_data_id = f"api_key_{api_key_obj.id}"
        
        # ì§ì ‘ ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰ (í ì—†ì´)
        try:
            request_data = {
                "session_id": api_data_id,
                "prompt": gen_request.prompt,
                "korean_prompt": gen_request.korean_prompt,
                "width": gen_request.width,
                "height": gen_request.height,
                "steps": gen_request.steps,
                "guidance_scale": gen_request.guidance_scale,
                "seed": gen_request.seed,
                "num_images": gen_request.num_images,
                "auto_translate": gen_request.auto_translate,
            }
            result = await execute_generation(request_data)
            return result
        except Exception as e:
            raise HTTPException(500, f"ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    # ê¸°ì¡´ ì„¸ì…˜ ì¸ì¦ ë°©ì‹
    session = await get_session_from_request(request)
    require_auth(session)
    
    # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìë™ ë¡œë“œ
    if pipe is None:
        success, message = await ensure_generation_model_loaded(session.data_id)
        if not success:
            raise HTTPException(400, f"ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {message}")
    
    if not gen_request.prompt.strip():
        raise HTTPException(400, "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # Rate limit ì²´í¬
    # ê³„ì • ë‹¨ìœ„ë¡œ ì œí•œ(ì„¸ì…˜ ë‹¨ìœ„ êµ¬ë¶„ ì œê±°)
    exceeded, count = session_manager.check_rate_limit(session.session_id)
    if exceeded:
        await ws_manager.send_to_session(session.data_id, {
            "type": "warning",
            "content": f"âš ï¸ ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. (ë¶„ë‹¹ {count}íšŒ) ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        })
    
    # ìš”ì²­ ë°ì´í„° ì¤€ë¹„
    request_data = {
        # ì‹¤í–‰/ì•Œë¦¼/í ëª¨ë‘ ê³„ì •(data_id) ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        "session_id": session.data_id,
        "prompt": gen_request.prompt,
        "korean_prompt": gen_request.korean_prompt,
        "width": gen_request.width,
        "height": gen_request.height,
        "steps": gen_request.steps,
        "guidance_scale": gen_request.guidance_scale,
        "seed": gen_request.seed,
        "num_images": gen_request.num_images,
        "auto_translate": gen_request.auto_translate,
    }
    
    # íì— ì¶”ê°€
    item_id, position = await generation_queue.add_to_queue(session.data_id, request_data)
    
    # í ìƒíƒœ ì•Œë¦¼
    if position > 1:
        await ws_manager.send_to_session(session.data_id, {
            "type": "queue_status",
            "status": "queued",
            "position": position,
            "message": f"â³ ëŒ€ê¸°ì—´ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆœì„œ: {position})"
        })
    else:
        await ws_manager.send_to_session(session.data_id, {
            "type": "queue_status",
            "status": "processing",
            "position": 0,
            "message": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
        })
    
    response = JSONResponse(content={
        "success": True,
        "queued": True,
        "item_id": item_id,
        "position": position,
        "message": f"ìš”ì²­ì´ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆœì„œ: {position})"
    })
    set_session_cookie(response, session)
    
    return response


@app.post("/api/preview")
async def generate_preview(
    request: Request, 
    gen_request: GenerateRequest,
    api_key: Optional[str] = Depends(get_api_key_auth)
):
    """ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° (256x256)"""
    gen_request.width = 256
    gen_request.height = 256
    gen_request.steps = min(gen_request.steps, 4)
    gen_request.num_images = 1
    return await generate_image(request, gen_request, api_key)


@app.post("/api/translate")
async def translate_text(request: Request, trans_request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ë²ˆì—­ (í•œêµ­ì–´ â†’ ì˜ì–´)"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = translator.translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/translate-reverse")
async def reverse_translate_text(request: Request, trans_request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ì—­ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = translator.reverse_translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/enhance")
async def enhance_prompt(request: Request, enhance_request: EnhanceRequest):
    """í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    enhanced, success = prompt_enhancer.enhance(enhance_request.prompt, enhance_request.style)
    return {"success": success, "enhanced": enhanced}


@app.get("/api/templates")
async def get_templates():
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ëª©ë¡"""
    return {"templates": PROMPT_TEMPLATES}


@app.get("/api/model-status")
async def get_model_download_status():
    """ê° ëª¨ë¸ì˜ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸"""
    from huggingface_hub import try_to_load_from_cache
    
    status = {}
    
    for option_name, option_info in QUANTIZATION_OPTIONS.items():
        is_downloaded = False
        
        try:
            if option_info.get("is_gguf", False):
                filename = option_info.get("filename", "")
                repo_id = option_info.get("repo", "")
                
                if filename and repo_id:
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename=filename
                    )
                    is_downloaded = cached_path is not None
            else:
                repo_id = option_info.get("repo", "")
                if repo_id:
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename="model_index.json"
                    )
                    is_downloaded = cached_path is not None
        except Exception as e:
            print(f"ìºì‹œ í™•ì¸ ì˜¤ë¥˜ ({option_name}): {e}")
            is_downloaded = False
        
        status[option_name] = is_downloaded
    
    return {"status": status}


# ============= íˆìŠ¤í† ë¦¬/ì¦ê²¨ì°¾ê¸°/ê°¤ëŸ¬ë¦¬ APIëŠ” routers/history.py, routers/gallery.pyë¡œ ì´ë™ë¨ =============


# ============= ì„¤ì • APIëŠ” routers/settings_router.pyë¡œ ì´ë™ë¨ =============


# ============= ì¸ì¦ APIëŠ” routers/auth.pyë¡œ ì´ë™ë¨ =============


# ============= ê´€ë¦¬ì APIëŠ” routers/admin.pyë¡œ ì´ë™ë¨ =============


# ============= WebSocket =============
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket, z_image_session: Optional[str] = Cookie(default=None)):
    """ì›¹ì†Œì¼“ ì—°ê²° (ì„¸ì…˜ë³„)"""
    # ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°
    # ë¹„ë¡œê·¸ì¸ì—ì„œ ì„¸ì…˜ì„ ë§Œë“¤ì§€ ì•Šê¸° ìœ„í•´ "ê¸°ì¡´ ì„¸ì…˜ ì¡°íšŒ"ë§Œ ì‹œë„
    session = session_manager.get_session(z_image_session) if z_image_session else None
    
    # ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ê²½ìš° ì—°ê²° ê±°ë¶€
    if not session or not session.is_authenticated:
        await websocket.close(code=4001, reason="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ê³„ì •(data_id) ë‹¨ìœ„ë¡œ WebSocket ë£¸ì„ í†µì¼ (ì„¸ì…˜ êµ¬ë¶„ ì œê±°)
    await ws_manager.connect(websocket, session.data_id)
    update_activity()
    
    try:
        # ì—°ê²° ì‹œ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "connected",
            "content": "ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.",
            # í”„ë¡ íŠ¸ì—ì„œ ì“°ëŠ” ê°’ë„ ê³„ì • í‚¤ë¡œ í†µì¼
            "session_id": session.data_id,
            "connected_users": ws_manager.get_session_count()
        })
        
        # í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "model_status_change",
            "model_loaded": pipe is not None,
            "current_model": current_model
        })
        
        # í¸ì§‘ ëª¨ë¸ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "edit_model_status_change",
            "model_loaded": qwen_edit_manager.is_loaded,
            "current_model": qwen_edit_manager.current_model
        })
        
        # ì ‘ì†ì ìˆ˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        await ws_manager.broadcast({
            "type": "user_count",
            "count": ws_manager.get_session_count()
        })
        
        while True:
            data = await websocket.receive_text()
            update_activity()
            
            # í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬
            try:
                message = json.loads(data)
                # í•‘/í ì²˜ë¦¬
                if message.get("type") == "ping":
                    await websocket.send_json({"type": "pong"})
            except:
                pass
            
    except WebSocketDisconnect:
        await ws_manager.disconnect(websocket)
        
        # ì ‘ì†ì ìˆ˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        await ws_manager.broadcast({
            "type": "user_count",
            "count": ws_manager.get_session_count()
        })


# ============= Qwen-Image-Edit API =============

def update_edit_activity():
    """í¸ì§‘ ëª¨ë¸ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸"""
    global edit_last_activity_time
    edit_last_activity_time = time.time()


@app.get("/api/edit/status")
async def get_edit_status(request: Request):
    """í¸ì§‘ ëª¨ë¸ ìƒíƒœ"""
    update_edit_activity()
    
    session = await get_session_from_request(request)
    require_auth(session)
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)
    
    status = {
        "model_loaded": qwen_edit_manager.is_loaded,
        "current_model": qwen_edit_manager.current_model,
        "current_quantization": "NF4 (4bit)",  # Qwenì€ 4bit NF4 ê³ ì •
        "cpu_offload_enabled": qwen_edit_manager.cpu_offload_enabled,
        # ì €ì¥ëœ(ê¸°ë³¸) í¸ì§‘ ëª¨ë¸ ì„¤ì •ê°’ - ìƒˆë¡œê³ ì¹¨/ì¬ì‹œì‘ í›„ UIì—ì„œ ìœ ì§€ë˜ë„ë¡ ì œê³µ
        "saved_edit_cpu_offload": settings.get("edit_cpu_offload", True),
        "device": qwen_edit_manager.device or qwen_edit_manager.get_device(),
        "vram": get_vram_info(),
        "session_id": session.data_id,
        "is_admin": is_admin,
        # Qwenì€ 4bit NF4 ê³ ì • (~16GB with CPU offload)
        "quantization_options": ["NF4 (4bit)"],
        "quantization_details": {
            "NF4 (4bit)": {
                "type": "nf4",
                "estimated_vram": "~16GB (CPU offload)",
            }
        },
    }
    
    # ê´€ë¦¬ìì¸ ê²½ìš° ì¶”ê°€ ì •ë³´
    if is_admin:
        status["available_devices"] = gpu_monitor.get_available_devices()
    
    return status


@app.post("/api/edit/model/load")
async def load_edit_model(request: Request, model_request: EditModelLoadRequest):
    """Qwen-Image-Edit ëª¨ë¸ ë¡œë“œ"""
    global edit_model_lock
    
    if edit_model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    # GPU ì„ íƒ (ê´€ë¦¬ìë§Œ íŠ¹ì • GPU ì§€ì • ê°€ëŠ¥)
    target_device = model_request.target_device
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)

    # UIê°€ target_device="auto"ë¡œ ë³´ë‚´ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ,
    # ê´€ë¦¬ìê°€ ì„¤ì •í•œ í¸ì§‘ ê¸°ë³¸ GPUë¥¼ ìë™ ì ìš©í•œë‹¤.
    if target_device == "auto":
        target_device = settings.get("edit_gpu", DEFAULT_GPU_SETTINGS["edit_gpu"])

    if not is_admin and target_device != "auto":
        # ê´€ë¦¬ìê°€ ì•„ë‹Œ ê²½ìš° autoë¡œ ê°•ì œ
        target_device = "auto"
    
    async with edit_model_lock:
        async def progress_callback(percent, label, detail):
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": percent,
                "label": label,
                "detail": detail,
                "stage": "loading" if percent < 100 else "complete"
            })
        
        try:
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 0,
                "label": "ğŸ”§ í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì‹œì‘...",
                "detail": "",
                "stage": "init"
            })
            
            # CPU ì˜¤í”„ë¡œë”©ì€ ê´€ë¦¬ìë§Œ ë³€ê²½ ê°€ëŠ¥ (Qwenì€ 4bit NF4 ê³ ì •)
            requested_cpu_offload = model_request.cpu_offload
            if not is_admin:
                requested_cpu_offload = settings.get("edit_cpu_offload", requested_cpu_offload)

            success, message = await qwen_edit_manager.load_model(
                cpu_offload=requested_cpu_offload,
                model_path=model_request.model_path if model_request.model_path else None,
                target_device=target_device,
                progress_callback=progress_callback
            )
            
            if success:
                await ws_manager.broadcast({
                    "type": "edit_model_status_change",
                    "model_loaded": True,
                    "current_model": qwen_edit_manager.current_model,
                    "device": qwen_edit_manager.device
                })
                await ws_manager.broadcast({
                    "type": "edit_system",
                    "content": f"âœ… í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({qwen_edit_manager.device})"
                })
                return {"success": True, "message": message, "device": qwen_edit_manager.device}
            else:
                await ws_manager.broadcast({
                    "type": "edit_model_progress",
                    "progress": 0,
                    "label": "âŒ ë¡œë“œ ì‹¤íŒ¨",
                    "detail": message,
                    "stage": "error"
                })
                raise HTTPException(500, message)
                
        except Exception as e:
            await ws_manager.broadcast({
                "type": "edit_system",
                "content": f"âŒ í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            })
            raise HTTPException(500, str(e))


@app.post("/api/edit/model/unload")
async def unload_edit_model(request: Request):
    """Qwen-Image-Edit ëª¨ë¸ ì–¸ë¡œë“œ"""
    global edit_model_lock
    
    if edit_model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    async with edit_model_lock:
        try:
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 50,
                "label": "í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...",
                "detail": ""
            })
            
            success, message = await qwen_edit_manager.unload_model()
            
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 100,
                "label": "ì–¸ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM: {get_vram_info()}"
            })
            
            await ws_manager.broadcast({
                "type": "edit_model_status_change",
                "model_loaded": False,
                "current_model": None
            })
            
            await ws_manager.broadcast({
                "type": "complete",
                "content": "âœ… í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!"
            })
            
            return {"success": success, "message": message}
            
        except Exception as e:
            raise HTTPException(500, str(e))


@app.post("/api/edit/generate", summary="Edit Image", description="ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰ (Qwen)")
async def edit_image(
    request: Request,
    images: List[UploadFile] = File(..., description="í¸ì§‘í•  ì´ë¯¸ì§€ (1~3ì¥)"),
    prompt: str = Form(...),
    negative_prompt: str = Form(" "),
    korean_prompt: str = Form(""),
    steps: int = Form(20),
    true_cfg_scale: float = Form(4.0),
    guidance_scale: float = Form(1.0),
    seed: int = Form(-1),
    num_images: int = Form(1),
    auto_translate: str = Form("true"),
    api_key: Optional[str] = Depends(get_api_key_auth)
):
    """ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰ (Qwen - 1~3ì¥ ì´ë¯¸ì§€ ì…ë ¥ ì§€ì›)"""
    update_edit_activity()
    
    # API í‚¤ ë˜ëŠ” ì„¸ì…˜ ì¸ì¦ í™•ì¸ (Swagger docsì˜ Authorize ë²„íŠ¼ ë˜ëŠ” í—¤ë”ì—ì„œ)
    api_key_str = api_key or get_api_key_from_request(request)
    api_key_obj = None
    session = None
    data_id = None
    use_websocket = True
    
    if api_key_str:
        # API í‚¤ ì¸ì¦
        is_valid, api_key_obj = api_key_manager.validate_api_key(api_key_str)
        if not is_valid:
            raise HTTPException(401, "ìœ íš¨í•˜ì§€ ì•Šì€ API í‚¤ì…ë‹ˆë‹¤.")
        data_id = f"api_key_{api_key_obj.id}"
        use_websocket = False  # API í‚¤ ì‚¬ìš© ì‹œ ì›¹ì†Œì¼“ ì•Œë¦¼ ë¹„í™œì„±í™”
    else:
        # ê¸°ì¡´ ì„¸ì…˜ ì¸ì¦
        session = await get_session_from_request(request)
        require_auth(session)
        data_id = session.data_id
    
    # í¸ì§‘ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìë™ ë¡œë“œ
    if not qwen_edit_manager.is_loaded:
        success, message = await ensure_edit_model_loaded(data_id if use_websocket else None)
        if not success:
            raise HTTPException(400, f"í¸ì§‘ ëª¨ë¸ ìë™ ë¡œë“œ ì‹¤íŒ¨: {message}")
    
    if not prompt.strip():
        raise HTTPException(400, "í¸ì§‘ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # Formì—ì„œ ë°›ì€ auto_translate ë¬¸ìì—´ì„ boolë¡œ ë³€í™˜
    auto_translate_bool = auto_translate.lower() in ("true", "1", "yes")
    
    try:
        # ì´ë²ˆ í¸ì§‘ ìš”ì²­ì˜ ê³ ìœ  ID (ì…ë ¥/ì°¸ì¡° ì´ë¯¸ì§€ íŒŒì¼ëª… ë“±ì— ì‚¬ìš©)
        run_id = datetime.now().strftime("%Y%m%d%H%M%S%f")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„¸ì…˜ ë˜ëŠ” API í‚¤ë³„)
        if session:
            outputs_dir = session.get_outputs_dir()
        else:
            outputs_dir = OUTPUTS_DIR / data_id
        outputs_dir.mkdir(parents=True, exist_ok=True)
        
        # ì´ë¯¸ì§€ ë¡œë“œ (1~3ì¥)
        if len(images) > 3:
            raise HTTPException(400, "ìµœëŒ€ 3ì¥ì˜ ì´ë¯¸ì§€ë§Œ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        pil_images = []
        original_image_urls = []
        
        for idx, img_file in enumerate(images):
            image_data = await img_file.read()
            pil_image = Image.open(BytesIO(image_data)).convert("RGB")
            pil_images.append(pil_image)
            
            # ì—…ë¡œë“œëœ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥ í´ë”ì— ì €ì¥ (í¸ì§‘ê¸°ë¡ì—ì„œ ì›ë³¸ í™•ì¸ìš©)
            original_filename = f"edit_input_{run_id}_{idx+1}.png"
            original_output_path = outputs_dir / original_filename
            pil_image.save(original_output_path, format="PNG")
            original_image_urls.append(f"/outputs/{data_id}/{original_filename}")
        
        # í”„ë¡¬í”„íŠ¸ ë²ˆì—­
        final_prompt = prompt
        if auto_translate_bool and edit_translator.is_korean(prompt):
            if use_websocket:
                await ws_manager.send_to_session(data_id, {
                    "type": "edit_system",
                    "content": "ğŸŒ í¸ì§‘ ì§€ì‹œì–´ ë²ˆì—­ ì¤‘..."
                })
            final_prompt, success = edit_translator.translate(prompt)
            if not success and use_websocket:
                await ws_manager.send_to_session(data_id, {
                    "type": "edit_system",
                    "content": "âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"
                })
        
        # í¸ì§‘ ì‹œì‘ ë©”ì‹œì§€
        if use_websocket:
            await ws_manager.send_to_session(data_id, {
                "type": "edit_system",
                "content": "ğŸ¨ ì´ë¯¸ì§€ í¸ì§‘ ì¤‘..."
            })
        
        # ì§„í–‰ ìƒí™© ì½œë°± ì •ì˜
        async def edit_progress_callback(current_image: int, total_images: int, current_step: int, total_steps: int):
            if not use_websocket:
                return
            # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚° (ì´ë¯¸ì§€ + ìŠ¤í… ê¸°ì¤€)
            image_progress = (current_image - 1) / total_images
            step_progress = current_step / total_steps / total_images
            overall_progress = int((image_progress + step_progress) * 100)
            
            await ws_manager.send_to_session(data_id, {
                "type": "edit_progress",
                "current_image": current_image,
                "total_images": total_images,
                "current_step": current_step,
                "total_steps": total_steps,
                "progress": overall_progress
            })
        
        # ìƒíƒœ ë©”ì‹œì§€ ì½œë°± ì •ì˜ (ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ë“±)
        async def edit_status_callback(message: str):
            if not use_websocket:
                return
            await ws_manager.send_to_session(data_id, {
                "type": "edit_system",
                "content": message
            })
        
        # í¸ì§‘ ì‹¤í–‰ (Qwen)
        success, results, message = await qwen_edit_manager.edit_image(
            images=pil_images,
            prompt=final_prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=steps,
            true_cfg_scale=true_cfg_scale,
            guidance_scale=guidance_scale,
            seed=seed,
            num_images=num_images,
            progress_callback=edit_progress_callback,
            status_callback=edit_status_callback
        )
        
        if not success:
            raise HTTPException(500, message)
        
        # ê²°ê³¼ ì €ì¥ ë° ë°˜í™˜
        images_response = []
        result_paths = []
        
        for i, result in enumerate(results):
            result_image = result["image"]
            seed = result["seed"]
            
            # íŒŒì¼ëª… ìƒì„±
            filename = filename_generator.generate(
                pattern=settings.get("filename_pattern", "{date}_{time}_{seed}"),
                prompt=final_prompt,
                seed=seed
            )
            filename = f"edit_{filename}"
            output_path = outputs_dir / filename
            
            # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
            metadata = ImageMetadata.create_metadata(
                prompt=final_prompt,
                seed=seed,
                width=result_image.width,
                height=result_image.height,
                steps=steps,
                guidance_scale=guidance_scale,
                model="Qwen-Image-Edit",
            )
            ImageMetadata.save_with_metadata(result_image, output_path, metadata)
            
            result_paths.append(str(output_path))
            images_response.append({
                "base64": image_to_base64(result_image),
                "filename": filename,
                "seed": seed,
                "path": f"/outputs/{data_id}/{filename}"
            })
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        edit_history_mgr = get_edit_history_manager_sync(data_id)
        history_entry = edit_history_mgr.add(
            prompt=final_prompt,
            negative_prompt=negative_prompt,
            korean_prompt=korean_prompt,
            settings={
                "steps": steps,
                "true_cfg_scale": true_cfg_scale,
                "guidance_scale": guidance_scale,
                "seed": results[0]["seed"] if results else -1,
            },
            original_image_paths=original_image_urls,
            result_image_paths=[img["path"] for img in images_response]
        )
        
        # ì™„ë£Œ ë©”ì‹œì§€
        if use_websocket:
            await ws_manager.send_to_session(data_id, {
                "type": "edit_system",
                "content": f"âœ… í¸ì§‘ ì™„ë£Œ! (ì‹œë“œ: {results[0]['seed'] if results else 'N/A'})"
            })
            
            # ê²°ê³¼ ì „ì†¡
            await ws_manager.send_to_session(data_id, {
                "type": "edit_result",
                "images": images_response,
                "seed": results[0]["seed"] if results else -1,
                "prompt": final_prompt,
                "history_id": history_entry.id
            })
        
        return {
            "success": True,
            "images": images_response,
            "seed": results[0]["seed"] if results else -1,
            "prompt": final_prompt,
            "history_id": history_entry.id
        }
        
    except Exception as e:
        if use_websocket:
            await ws_manager.send_to_session(data_id, {
                "type": "error",
                "content": f"âŒ í¸ì§‘ ì˜¤ë¥˜: {str(e)}"
            })
        raise HTTPException(500, str(e))


@app.post("/api/edit/translate")
async def translate_edit_instruction(request: Request, trans_request: EditTranslateRequest):
    """í¸ì§‘ ì§€ì‹œì–´ ë²ˆì—­"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = edit_translator.translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/edit/enhance")
async def enhance_edit_instruction(request: Request, enhance_request: EditEnhanceRequest):
    """í¸ì§‘ ì§€ì‹œì–´ í–¥ìƒ"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    enhanced, success = edit_enhancer.enhance(enhance_request.instruction)
    return {"success": success, "enhanced": enhanced}


@app.post("/api/edit/suggest")
async def suggest_edits(request: Request, suggest_request: EditSuggestRequest):
    """í¸ì§‘ ì•„ì´ë””ì–´ ì œì•ˆ"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    suggestions, success = edit_suggester.suggest(
        context=suggest_request.context,
        image_description=suggest_request.image_description
    )
    
    # í•œêµ­ì–´ ì œì•ˆë„ í•¨ê»˜ ë°˜í™˜
    korean_suggestions, _ = edit_suggester.suggest_korean(
        context=suggest_request.context,
        image_description=suggest_request.image_description
    )
    
    return {
        "success": success,
        "suggestions": suggestions,
        "suggestions_korean": korean_suggestions
    }


# ============= í¸ì§‘ íˆìŠ¤í† ë¦¬ API =============
@app.get("/api/edit/history")
async def get_edit_history(request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ëª©ë¡ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    entries = edit_history_mgr.get_all()
    
    response = JSONResponse(content={"history": [e.to_dict() for e in entries[:50]]})
    set_session_cookie(response, session)
    return response


@app.get("/api/edit/history/{history_id}")
async def get_edit_history_detail(history_id: str, request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    entry = edit_history_mgr.get_by_id(history_id)
    
    if not entry:
        raise HTTPException(404, "í¸ì§‘ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"history": entry.to_dict()}


@app.get("/api/edit/history/{history_id}/chain")
async def get_edit_history_chain(history_id: str, request: Request):
    """ë©€í‹°í„´ í¸ì§‘ ì²´ì¸ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    chain = edit_history_mgr.get_chain(history_id)
    
    return {"chain": [e.to_dict() for e in chain]}


@app.patch("/api/edit/history/{history_id}/conversation")
async def update_edit_history_conversation(history_id: str, request: Request, conv_request: EditConversationUpdateRequest):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ëŒ€í™” ë‚´ìš© ì—…ë°ì´íŠ¸ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    
    success = edit_history_mgr.update_conversation(history_id, conv_request.conversation)
    if not success:
        raise HTTPException(404, "í¸ì§‘ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"success": True}


@app.delete("/api/edit/history")
async def clear_edit_history(request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ì‚­ì œ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    edit_history_mgr.clear()
    return {"success": True}


@app.delete("/api/edit/history/{history_id}")
async def delete_edit_history_entry(history_id: str, request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ í•­ëª© ì‚­ì œ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    success = edit_history_mgr.delete(history_id)
    return {"success": success}


# ============= ë©”ì¸ =============
if __name__ == "__main__":
    # ì¶œë ¥ í´ë” ìƒì„±
    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
    
    print("[*] Z-Image WebUI starting...")
    print(f"[*] http://localhost:{SERVER_PORT}")
    print("[*] Multi-user support enabled")
    
    uvicorn.run(
        app,
        host=SERVER_HOST,
        port=SERVER_PORT,
        reload=SERVER_RELOAD
    )
