"""Z-Image WebUI - FastAPI ê¸°ë°˜ ëŒ€í™”í˜• ì´ë¯¸ì§€ ìƒì„± ì›¹ì•± (ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›)"""

import os
import sys
import json
import asyncio
import base64
import random
import gc
import time
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
from io import BytesIO
from contextlib import asynccontextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent
sys.path.insert(0, str(ROOT_DIR))

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File, Form, Response, Cookie
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.requests import Request
from pydantic import BaseModel
import uvicorn

import torch
from PIL import Image

# ë¡œì»¬ ëª¨ë“ˆ
from config.defaults import (
    QUANTIZATION_OPTIONS,
    EDIT_QUANTIZATION_OPTIONS,
    RESOLUTION_PRESETS,
    OUTPUTS_DIR,
    SERVER_HOST,
    SERVER_PORT,
    SERVER_RELOAD,
    LONGCAT_EDIT_AUTO_UNLOAD_TIMEOUT,
)
from config.templates import PROMPT_TEMPLATES
from utils.settings import settings
from utils.translator import translator
from utils.prompt_enhancer import prompt_enhancer
from utils.metadata import ImageMetadata, filename_generator
from utils.history import get_history_manager_sync, HistoryManager
from utils.favorites import get_favorites_manager_sync, FavoritesManager
from utils.upscaler import upscaler, REALESRGAN_AVAILABLE
from utils.session import session_manager, is_localhost, SessionManager, SessionInfo
from utils.queue_manager import generation_queue, GenerationQueueManager
from utils.longcat_edit import longcat_edit_manager
from utils.edit_history import get_edit_history_manager_sync, EditHistoryManager
from utils.edit_llm import edit_translator, edit_enhancer, edit_suggester


# ============= ì „ì—­ ë³€ìˆ˜ =============
pipe = None
current_model = None
device = None
last_activity_time = time.time()  # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
auto_unload_task = None  # ìë™ ì–¸ë¡œë“œ ì²´í¬ íƒœìŠ¤í¬
model_lock = asyncio.Lock()  # ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ì ê¸ˆ

# LongCat-Image-Edit ê´€ë ¨
edit_last_activity_time = time.time()  # í¸ì§‘ ëª¨ë¸ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
edit_auto_unload_task = None  # í¸ì§‘ ëª¨ë¸ ìë™ ì–¸ë¡œë“œ íƒœìŠ¤í¬
edit_model_lock = asyncio.Lock()  # í¸ì§‘ ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ì ê¸ˆ


# ============= ìë™ ì–¸ë¡œë“œ ê´€ë ¨ í•¨ìˆ˜ =============
def update_activity():
    """ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸"""
    global last_activity_time
    last_activity_time = time.time()


async def auto_unload_checker():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ ì–¸ë¡œë“œ ì²´í¬"""
    global pipe, current_model, last_activity_time
    
    while True:
        await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
        
        # ìë™ ì–¸ë¡œë“œ ì„¤ì • í™•ì¸
        if not settings.get("auto_unload_enabled", True):
            continue
        
        # ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
        if pipe is None:
            continue
        
        # ìƒì„± ì¤‘ì´ë©´ ìŠ¤í‚µ
        if generation_queue.is_processing():
            update_activity()  # ìƒì„± ì¤‘ì—ëŠ” í™œë™ìœ¼ë¡œ ê°„ì£¼
            continue
        
        # íƒ€ì„ì•„ì›ƒ ì²´í¬
        timeout_minutes = settings.get("auto_unload_timeout", 10)
        timeout_seconds = timeout_minutes * 60
        elapsed = time.time() - last_activity_time
        
        if elapsed >= timeout_seconds:
            print(f"â° ìë™ ì–¸ë¡œë“œ: {timeout_minutes}ë¶„ ë™ì•ˆ í™œë™ì´ ì—†ì–´ ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤.")
            
            try:
                # ëª¨ë¸ ì–¸ë¡œë“œ
                del pipe
                pipe = None
                current_model = None
                
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                gc.collect()
                
                # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼
                await ws_manager.broadcast({
                    "type": "system",
                    "content": f"â° {timeout_minutes}ë¶„ ë™ì•ˆ í™œë™ì´ ì—†ì–´ ëª¨ë¸ì´ ìë™ ì–¸ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. VRAMì„ ì ˆì•½í•©ë‹ˆë‹¤."
                })
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 100, 
                    "label": "â° ìë™ ì–¸ë¡œë“œ ì™„ë£Œ",
                    "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}",
                    "stage": "complete"
                })
                
                print(f"âœ… ìë™ ì–¸ë¡œë“œ ì™„ë£Œ. VRAM: {get_vram_info()}")
                
            except Exception as e:
                print(f"âŒ ìë™ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” lifespan í•¸ë“¤ëŸ¬"""
    global auto_unload_task
    
    # ì‹œì‘ ì‹œ: ìë™ ì–¸ë¡œë“œ ì²´í¬ íƒœìŠ¤í¬ ì‹œì‘
    auto_unload_task = asyncio.create_task(auto_unload_checker())
    print("ğŸ”„ ìë™ ì–¸ë¡œë“œ ì²´ì»¤ ì‹œì‘ë¨")
    
    # í ì›Œì»¤ ì‹œì‘
    await generation_queue.start_worker()
    print("ğŸ”„ ì´ë¯¸ì§€ ìƒì„± í ì›Œì»¤ ì‹œì‘ë¨")
    
    # í ì½œë°± ì„¤ì •
    generation_queue.set_callbacks(
        on_status_change=on_queue_status_change,
        on_broadcast=on_queue_broadcast,
        generate_func=execute_generation
    )
    
    yield
    
    # ì¢…ë£Œ ì‹œ: íƒœìŠ¤í¬ ì·¨ì†Œ
    if auto_unload_task:
        auto_unload_task.cancel()
        try:
            await auto_unload_task
        except asyncio.CancelledError:
            pass
    
    # í ì›Œì»¤ ì¤‘ì§€
    await generation_queue.stop_worker()


# ============= FastAPI ì•± ì„¤ì • =============
app = FastAPI(title="Z-Image WebUI", version="2.0.0", lifespan=lifespan)

# ì •ì  íŒŒì¼ ë° í…œí”Œë¦¿
app.mount("/static", StaticFiles(directory=ROOT_DIR / "static"), name="static")
templates = Jinja2Templates(directory=ROOT_DIR / "templates")


# ============= Pydantic ëª¨ë¸ =============
class GenerateRequest(BaseModel):
    prompt: str
    korean_prompt: str = ""  # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ (ì›ë³¸)
    width: int = 512
    height: int = 512
    steps: int = 8
    guidance_scale: float = 0.0
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


class ModelLoadRequest(BaseModel):
    quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"
    model_path: str = ""
    cpu_offload: bool = False


class SettingsRequest(BaseModel):
    openai_api_key: str = ""  # ë ˆê±°ì‹œ í˜¸í™˜
    output_path: str = ""
    filename_pattern: str = "{date}_{time}_{seed}"
    # LLM Provider ì„¤ì •
    llm_provider: str = ""
    llm_api_key: str = ""
    llm_base_url: str = ""
    llm_model: str = ""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²ˆì—­/í–¥ìƒ)
    translate_system_prompt: Optional[str] = None
    enhance_system_prompt: Optional[str] = None
    # ìë™ ì–¸ë¡œë“œ ì„¤ì •
    auto_unload_enabled: Optional[bool] = None
    auto_unload_timeout: Optional[int] = None


class FavoriteRequest(BaseModel):
    name: str
    prompt: str
    settings: dict = {}


class TranslateRequest(BaseModel):
    text: str


class EnhanceRequest(BaseModel):
    prompt: str
    style: str = "ê¸°ë³¸"


class ConversationUpdateRequest(BaseModel):
    conversation: List[Dict[str, Any]]


# ============= í¸ì§‘ ê´€ë ¨ Pydantic ëª¨ë¸ =============
class EditModelLoadRequest(BaseModel):
    quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"
    model_path: str = ""
    cpu_offload: bool = True  # ê¸°ë³¸ í™œì„±í™” (VRAM ì ˆì•½)


class EditGenerateRequest(BaseModel):
    prompt: str
    korean_prompt: str = ""
    steps: int = 50
    guidance_scale: float = 4.5
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


class EditTranslateRequest(BaseModel):
    text: str


class EditEnhanceRequest(BaseModel):
    instruction: str


class EditSuggestRequest(BaseModel):
    context: str = ""
    image_description: str = ""


class EditConversationUpdateRequest(BaseModel):
    conversation: List[Dict[str, Any]]


# ============= ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =============
def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def image_to_base64(image: Image.Image) -> str:
    """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode()


def get_vram_info() -> str:
    """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
    if torch.cuda.is_available():
        vram_used = torch.cuda.memory_allocated() / 1024**3
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        return f"{vram_used:.1f}GB / {vram_total:.1f}GB"
    return "N/A"


async def get_session_from_request(request: Request) -> SessionInfo:
    """ìš”ì²­ì—ì„œ ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„± - í´ë¼ì´ì–¸íŠ¸ IP ê¸°ë°˜"""
    session_id = request.cookies.get(SessionManager.COOKIE_NAME)
    # í´ë¼ì´ì–¸íŠ¸ IP ê°€ì ¸ì˜¤ê¸°
    client_host = request.client.host if request.client else None
    session = await session_manager.get_or_create_session(session_id, client_host)
    return session


def set_session_cookie(response: Response, session: SessionInfo):
    """ì‘ë‹µì— ì„¸ì…˜ ì¿ í‚¤ ì„¤ì •"""
    response.set_cookie(
        key=SessionManager.COOKIE_NAME,
        value=session.session_id,
        max_age=SessionManager.COOKIE_MAX_AGE,
        httponly=True,
        samesite="lax"
    )


# ============= ì›¹ì†Œì¼“ ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„) =============
class SessionConnectionManager:
    """ì„¸ì…˜ë³„ WebSocket ì—°ê²° ê´€ë¦¬"""
    
    def __init__(self):
        # session_id -> List[WebSocket]
        self._connections: Dict[str, List[WebSocket]] = {}
        self._websocket_sessions: Dict[WebSocket, str] = {}  # ì—­ë°©í–¥ ë§¤í•‘
        self._lock = asyncio.Lock()
    
    async def connect(self, websocket: WebSocket, session_id: str):
        """ì—°ê²° ì¶”ê°€"""
        await websocket.accept()
        async with self._lock:
            if session_id not in self._connections:
                self._connections[session_id] = []
            self._connections[session_id].append(websocket)
            self._websocket_sessions[websocket] = session_id
    
    async def disconnect(self, websocket: WebSocket):
        """ì—°ê²° ì œê±°"""
        async with self._lock:
            session_id = self._websocket_sessions.get(websocket)
            if session_id:
                if session_id in self._connections:
                    if websocket in self._connections[session_id]:
                        self._connections[session_id].remove(websocket)
                    # ì„¸ì…˜ì˜ ëª¨ë“  ì—°ê²°ì´ ëŠì–´ì§€ë©´ íì—ì„œ ì œê±°
                    if not self._connections[session_id]:
                        del self._connections[session_id]
                        # íì—ì„œ í•´ë‹¹ ì„¸ì…˜ ìš”ì²­ ì œê±°
                        await generation_queue.remove_session_items(session_id)
                del self._websocket_sessions[websocket]
    
    async def send_to_session(self, session_id: str, message: dict):
        """íŠ¹ì • ì„¸ì…˜ì— ë©”ì‹œì§€ ì „ì†¡"""
        async with self._lock:
            connections = self._connections.get(session_id, [])
            for ws in connections:
                try:
                    await ws.send_json(message)
                except:
                    pass
    
    async def broadcast(self, message: dict):
        """ëª¨ë“  ì—°ê²°ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        async with self._lock:
            for connections in self._connections.values():
                for ws in connections:
                    try:
                        await ws.send_json(message)
                    except:
                        pass
    
    def get_connection_count(self) -> int:
        """ì´ ì—°ê²° ìˆ˜"""
        return sum(len(conns) for conns in self._connections.values())
    
    def get_session_count(self) -> int:
        """ì—°ê²°ëœ ì„¸ì…˜ ìˆ˜"""
        return len(self._connections)
    
    def get_session_id(self, websocket: WebSocket) -> Optional[str]:
        """WebSocketì˜ ì„¸ì…˜ ID ê°€ì ¸ì˜¤ê¸°"""
        return self._websocket_sessions.get(websocket)


ws_manager = SessionConnectionManager()


# ============= í ì½œë°± í•¨ìˆ˜ë“¤ =============
async def on_queue_status_change(session_id: str, event_type: str, data: dict):
    """í ìƒíƒœ ë³€ê²½ ì‹œ ì„¸ì…˜ì— ì•Œë¦¼"""
    if event_type == "generation_start":
        await ws_manager.send_to_session(session_id, {
            "type": "queue_status",
            "status": "processing",
            "position": 0,
            "message": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
        })
    elif event_type == "queue_position":
        await ws_manager.send_to_session(session_id, {
            "type": "queue_status",
            "status": "waiting",
            "position": data["position"],
            "message": f"â³ ëŒ€ê¸° ì¤‘... (ìˆœì„œ: {data['position']})"
        })
    elif event_type == "generation_error":
        await ws_manager.send_to_session(session_id, {
            "type": "error",
            "content": f"âŒ ìƒì„± ì˜¤ë¥˜: {data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        })
    elif event_type == "generation_complete":
        # ê²°ê³¼ëŠ” execute_generationì—ì„œ ì§ì ‘ ì „ì†¡
        pass


async def on_queue_broadcast(data: dict):
    """í ìƒíƒœ ì „ì²´ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
    await ws_manager.broadcast(data)


async def execute_generation(request_data: dict) -> dict:
    """ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰"""
    global pipe, current_model
    
    session_id = request_data.get("session_id")
    session = session_manager.get_session(session_id)
    
    if pipe is None:
        raise Exception("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    prompt = request_data.get("prompt", "")
    korean_prompt = request_data.get("korean_prompt", "")
    width = request_data.get("width", 512)
    height = request_data.get("height", 512)
    steps = request_data.get("steps", 8)
    guidance_scale = request_data.get("guidance_scale", 0.0)
    seed = request_data.get("seed", -1)
    num_images = request_data.get("num_images", 1)
    auto_translate = request_data.get("auto_translate", True)
    
    # ë²ˆì—­
    final_prompt = prompt
    if auto_translate and translator.is_korean(prompt):
        await ws_manager.send_to_session(session_id, {
            "type": "system",
            "content": "ğŸŒ í”„ë¡¬í”„íŠ¸ ë²ˆì—­ ì¤‘..."
        })
        final_prompt, success = translator.translate(prompt)
        if not success:
            await ws_manager.send_to_session(session_id, {
                "type": "warning",
                "content": "âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"
            })
    
    # ì‹œë“œ ì„¤ì •
    if seed == -1:
        seed = random.randint(0, 2147483647)
    
    # ìƒì„± ì‹œì‘ ë©”ì‹œì§€
    await ws_manager.send_to_session(session_id, {
        "type": "system",
        "content": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘..."
    })
    
    # ì„¸ì…˜ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
    if session:
        outputs_dir = session.get_outputs_dir()
    else:
        outputs_dir = OUTPUTS_DIR
    
    images = []
    for i in range(num_images):
        current_seed = seed + i
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
        percent = ((i) / num_images) * 100
        await ws_manager.send_to_session(session_id, {
            "type": "image_progress",
            "progress": percent,
            "current": i + 1,
            "total": num_images
        })
        await asyncio.sleep(0.05)
        
        generator = torch.Generator(device).manual_seed(current_seed)
        
        # ë™ê¸° pipe í˜¸ì¶œì„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        def run_pipe():
            return pipe(
                prompt=final_prompt,
                height=height,
                width=width,
                num_inference_steps=steps,
                guidance_scale=guidance_scale,
                generator=generator,
            ).images[0]
        
        image = await asyncio.to_thread(run_pipe)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥
        metadata = ImageMetadata.create_metadata(
            prompt=final_prompt,
            seed=current_seed,
            width=width,
            height=height,
            steps=steps,
            guidance_scale=guidance_scale,
            model=current_model or "unknown",
        )
        
        outputs_dir.mkdir(parents=True, exist_ok=True)
        filename = filename_generator.generate(
            pattern=settings.get("filename_pattern", "{date}_{time}_{seed}"),
            prompt=final_prompt,
            seed=current_seed
        )
        output_path = outputs_dir / filename
        ImageMetadata.save_with_metadata(image, output_path, metadata)
        
        images.append({
            "base64": image_to_base64(image),
            "filename": filename,
            "seed": current_seed,
            "path": f"/outputs/{session_id}/{filename}" if session else f"/outputs/{filename}"
        })
    
    # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ì„¸ì…˜ë³„)
    if session:
        history_mgr = get_history_manager_sync(session_id)
    else:
        from utils.history import history_manager
        history_mgr = history_manager
    
    history_entry = history_mgr.add(
        prompt=prompt,
        korean_prompt=korean_prompt,
        settings={
            "width": width,
            "height": height,
            "steps": steps,
            "guidance_scale": guidance_scale,
            "seed": seed,
        }
    )
    
    # ì™„ë£Œ ë©”ì‹œì§€
    await ws_manager.send_to_session(session_id, {
        "type": "complete",
        "content": f"âœ… {len(images)}ì¥ ìƒì„± ì™„ë£Œ! (ì‹œë“œ: {seed})"
    })
    
    # ì´ë¯¸ì§€ ê²°ê³¼ ì „ì†¡
    await ws_manager.send_to_session(session_id, {
        "type": "generation_result",
        "images": images,
        "seed": seed,
        "prompt": final_prompt,
        "history_id": history_entry.id
    })
    
    return {
        "success": True,
        "images": images,
        "seed": seed,
        "prompt": final_prompt,
        "history_id": history_entry.id
    }


# ============= ì„¸ì…˜ë³„ ì¶œë ¥ í´ë” ì •ì  íŒŒì¼ ì œê³µ =============
@app.get("/outputs/{session_id}/{filename:path}")
async def serve_session_output(session_id: str, filename: str, request: Request):
    """ì„¸ì…˜ë³„ ì¶œë ¥ íŒŒì¼ ì œê³µ"""
    # ì„¸ì…˜ ID ê²€ì¦
    if not session_manager.validate_session_id(session_id):
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    file_path = OUTPUTS_DIR / session_id / filename
    if not file_path.exists():
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(file_path)


# ë ˆê±°ì‹œ ì¶œë ¥ í´ë” (ì„¸ì…˜ ì—†ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ìš©)
@app.get("/outputs/{filename:path}")
async def serve_legacy_output(filename: str):
    """ë ˆê±°ì‹œ ì¶œë ¥ íŒŒì¼ ì œê³µ"""
    # ì„¸ì…˜ IDì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ í™•ì¸ (UUID í˜•ì‹)
    if "/" in filename or session_manager.validate_session_id(filename.split("/")[0] if "/" in filename else ""):
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    file_path = OUTPUTS_DIR / filename
    if not file_path.exists():
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(file_path)


# ============= API ì—”ë“œí¬ì¸íŠ¸ =============

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    update_activity()
    session = await get_session_from_request(request)
    
    response = templates.TemplateResponse("index.html", {"request": request})
    set_session_cookie(response, session)
    
    return response


@app.get("/api/status")
async def get_status(request: Request):
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    global pipe, current_model, device
    update_activity()
    
    session = await get_session_from_request(request)
    queue_status = generation_queue.get_queue_status()
    
    return {
        "model_loaded": pipe is not None,
        "current_model": current_model,
        "device": device or get_device(),
        "vram": get_vram_info(),
        "is_generating": queue_status["is_processing"],
        "upscaler_available": REALESRGAN_AVAILABLE,
        "queue_length": queue_status["queue_length"],
        "connected_users": ws_manager.get_session_count(),
        "session_id": session.session_id,
        "is_admin": is_localhost(request.client.host if request.client else None),
    }


@app.post("/api/model/load")
async def load_model(request: Request, model_request: ModelLoadRequest):
    """ëª¨ë¸ ë¡œë“œ"""
    global pipe, current_model, device, model_lock
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    async with model_lock:
        device = get_device()
        quant_info = QUANTIZATION_OPTIONS.get(model_request.quantization)
        
        if not quant_info:
            raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {model_request.quantization}")
        
        repo_id = quant_info["repo"]
        dtype = quant_info["type"]
        is_gguf = quant_info.get("is_gguf", False)
        
        try:
            # 1ë‹¨ê³„: ë¡œë”© ì¤€ë¹„
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 5, 
                "label": "ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...",
                "detail": f"ì–‘ìí™”: {dtype}, ë””ë°”ì´ìŠ¤: {device}",
                "stage": "init"
            })
            await asyncio.sleep(0.1)
            
            from diffusers import ZImagePipeline
            
            if is_gguf:
                # GGUF ì–‘ìí™” ëª¨ë¸ ë¡œë“œ
                from diffusers import ZImageTransformer2DModel, GGUFQuantizationConfig
                from huggingface_hub import hf_hub_download
                
                filename = quant_info["filename"]
                
                # 2ë‹¨ê³„: GGUF ë‹¤ìš´ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 10, 
                    "label": "ğŸ“¥ GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                    "detail": f"íŒŒì¼: {filename} (ìºì‹œ í™•ì¸ ì¤‘...)",
                    "stage": "download"
                })
                await asyncio.sleep(0.1)
                
                gguf_path = await asyncio.to_thread(
                    hf_hub_download,
                    repo_id=repo_id, 
                    filename=filename,
                    cache_dir=model_request.model_path if model_request.model_path else None
                )
                
                # 3ë‹¨ê³„: GGUF Transformer ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 30, 
                    "label": "ğŸ”„ GGUF Transformer ë¡œë”© ì¤‘...",
                    "detail": f"ì–‘ìí™” íƒ€ì…: {dtype} (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)",
                    "stage": "load_transformer"
                })
                await asyncio.sleep(0.1)
                
                transformer = await asyncio.to_thread(
                    ZImageTransformer2DModel.from_single_file,
                    gguf_path,
                    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
                    torch_dtype=torch.bfloat16,
                )
                
                # 4ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 55, 
                    "label": "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...",
                    "detail": "ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ ë° GGUF Transformer ê²°í•©",
                    "stage": "load_pipeline"
                })
                await asyncio.sleep(0.1)
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    "Tongyi-MAI/Z-Image-Turbo",
                    transformer=transformer,
                    torch_dtype=torch.bfloat16,
                )
            else:
                # ê¸°ë³¸ BF16 ëª¨ë¸ ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 15, 
                    "label": "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                    "detail": f"ì €ì¥ì†Œ: {repo_id} (ìºì‹œì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤)",
                    "stage": "download"
                })
                await asyncio.sleep(0.1)
                
                load_kwargs = {
                    "torch_dtype": torch.bfloat16,
                }
                if model_request.model_path:
                    load_kwargs["cache_dir"] = model_request.model_path
                
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 30, 
                    "label": "ğŸ”„ ëª¨ë¸ íŒŒì¼ ë¡œë”© ì¤‘...",
                    "detail": "ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ìºì‹œì—ì„œ ë¡œë“œ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª‡ ë¶„ ì†Œìš”)",
                    "stage": "load_model"
                })
                await asyncio.sleep(0.1)
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    repo_id,
                    **load_kwargs
                )
            
            # 5ë‹¨ê³„: ë””ë°”ì´ìŠ¤ ì „ì†¡
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 75, 
                "label": f"ğŸš€ {device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...",
                "detail": "VRAMìœ¼ë¡œ ëª¨ë¸ ë³µì‚¬ ì¤‘...",
                "stage": "to_device"
            })
            await asyncio.sleep(0.1)
            
            if model_request.cpu_offload:
                await asyncio.to_thread(pipe.enable_model_cpu_offload)
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 95, 
                    "label": "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì • ì¤‘...",
                    "detail": "VRAM ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ RAM ì‚¬ìš©",
                    "stage": "cpu_offload"
                })
            else:
                await asyncio.to_thread(pipe.to, device)
            
            current_model = model_request.quantization
            
            # 6ë‹¨ê³„: ì™„ë£Œ
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 100, 
                "label": "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}",
                "stage": "complete"
            })
            
            await ws_manager.broadcast({
                "type": "model_status_change",
                "model_loaded": True,
                "current_model": current_model
            })
            
            await ws_manager.broadcast({
                "type": "complete",
                "content": f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({dtype}, {device})"
            })
            
            return {"success": True, "message": f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {repo_id} ({dtype})"}
            
        except Exception as e:
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 0, 
                "label": "âŒ ë¡œë“œ ì‹¤íŒ¨",
                "detail": str(e),
                "stage": "error"
            })
            await ws_manager.broadcast({"type": "error", "content": f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"})
            raise HTTPException(500, str(e))


@app.post("/api/model/unload")
async def unload_model(request: Request):
    """ëª¨ë¸ ì–¸ë¡œë“œ"""
    global pipe, current_model, model_lock
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    async with model_lock:
        if pipe is None:
            return {"success": True, "message": "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 30, 
                "label": "ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...",
                "detail": ""
            })
            
            del pipe
            pipe = None
            current_model = None
            
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 60, 
                "label": "VRAM ì •ë¦¬ ì¤‘...",
                "detail": ""
            })
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            gc.collect()
            
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 100, 
                "label": "ì–¸ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}"
            })
            
            await ws_manager.broadcast({
                "type": "model_status_change",
                "model_loaded": False,
                "current_model": None
            })
            
            await ws_manager.broadcast({"type": "complete", "content": "âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!"})
            return {"success": True, "message": "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"}
            
        except Exception as e:
            raise HTTPException(500, str(e))


@app.post("/api/generate")
async def generate_image(request: Request, gen_request: GenerateRequest):
    """ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ (íì— ì¶”ê°€)"""
    update_activity()
    
    session = await get_session_from_request(request)
    
    if pipe is None:
        raise HTTPException(400, "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not gen_request.prompt.strip():
        raise HTTPException(400, "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # Rate limit ì²´í¬
    exceeded, count = session_manager.check_rate_limit(session.session_id)
    if exceeded:
        await ws_manager.send_to_session(session.session_id, {
            "type": "warning",
            "content": f"âš ï¸ ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. (ë¶„ë‹¹ {count}íšŒ) ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        })
    
    # ìš”ì²­ ë°ì´í„° ì¤€ë¹„
    request_data = {
        "session_id": session.session_id,
        "prompt": gen_request.prompt,
        "korean_prompt": gen_request.korean_prompt,
        "width": gen_request.width,
        "height": gen_request.height,
        "steps": gen_request.steps,
        "guidance_scale": gen_request.guidance_scale,
        "seed": gen_request.seed,
        "num_images": gen_request.num_images,
        "auto_translate": gen_request.auto_translate,
    }
    
    # íì— ì¶”ê°€
    item_id, position = await generation_queue.add_to_queue(session.session_id, request_data)
    
    # í ìƒíƒœ ì•Œë¦¼
    if position > 1:
        await ws_manager.send_to_session(session.session_id, {
            "type": "queue_status",
            "status": "queued",
            "position": position,
            "message": f"â³ ëŒ€ê¸°ì—´ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆœì„œ: {position})"
        })
    else:
        await ws_manager.send_to_session(session.session_id, {
            "type": "queue_status",
            "status": "processing",
            "position": 0,
            "message": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
        })
    
    response = JSONResponse(content={
        "success": True,
        "queued": True,
        "item_id": item_id,
        "position": position,
        "message": f"ìš”ì²­ì´ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆœì„œ: {position})"
    })
    set_session_cookie(response, session)
    
    return response


@app.post("/api/preview")
async def generate_preview(request: Request, gen_request: GenerateRequest):
    """ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° (256x256)"""
    gen_request.width = 256
    gen_request.height = 256
    gen_request.steps = min(gen_request.steps, 4)
    gen_request.num_images = 1
    return await generate_image(request, gen_request)


@app.post("/api/translate")
async def translate_text(request: Request, trans_request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ë²ˆì—­ (í•œêµ­ì–´ â†’ ì˜ì–´)"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = translator.translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/translate-reverse")
async def reverse_translate_text(request: Request, trans_request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ì—­ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = translator.reverse_translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/enhance")
async def enhance_prompt(request: Request, enhance_request: EnhanceRequest):
    """í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    enhanced, success = prompt_enhancer.enhance(enhance_request.prompt, enhance_request.style)
    return {"success": success, "enhanced": enhanced}


@app.get("/api/templates")
async def get_templates():
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ëª©ë¡"""
    return {"templates": PROMPT_TEMPLATES}


@app.get("/api/model-status")
async def get_model_download_status():
    """ê° ëª¨ë¸ì˜ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸"""
    from huggingface_hub import try_to_load_from_cache
    
    status = {}
    
    for option_name, option_info in QUANTIZATION_OPTIONS.items():
        is_downloaded = False
        
        try:
            if option_info.get("is_gguf", False):
                filename = option_info.get("filename", "")
                repo_id = option_info.get("repo", "")
                
                if filename and repo_id:
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename=filename
                    )
                    is_downloaded = cached_path is not None
            else:
                repo_id = option_info.get("repo", "")
                if repo_id:
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename="model_index.json"
                    )
                    is_downloaded = cached_path is not None
        except Exception as e:
            print(f"ìºì‹œ í™•ì¸ ì˜¤ë¥˜ ({option_name}): {e}")
            is_downloaded = False
        
        status[option_name] = is_downloaded
    
    return {"status": status}


# ============= ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ API =============
@app.get("/api/history")
async def get_history(request: Request):
    """íˆìŠ¤í† ë¦¬ ëª©ë¡ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    history_mgr = get_history_manager_sync(session.session_id)
    entries = history_mgr.get_all()
    
    response = JSONResponse(content={"history": [e.to_dict() for e in entries[:50]]})
    set_session_cookie(response, session)
    return response


@app.get("/api/history/{history_id}")
async def get_history_detail(history_id: str, request: Request):
    """íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    history_mgr = get_history_manager_sync(session.session_id)
    entry = history_mgr.get_by_id(history_id)
    
    if not entry:
        raise HTTPException(404, "íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"history": entry.to_dict()}


@app.patch("/api/history/{history_id}/conversation")
async def update_history_conversation(history_id: str, request: Request, conv_request: ConversationUpdateRequest):
    """íˆìŠ¤í† ë¦¬ì˜ ëŒ€í™” ë‚´ìš© ì—…ë°ì´íŠ¸ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    history_mgr = get_history_manager_sync(session.session_id)
    entry = history_mgr.get_by_id(history_id)
    
    if not entry:
        raise HTTPException(404, "íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    entry.conversation = conv_request.conversation
    history_mgr._save()
    
    return {"success": True}


@app.delete("/api/history")
async def clear_history(request: Request):
    """íˆìŠ¤í† ë¦¬ ì‚­ì œ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    history_mgr = get_history_manager_sync(session.session_id)
    history_mgr.clear()
    return {"success": True}


# ============= ì„¸ì…˜ë³„ ì¦ê²¨ì°¾ê¸° API =============
@app.get("/api/favorites")
async def get_favorites(request: Request):
    """ì¦ê²¨ì°¾ê¸° ëª©ë¡ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    fav_mgr = get_favorites_manager_sync(session.session_id)
    entries = fav_mgr.get_all()
    
    response = JSONResponse(content={"favorites": [e.to_dict() for e in entries]})
    set_session_cookie(response, session)
    return response


@app.post("/api/favorites")
async def add_favorite(request: Request, fav_request: FavoriteRequest):
    """ì¦ê²¨ì°¾ê¸° ì¶”ê°€ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    fav_mgr = get_favorites_manager_sync(session.session_id)
    entry = fav_mgr.add(
        name=fav_request.name,
        prompt=fav_request.prompt,
        settings=fav_request.settings
    )
    return {"success": True, "id": entry.id}


@app.delete("/api/favorites/{fav_id}")
async def delete_favorite(fav_id: str, request: Request):
    """ì¦ê²¨ì°¾ê¸° ì‚­ì œ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    fav_mgr = get_favorites_manager_sync(session.session_id)
    success = fav_mgr.delete(fav_id)
    return {"success": success}


# ============= ì„¸ì…˜ë³„ ê°¤ëŸ¬ë¦¬ API =============
@app.get("/api/gallery")
async def get_gallery(request: Request):
    """ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ ëª©ë¡ (ì„¸ì…˜ë³„)"""
    session = await get_session_from_request(request)
    outputs_dir = session.get_outputs_dir()
    
    images = []
    if outputs_dir.exists():
        for f in sorted(outputs_dir.glob("*.png"), key=lambda x: x.stat().st_mtime, reverse=True)[:50]:
            metadata = ImageMetadata.read_metadata(f)
            images.append({
                "filename": f.name,
                "path": f"/outputs/{session.session_id}/{f.name}",
                "metadata": metadata
            })
    
    response = JSONResponse(content={"images": images})
    set_session_cookie(response, session)
    return response


# ============= ì„¤ì • API (localhost ì „ìš© ì“°ê¸°) =============
@app.post("/api/settings")
async def save_settings(request: Request, settings_request: SettingsRequest):
    """ì„¤ì • ì €ì¥ (localhostë§Œ í—ˆìš©)"""
    # localhost ì²´í¬
    client_host = request.client.host if request.client else None
    if not is_localhost(client_host):
        raise HTTPException(403, "ì„¤ì • ë³€ê²½ì€ localhostì—ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    from utils.llm_client import llm_client
    
    # ë ˆê±°ì‹œ í˜¸í™˜
    if settings_request.openai_api_key:
        settings.set("openai_api_key", settings_request.openai_api_key)
        translator.set_api_key(settings_request.openai_api_key)
        prompt_enhancer.set_api_key(settings_request.openai_api_key)
    
    # LLM Provider ì„¤ì •
    if settings_request.llm_provider:
        settings.set("llm_provider", settings_request.llm_provider)
    
    if settings_request.llm_api_key:
        settings.set("llm_api_key", settings_request.llm_api_key)
        settings.set("openai_api_key", settings_request.llm_api_key)
    
    if settings_request.llm_base_url is not None:
        settings.set("llm_base_url", settings_request.llm_base_url)
    
    if settings_request.llm_model is not None:
        settings.set("llm_model", settings_request.llm_model)
    
    llm_client.invalidate()
    
    if settings_request.output_path:
        settings.set("output_path", settings_request.output_path)
    
    if settings_request.filename_pattern:
        settings.set("filename_pattern", settings_request.filename_pattern)
    
    if settings_request.translate_system_prompt is not None:
        settings.set("translate_system_prompt", settings_request.translate_system_prompt)
    
    if settings_request.enhance_system_prompt is not None:
        settings.set("enhance_system_prompt", settings_request.enhance_system_prompt)
    
    if settings_request.auto_unload_enabled is not None:
        settings.set("auto_unload_enabled", settings_request.auto_unload_enabled)
    
    if settings_request.auto_unload_timeout is not None:
        timeout = max(1, min(1440, settings_request.auto_unload_timeout))
        settings.set("auto_unload_timeout", timeout)
    
    return {"success": True}


@app.get("/api/settings")
async def get_settings(request: Request):
    """ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    from utils.settings import LLM_PROVIDERS
    from utils.translator import Translator
    from utils.prompt_enhancer import PromptEnhancer
    from utils.edit_llm import EditTranslator, EditEnhancer, EditSuggester
    
    session = await get_session_from_request(request)
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)
    
    # ì„¸ì…˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê°œì¸í™”)
    session_translate_prompt = session.get_setting("translate_system_prompt")
    session_enhance_prompt = session.get_setting("enhance_system_prompt")
    
    # ì„¸ì…˜ì— ì„¤ì •ì´ ì—†ìœ¼ë©´ ì „ì—­ ì„¤ì • ì‚¬ìš©, ì „ì—­ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    translate_prompt = session_translate_prompt or settings.get("translate_system_prompt") or Translator.DEFAULT_SYSTEM_PROMPT
    enhance_prompt = session_enhance_prompt or settings.get("enhance_system_prompt") or PromptEnhancer.DEFAULT_SYSTEM_PROMPT
    
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„¸ì…˜ë³„ ê°œì¸í™”)
    session_edit_translate = session.get_setting("edit_translate_system_prompt")
    session_edit_enhance = session.get_setting("edit_enhance_system_prompt")
    session_edit_suggest = session.get_setting("edit_suggest_system_prompt")
    
    edit_translate_prompt = session_edit_translate or settings.get("edit_translate_system_prompt") or EditTranslator.DEFAULT_SYSTEM_PROMPT
    edit_enhance_prompt = session_edit_enhance or settings.get("edit_enhance_system_prompt") or EditEnhancer.DEFAULT_SYSTEM_PROMPT
    edit_suggest_prompt = session_edit_suggest or settings.get("edit_suggest_system_prompt") or EditSuggester.DEFAULT_SYSTEM_PROMPT
    
    return {
        # ê´€ë¦¬ì ì—¬ë¶€
        "is_admin": is_admin,
        # ë ˆê±°ì‹œ í˜¸í™˜
        "openai_api_key": "***" if settings.get("openai_api_key") else "",
        # LLM Provider ì„¤ì •
        "llm_provider": settings.get("llm_provider", "env"),
        "llm_api_key": "***" if settings.get("llm_api_key") else "",
        "llm_base_url": settings.get("llm_base_url", ""),
        "llm_model": settings.get("llm_model", ""),
        "llm_providers": {
            pid: {
                "name": pinfo["name"],
                "base_url": pinfo["base_url"],
                "default_model": pinfo["default_model"],
                "models": pinfo["models"],
            }
            for pid, pinfo in LLM_PROVIDERS.items()
        },
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„¸ì…˜ë³„ ê°œì¸í™”)
        "translate_system_prompt": translate_prompt,
        "enhance_system_prompt": enhance_prompt,
        "default_translate_system_prompt": Translator.DEFAULT_SYSTEM_PROMPT,
        "default_enhance_system_prompt": PromptEnhancer.DEFAULT_SYSTEM_PROMPT,
        # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„¸ì…˜ë³„ ê°œì¸í™”)
        "edit_translate_system_prompt": edit_translate_prompt,
        "edit_enhance_system_prompt": edit_enhance_prompt,
        "edit_suggest_system_prompt": edit_suggest_prompt,
        "default_edit_translate_system_prompt": EditTranslator.DEFAULT_SYSTEM_PROMPT,
        "default_edit_enhance_system_prompt": EditEnhancer.DEFAULT_SYSTEM_PROMPT,
        "default_edit_suggest_system_prompt": EditSuggester.DEFAULT_SYSTEM_PROMPT,
        # ê¸°íƒ€ ì„¤ì •
        "output_path": str(settings.get("output_path", OUTPUTS_DIR)),
        "filename_pattern": settings.get("filename_pattern", "{date}_{time}_{seed}"),
        "quantization_options": list(QUANTIZATION_OPTIONS.keys()),
        "resolution_presets": RESOLUTION_PRESETS,
        # ìë™ ì–¸ë¡œë“œ ì„¤ì •
        "auto_unload_enabled": settings.get("auto_unload_enabled", True),
        "auto_unload_timeout": settings.get("auto_unload_timeout", 10),
    }


# ============= ì„¸ì…˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ API =============
class SystemPromptsRequest(BaseModel):
    translate_system_prompt: Optional[str] = None
    enhance_system_prompt: Optional[str] = None
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê°œì¸í™”)
    edit_translate_system_prompt: Optional[str] = None
    edit_enhance_system_prompt: Optional[str] = None
    edit_suggest_system_prompt: Optional[str] = None


@app.post("/api/settings/prompts")
async def save_session_prompts(request: Request, prompts_request: SystemPromptsRequest):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ (ì„¸ì…˜ë³„ ê°œì¸í™”, ëª¨ë“  ì‚¬ìš©ì ì ‘ê·¼ ê°€ëŠ¥)"""
    session = await get_session_from_request(request)
    session_settings = session.get_settings()
    
    # ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if prompts_request.translate_system_prompt is not None:
        if prompts_request.translate_system_prompt == '':
            # ë¹ˆ ë¬¸ìì—´ì´ë©´ ì„¤ì • ì‚­ì œ (ê¸°ë³¸ê°’ ì‚¬ìš©)
            session_settings.pop("translate_system_prompt", None)
        else:
            session_settings["translate_system_prompt"] = prompts_request.translate_system_prompt
    
    if prompts_request.enhance_system_prompt is not None:
        if prompts_request.enhance_system_prompt == '':
            # ë¹ˆ ë¬¸ìì—´ì´ë©´ ì„¤ì • ì‚­ì œ (ê¸°ë³¸ê°’ ì‚¬ìš©)
            session_settings.pop("enhance_system_prompt", None)
        else:
            session_settings["enhance_system_prompt"] = prompts_request.enhance_system_prompt
    
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if prompts_request.edit_translate_system_prompt is not None:
        if prompts_request.edit_translate_system_prompt == '':
            session_settings.pop("edit_translate_system_prompt", None)
        else:
            session_settings["edit_translate_system_prompt"] = prompts_request.edit_translate_system_prompt
    
    if prompts_request.edit_enhance_system_prompt is not None:
        if prompts_request.edit_enhance_system_prompt == '':
            session_settings.pop("edit_enhance_system_prompt", None)
        else:
            session_settings["edit_enhance_system_prompt"] = prompts_request.edit_enhance_system_prompt
    
    if prompts_request.edit_suggest_system_prompt is not None:
        if prompts_request.edit_suggest_system_prompt == '':
            session_settings.pop("edit_suggest_system_prompt", None)
        else:
            session_settings["edit_suggest_system_prompt"] = prompts_request.edit_suggest_system_prompt
    
    session.save_settings(session_settings)
    
    response = JSONResponse(content={"success": True})
    set_session_cookie(response, session)
    return response


@app.delete("/api/settings/prompts")
async def reset_session_prompts(request: Request):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™” (ì„¸ì…˜ë³„ ì„¤ì • ì‚­ì œ, ì „ì—­/ê¸°ë³¸ê°’ ì‚¬ìš©)"""
    session = await get_session_from_request(request)
    
    session_settings = session.get_settings()
    # ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if "translate_system_prompt" in session_settings:
        del session_settings["translate_system_prompt"]
    if "enhance_system_prompt" in session_settings:
        del session_settings["enhance_system_prompt"]
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if "edit_translate_system_prompt" in session_settings:
        del session_settings["edit_translate_system_prompt"]
    if "edit_enhance_system_prompt" in session_settings:
        del session_settings["edit_enhance_system_prompt"]
    if "edit_suggest_system_prompt" in session_settings:
        del session_settings["edit_suggest_system_prompt"]
    session.save_settings(session_settings)
    
    response = JSONResponse(content={"success": True})
    set_session_cookie(response, session)
    return response


# ============= ê´€ë¦¬ì API (localhost ì „ìš©) =============
@app.get("/api/admin/sessions")
async def get_all_sessions(request: Request):
    """ëª¨ë“  ì„¸ì…˜ ëª©ë¡ (ê´€ë¦¬ì ì „ìš©)"""
    client_host = request.client.host if request.client else None
    if not is_localhost(client_host):
        raise HTTPException(403, "ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    sessions = session_manager.get_all_sessions()
    return {"sessions": sessions}


@app.delete("/api/admin/sessions/{session_id}")
async def delete_session(session_id: str, request: Request):
    """ì„¸ì…˜ ì‚­ì œ (ê´€ë¦¬ì ì „ìš©)"""
    client_host = request.client.host if request.client else None
    if not is_localhost(client_host):
        raise HTTPException(403, "ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    success = await session_manager.delete_session(session_id)
    return {"success": success}


# ============= WebSocket =============
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket, z_image_session: Optional[str] = Cookie(default=None)):
    """ì›¹ì†Œì¼“ ì—°ê²° (ì„¸ì…˜ë³„)"""
    # ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° - í´ë¼ì´ì–¸íŠ¸ IP ê¸°ë°˜
    client_host = websocket.client.host if websocket.client else None
    session = await session_manager.get_or_create_session(z_image_session, client_host)
    
    await ws_manager.connect(websocket, session.session_id)
    update_activity()
    
    try:
        # ì—°ê²° ì‹œ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "connected",
            "content": "ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.",
            "session_id": session.session_id,
            "connected_users": ws_manager.get_session_count()
        })
        
        # í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "model_status_change",
            "model_loaded": pipe is not None,
            "current_model": current_model
        })
        
        # í¸ì§‘ ëª¨ë¸ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "edit_model_status_change",
            "model_loaded": longcat_edit_manager.is_loaded,
            "current_model": longcat_edit_manager.current_model
        })
        
        # ì ‘ì†ì ìˆ˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        await ws_manager.broadcast({
            "type": "user_count",
            "count": ws_manager.get_session_count()
        })
        
        while True:
            data = await websocket.receive_text()
            update_activity()
            
            # í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬
            try:
                message = json.loads(data)
                # í•‘/í ì²˜ë¦¬
                if message.get("type") == "ping":
                    await websocket.send_json({"type": "pong"})
            except:
                pass
            
    except WebSocketDisconnect:
        await ws_manager.disconnect(websocket)
        
        # ì ‘ì†ì ìˆ˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        await ws_manager.broadcast({
            "type": "user_count",
            "count": ws_manager.get_session_count()
        })


# ============= LongCat-Image-Edit API =============

def update_edit_activity():
    """í¸ì§‘ ëª¨ë¸ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸"""
    global edit_last_activity_time
    edit_last_activity_time = time.time()


@app.get("/api/edit/status")
async def get_edit_status(request: Request):
    """í¸ì§‘ ëª¨ë¸ ìƒíƒœ"""
    update_edit_activity()
    
    session = await get_session_from_request(request)
    
    return {
        "model_loaded": longcat_edit_manager.is_loaded,
        "current_model": longcat_edit_manager.current_model,
        "device": longcat_edit_manager.device or longcat_edit_manager.get_device(),
        "vram": get_vram_info(),
        "session_id": session.session_id,
        "quantization_options": list(EDIT_QUANTIZATION_OPTIONS.keys()),
    }


@app.post("/api/edit/model/load")
async def load_edit_model(request: Request, model_request: EditModelLoadRequest):
    """LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ"""
    global edit_model_lock
    
    if edit_model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    async with edit_model_lock:
        async def progress_callback(percent, label, detail):
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": percent,
                "label": label,
                "detail": detail,
                "stage": "loading" if percent < 100 else "complete"
            })
        
        try:
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 0,
                "label": "ğŸ”§ í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì‹œì‘...",
                "detail": "",
                "stage": "init"
            })
            
            success, message = await longcat_edit_manager.load_model(
                quantization=model_request.quantization,
                cpu_offload=model_request.cpu_offload,
                model_path=model_request.model_path if model_request.model_path else None,
                progress_callback=progress_callback
            )
            
            if success:
                await ws_manager.broadcast({
                    "type": "edit_model_status_change",
                    "model_loaded": True,
                    "current_model": longcat_edit_manager.current_model
                })
                await ws_manager.broadcast({
                    "type": "complete",
                    "content": f"âœ… í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!"
                })
                return {"success": True, "message": message}
            else:
                await ws_manager.broadcast({
                    "type": "edit_model_progress",
                    "progress": 0,
                    "label": "âŒ ë¡œë“œ ì‹¤íŒ¨",
                    "detail": message,
                    "stage": "error"
                })
                raise HTTPException(500, message)
                
        except Exception as e:
            await ws_manager.broadcast({
                "type": "error",
                "content": f"âŒ í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            })
            raise HTTPException(500, str(e))


@app.post("/api/edit/model/unload")
async def unload_edit_model(request: Request):
    """LongCat-Image-Edit ëª¨ë¸ ì–¸ë¡œë“œ"""
    global edit_model_lock
    
    if edit_model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    async with edit_model_lock:
        try:
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 50,
                "label": "í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...",
                "detail": ""
            })
            
            success, message = await longcat_edit_manager.unload_model()
            
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 100,
                "label": "ì–¸ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM: {get_vram_info()}"
            })
            
            await ws_manager.broadcast({
                "type": "edit_model_status_change",
                "model_loaded": False,
                "current_model": None
            })
            
            await ws_manager.broadcast({
                "type": "complete",
                "content": "âœ… í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!"
            })
            
            return {"success": success, "message": message}
            
        except Exception as e:
            raise HTTPException(500, str(e))


@app.post("/api/edit/generate")
async def edit_image(
    request: Request,
    image: UploadFile = File(...),
    prompt: str = Form(...),
    korean_prompt: str = Form(""),
    steps: int = Form(50),
    guidance_scale: float = Form(4.5),
    seed: int = Form(-1),
    num_images: int = Form(1),
    auto_translate: str = Form("true"),
    reference_image: Optional[UploadFile] = File(None)
):
    """ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰"""
    update_edit_activity()
    
    session = await get_session_from_request(request)
    
    if not longcat_edit_manager.is_loaded:
        raise HTTPException(400, "í¸ì§‘ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not prompt.strip():
        raise HTTPException(400, "í¸ì§‘ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # Formì—ì„œ ë°›ì€ auto_translate ë¬¸ìì—´ì„ boolë¡œ ë³€í™˜
    auto_translate_bool = auto_translate.lower() in ("true", "1", "yes")
    
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        image_data = await image.read()
        pil_image = Image.open(BytesIO(image_data)).convert("RGB")
        
        # ì°¸ì¡° ì´ë¯¸ì§€ ë¡œë“œ (ìˆìœ¼ë©´)
        ref_image = None
        if reference_image:
            ref_data = await reference_image.read()
            ref_image = Image.open(BytesIO(ref_data)).convert("RGB")
        
        # í”„ë¡¬í”„íŠ¸ ë²ˆì—­
        final_prompt = prompt
        if auto_translate_bool and edit_translator.is_korean(prompt):
            await ws_manager.send_to_session(session.session_id, {
                "type": "system",
                "content": "ğŸŒ í¸ì§‘ ì§€ì‹œì–´ ë²ˆì—­ ì¤‘..."
            })
            final_prompt, success = edit_translator.translate(prompt)
            if not success:
                await ws_manager.send_to_session(session.session_id, {
                    "type": "warning",
                    "content": "âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"
                })
        
        # í¸ì§‘ ì‹œì‘ ë©”ì‹œì§€
        await ws_manager.send_to_session(session.session_id, {
            "type": "system",
            "content": "ğŸ¨ ì´ë¯¸ì§€ í¸ì§‘ ì¤‘..."
        })
        
        # ì§„í–‰ ìƒí™© ì½œë°± ì •ì˜
        async def edit_progress_callback(current_image: int, total_images: int, steps: int):
            # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚° (ì´ë¯¸ì§€ ê¸°ì¤€)
            overall_progress = int((current_image - 1) / total_images * 100)
            
            await ws_manager.send_to_session(session.session_id, {
                "type": "edit_progress",
                "current_image": current_image,
                "total_images": total_images,
                "steps": steps,
                "progress": overall_progress
            })
        
        # í¸ì§‘ ì‹¤í–‰
        success, results, message = await longcat_edit_manager.edit_image(
            image=pil_image,
            prompt=final_prompt,
            num_inference_steps=steps,
            guidance_scale=guidance_scale,
            seed=seed,
            num_images=num_images,
            reference_image=ref_image,
            progress_callback=edit_progress_callback
        )
        
        if not success:
            raise HTTPException(500, message)
        
        # ì„¸ì…˜ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
        outputs_dir = session.get_outputs_dir()
        outputs_dir.mkdir(parents=True, exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ ë° ë°˜í™˜
        images_response = []
        result_paths = []
        
        for i, result in enumerate(results):
            result_image = result["image"]
            seed = result["seed"]
            
            # íŒŒì¼ëª… ìƒì„±
            filename = filename_generator.generate(
                pattern=settings.get("filename_pattern", "{date}_{time}_{seed}"),
                prompt=final_prompt,
                seed=seed
            )
            filename = f"edit_{filename}"
            output_path = outputs_dir / filename
            
            # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
            metadata = ImageMetadata.create_metadata(
                prompt=final_prompt,
                seed=seed,
                width=result_image.width,
                height=result_image.height,
                steps=steps,
                guidance_scale=guidance_scale,
                model="LongCat-Image-Edit",
            )
            ImageMetadata.save_with_metadata(result_image, output_path, metadata)
            
            result_paths.append(str(output_path))
            images_response.append({
                "base64": image_to_base64(result_image),
                "filename": filename,
                "seed": seed,
                "path": f"/outputs/{session.session_id}/{filename}"
            })
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        edit_history_mgr = get_edit_history_manager_sync(session.session_id)
        history_entry = edit_history_mgr.add(
            prompt=final_prompt,
            korean_prompt=korean_prompt,
            settings={
                "steps": steps,
                "guidance_scale": guidance_scale,
                "seed": results[0]["seed"] if results else -1,
            },
            result_image_paths=[img["path"] for img in images_response]
        )
        
        # ì™„ë£Œ ë©”ì‹œì§€
        await ws_manager.send_to_session(session.session_id, {
            "type": "complete",
            "content": f"âœ… í¸ì§‘ ì™„ë£Œ! (ì‹œë“œ: {results[0]['seed'] if results else 'N/A'})"
        })
        
        # ê²°ê³¼ ì „ì†¡
        await ws_manager.send_to_session(session.session_id, {
            "type": "edit_result",
            "images": images_response,
            "seed": results[0]["seed"] if results else -1,
            "prompt": final_prompt,
            "history_id": history_entry.id
        })
        
        return {
            "success": True,
            "images": images_response,
            "seed": results[0]["seed"] if results else -1,
            "prompt": final_prompt,
            "history_id": history_entry.id
        }
        
    except Exception as e:
        await ws_manager.send_to_session(session.session_id, {
            "type": "error",
            "content": f"âŒ í¸ì§‘ ì˜¤ë¥˜: {str(e)}"
        })
        raise HTTPException(500, str(e))


@app.post("/api/edit/translate")
async def translate_edit_instruction(request: Request, trans_request: EditTranslateRequest):
    """í¸ì§‘ ì§€ì‹œì–´ ë²ˆì—­"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = edit_translator.translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/edit/enhance")
async def enhance_edit_instruction(request: Request, enhance_request: EditEnhanceRequest):
    """í¸ì§‘ ì§€ì‹œì–´ í–¥ìƒ"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    enhanced, success = edit_enhancer.enhance(enhance_request.instruction)
    return {"success": success, "enhanced": enhanced}


@app.post("/api/edit/suggest")
async def suggest_edits(request: Request, suggest_request: EditSuggestRequest):
    """í¸ì§‘ ì•„ì´ë””ì–´ ì œì•ˆ"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    suggestions, success = edit_suggester.suggest(
        context=suggest_request.context,
        image_description=suggest_request.image_description
    )
    
    # í•œêµ­ì–´ ì œì•ˆë„ í•¨ê»˜ ë°˜í™˜
    korean_suggestions, _ = edit_suggester.suggest_korean(
        context=suggest_request.context,
        image_description=suggest_request.image_description
    )
    
    return {
        "success": success,
        "suggestions": suggestions,
        "suggestions_korean": korean_suggestions
    }


# ============= í¸ì§‘ íˆìŠ¤í† ë¦¬ API =============
@app.get("/api/edit/history")
async def get_edit_history(request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ëª©ë¡"""
    session = await get_session_from_request(request)
    edit_history_mgr = get_edit_history_manager_sync(session.session_id)
    entries = edit_history_mgr.get_all()
    
    response = JSONResponse(content={"history": [e.to_dict() for e in entries[:50]]})
    set_session_cookie(response, session)
    return response


@app.get("/api/edit/history/{history_id}")
async def get_edit_history_detail(history_id: str, request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´"""
    session = await get_session_from_request(request)
    edit_history_mgr = get_edit_history_manager_sync(session.session_id)
    entry = edit_history_mgr.get_by_id(history_id)
    
    if not entry:
        raise HTTPException(404, "í¸ì§‘ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"history": entry.to_dict()}


@app.get("/api/edit/history/{history_id}/chain")
async def get_edit_history_chain(history_id: str, request: Request):
    """ë©€í‹°í„´ í¸ì§‘ ì²´ì¸ ê°€ì ¸ì˜¤ê¸°"""
    session = await get_session_from_request(request)
    edit_history_mgr = get_edit_history_manager_sync(session.session_id)
    chain = edit_history_mgr.get_chain(history_id)
    
    return {"chain": [e.to_dict() for e in chain]}


@app.patch("/api/edit/history/{history_id}/conversation")
async def update_edit_history_conversation(history_id: str, request: Request, conv_request: EditConversationUpdateRequest):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ëŒ€í™” ë‚´ìš© ì—…ë°ì´íŠ¸"""
    session = await get_session_from_request(request)
    edit_history_mgr = get_edit_history_manager_sync(session.session_id)
    
    success = edit_history_mgr.update_conversation(history_id, conv_request.conversation)
    if not success:
        raise HTTPException(404, "í¸ì§‘ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"success": True}


@app.delete("/api/edit/history")
async def clear_edit_history(request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    session = await get_session_from_request(request)
    edit_history_mgr = get_edit_history_manager_sync(session.session_id)
    edit_history_mgr.clear()
    return {"success": True}


@app.delete("/api/edit/history/{history_id}")
async def delete_edit_history_entry(history_id: str, request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ í•­ëª© ì‚­ì œ"""
    session = await get_session_from_request(request)
    edit_history_mgr = get_edit_history_manager_sync(session.session_id)
    success = edit_history_mgr.delete(history_id)
    return {"success": success}


# ============= ë©”ì¸ =============
if __name__ == "__main__":
    # ì¶œë ¥ í´ë” ìƒì„±
    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ¨ Z-Image WebUI ì‹œì‘...")
    print(f"ğŸ“ http://localhost:{SERVER_PORT}")
    print("ğŸŒ ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì› í™œì„±í™”")
    
    uvicorn.run(
        app,
        host=SERVER_HOST,
        port=SERVER_PORT,
        reload=SERVER_RELOAD
    )
