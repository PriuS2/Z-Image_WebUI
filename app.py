"""Z-Image WebUI - FastAPI ê¸°ë°˜ ëŒ€í™”í˜• ì´ë¯¸ì§€ ìƒì„± ì›¹ì•±"""

import os
import sys
import json
import asyncio
import base64
import random
import gc
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
from io import BytesIO

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent
sys.path.insert(0, str(ROOT_DIR))

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.requests import Request
from pydantic import BaseModel
import uvicorn

import torch
from PIL import Image

# ë¡œì»¬ ëª¨ë“ˆ
from config.defaults import (
    QUANTIZATION_OPTIONS,
    RESOLUTION_PRESETS,
    OUTPUTS_DIR,
)
from config.templates import PROMPT_TEMPLATES
from utils.settings import settings
from utils.translator import translator
from utils.prompt_enhancer import prompt_enhancer
from utils.metadata import ImageMetadata, filename_generator
from utils.history import history_manager
from utils.favorites import favorites_manager
from utils.upscaler import upscaler, REALESRGAN_AVAILABLE


# ============= FastAPI ì•± ì„¤ì • =============
app = FastAPI(title="Z-Image WebUI", version="1.0.0")

# ì •ì  íŒŒì¼ ë° í…œí”Œë¦¿
app.mount("/static", StaticFiles(directory=ROOT_DIR / "static"), name="static")
app.mount("/outputs", StaticFiles(directory=OUTPUTS_DIR), name="outputs")
templates = Jinja2Templates(directory=ROOT_DIR / "templates")


# ============= ì „ì—­ ë³€ìˆ˜ =============
pipe = None
current_model = None
device = None
is_generating = False


# ============= Pydantic ëª¨ë¸ =============
class GenerateRequest(BaseModel):
    prompt: str
    width: int = 512
    height: int = 512
    steps: int = 8
    guidance_scale: float = 0.0
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


class ModelLoadRequest(BaseModel):
    quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"
    model_path: str = ""
    cpu_offload: bool = False


class SettingsRequest(BaseModel):
    openai_api_key: str = ""  # ë ˆê±°ì‹œ í˜¸í™˜
    output_path: str = ""
    filename_pattern: str = "{date}_{time}_{seed}"
    # LLM Provider ì„¤ì •
    llm_provider: str = ""
    llm_api_key: str = ""
    llm_base_url: str = ""
    llm_model: str = ""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²ˆì—­/í–¥ìƒ)
    translate_system_prompt: Optional[str] = None
    enhance_system_prompt: Optional[str] = None


class FavoriteRequest(BaseModel):
    name: str
    prompt: str
    settings: dict = {}


class TranslateRequest(BaseModel):
    text: str


class EnhanceRequest(BaseModel):
    prompt: str
    style: str = "ê¸°ë³¸"


# ============= ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =============
def get_device():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def image_to_base64(image: Image.Image) -> str:
    """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode()


def get_vram_info() -> str:
    """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
    if torch.cuda.is_available():
        vram_used = torch.cuda.memory_allocated() / 1024**3
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        return f"{vram_used:.1f}GB / {vram_total:.1f}GB"
    return "N/A"


# ============= ì›¹ì†Œì¼“ ì—°ê²° ê´€ë¦¬ =============
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    
    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
    
    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except:
                pass


manager = ConnectionManager()


# ============= API ì—”ë“œí¬ì¸íŠ¸ =============

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/api/status")
async def get_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    global pipe, current_model, device
    return {
        "model_loaded": pipe is not None,
        "current_model": current_model,
        "device": device or get_device(),
        "vram": get_vram_info(),
        "is_generating": is_generating,
        "upscaler_available": REALESRGAN_AVAILABLE,
    }


@app.post("/api/model/load")
async def load_model(request: ModelLoadRequest):
    """ëª¨ë¸ ë¡œë“œ"""
    global pipe, current_model, device
    
    device = get_device()
    quant_info = QUANTIZATION_OPTIONS.get(request.quantization)
    
    if not quant_info:
        raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {request.quantization}")
    
    repo_id = quant_info["repo"]
    dtype = quant_info["type"]
    is_gguf = quant_info.get("is_gguf", False)
    
    try:
        # 1ë‹¨ê³„: ë¡œë”© ì¤€ë¹„
        await manager.broadcast({
            "type": "model_progress", 
            "progress": 5, 
            "label": "ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...",
            "detail": f"ì–‘ìí™”: {dtype}, ë””ë°”ì´ìŠ¤: {device}",
            "stage": "init"
        })
        await asyncio.sleep(0.1)  # ë©”ì‹œì§€ ì „ì†¡ ëŒ€ê¸°
        
        from diffusers import ZImagePipeline
        
        if is_gguf:
            # GGUF ì–‘ìí™” ëª¨ë¸ ë¡œë“œ
            from diffusers import ZImageTransformer2DModel, GGUFQuantizationConfig
            from huggingface_hub import hf_hub_download
            
            filename = quant_info["filename"]
            
            # 2ë‹¨ê³„: GGUF ë‹¤ìš´ë¡œë“œ
            await manager.broadcast({
                "type": "model_progress", 
                "progress": 10, 
                "label": "ğŸ“¥ GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                "detail": f"íŒŒì¼: {filename} (ìºì‹œ í™•ì¸ ì¤‘...)",
                "stage": "download"
            })
            await asyncio.sleep(0.1)
            
            # GGUF íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ìºì‹œë¨)
            gguf_path = await asyncio.to_thread(
                hf_hub_download,
                repo_id=repo_id, 
                filename=filename,
                cache_dir=request.model_path if request.model_path else None
            )
            
            # 3ë‹¨ê³„: GGUF Transformer ë¡œë“œ
            await manager.broadcast({
                "type": "model_progress", 
                "progress": 30, 
                "label": "ğŸ”„ GGUF Transformer ë¡œë”© ì¤‘...",
                "detail": f"ì–‘ìí™” íƒ€ì…: {dtype} (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)",
                "stage": "load_transformer"
            })
            await asyncio.sleep(0.1)
            
            # GGUF Transformer ë¡œë“œ (ë™ê¸° ì‘ì—…ì„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
            transformer = await asyncio.to_thread(
                ZImageTransformer2DModel.from_single_file,
                gguf_path,
                quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
                torch_dtype=torch.bfloat16,
            )
            
            # 4ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ êµ¬ì„±
            await manager.broadcast({
                "type": "model_progress", 
                "progress": 55, 
                "label": "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...",
                "detail": "ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ ë° GGUF Transformer ê²°í•©",
                "stage": "load_pipeline"
            })
            await asyncio.sleep(0.1)
            
            # íŒŒì´í”„ë¼ì¸ êµ¬ì„± (GGUF transformer ì‚¬ìš©)
            pipe = await asyncio.to_thread(
                ZImagePipeline.from_pretrained,
                "Tongyi-MAI/Z-Image-Turbo",
                transformer=transformer,
                torch_dtype=torch.bfloat16,
            )
        else:
            # ê¸°ë³¸ BF16 ëª¨ë¸ ë¡œë“œ
            # 2ë‹¨ê³„: ë‹¤ìš´ë¡œë“œ/ë¡œë“œ
            await manager.broadcast({
                "type": "model_progress", 
                "progress": 15, 
                "label": "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                "detail": f"ì €ì¥ì†Œ: {repo_id} (ìºì‹œì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤)",
                "stage": "download"
            })
            await asyncio.sleep(0.1)
            
            load_kwargs = {
                "torch_dtype": torch.bfloat16,
            }
            if request.model_path:
                load_kwargs["cache_dir"] = request.model_path
            
            # 3ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ ë¡œë”©
            await manager.broadcast({
                "type": "model_progress", 
                "progress": 30, 
                "label": "ğŸ”„ ëª¨ë¸ íŒŒì¼ ë¡œë”© ì¤‘...",
                "detail": "ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ìºì‹œì—ì„œ ë¡œë“œ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª‡ ë¶„ ì†Œìš”)",
                "stage": "load_model"
            })
            await asyncio.sleep(0.1)
            
            pipe = await asyncio.to_thread(
                ZImagePipeline.from_pretrained,
                repo_id,
                **load_kwargs
            )
        
        # 5ë‹¨ê³„: ë””ë°”ì´ìŠ¤ ì „ì†¡
        await manager.broadcast({
            "type": "model_progress", 
            "progress": 75, 
            "label": f"ğŸš€ {device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...",
            "detail": "VRAMìœ¼ë¡œ ëª¨ë¸ ë³µì‚¬ ì¤‘... (VRAM í¬ê¸°ì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)",
            "stage": "to_device"
        })
        await asyncio.sleep(0.1)
        
        if request.cpu_offload:
            await asyncio.to_thread(pipe.enable_model_cpu_offload)
            await manager.broadcast({
                "type": "model_progress", 
                "progress": 95, 
                "label": "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì • ì¤‘...",
                "detail": "VRAM ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ RAM ì‚¬ìš©",
                "stage": "cpu_offload"
            })
        else:
            await asyncio.to_thread(pipe.to, device)
        
        current_model = request.quantization
        
        # 6ë‹¨ê³„: ì™„ë£Œ
        await manager.broadcast({
            "type": "model_progress", 
            "progress": 100, 
            "label": "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!",
            "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}",
            "stage": "complete"
        })
        
        await manager.broadcast({
            "type": "complete",
            "content": f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({dtype}, {device})"
        })
        
        return {"success": True, "message": f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {repo_id} ({dtype})"}
        
    except Exception as e:
        await manager.broadcast({
            "type": "model_progress", 
            "progress": 0, 
            "label": "âŒ ë¡œë“œ ì‹¤íŒ¨",
            "detail": str(e),
            "stage": "error"
        })
        await manager.broadcast({"type": "error", "content": f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"})
        raise HTTPException(500, str(e))


@app.post("/api/model/unload")
async def unload_model():
    """ëª¨ë¸ ì–¸ë¡œë“œ"""
    global pipe, current_model
    
    if pipe is None:
        return {"success": True, "message": "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}
    
    try:
        await manager.broadcast({
            "type": "model_progress", 
            "progress": 30, 
            "label": "ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...",
            "detail": ""
        })
        
        del pipe
        pipe = None
        current_model = None
        
        await manager.broadcast({
            "type": "model_progress", 
            "progress": 60, 
            "label": "VRAM ì •ë¦¬ ì¤‘...",
            "detail": ""
        })
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        gc.collect()
        
        await manager.broadcast({
            "type": "model_progress", 
            "progress": 100, 
            "label": "ì–¸ë¡œë“œ ì™„ë£Œ!",
            "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}"
        })
        
        await manager.broadcast({"type": "complete", "content": "âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!"})
        return {"success": True, "message": "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"}
        
    except Exception as e:
        raise HTTPException(500, str(e))


@app.post("/api/generate")
async def generate_image(request: GenerateRequest):
    """ì´ë¯¸ì§€ ìƒì„±"""
    global pipe, is_generating
    
    if pipe is None:
        raise HTTPException(400, "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if is_generating:
        raise HTTPException(400, "ì´ë¯¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤.")
    
    if not request.prompt.strip():
        raise HTTPException(400, "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    is_generating = True
    
    try:
        # ë²ˆì—­
        final_prompt = request.prompt
        if request.auto_translate and translator.is_korean(request.prompt):
            await manager.broadcast({"type": "system", "content": "ğŸŒ í”„ë¡¬í”„íŠ¸ ë²ˆì—­ ì¤‘..."})
            final_prompt, success = translator.translate(request.prompt)
            if not success:
                await manager.broadcast({"type": "warning", "content": "âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"})
        
        # ì‹œë“œ ì„¤ì •
        seed = request.seed if request.seed != -1 else random.randint(0, 2147483647)
        
        images = []
        for i in range(request.num_images):
            current_seed = seed + i
            await manager.broadcast({
                "type": "progress",
                "content": f"ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘... ({i+1}/{request.num_images})"
            })
            
            generator = torch.Generator(device).manual_seed(current_seed)
            
            image = pipe(
                prompt=final_prompt,
                height=request.height,
                width=request.width,
                num_inference_steps=request.steps,
                guidance_scale=request.guidance_scale,
                generator=generator,
            ).images[0]
            
            # ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥
            metadata = ImageMetadata.create_metadata(
                prompt=final_prompt,
                seed=current_seed,
                width=request.width,
                height=request.height,
                steps=request.steps,
                guidance_scale=request.guidance_scale,
                model=current_model or "unknown",
            )
            
            OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
            filename = filename_generator.generate(
                pattern=settings.get("filename_pattern", "{date}_{time}_{seed}"),
                prompt=final_prompt,
                seed=current_seed
            )
            output_path = OUTPUTS_DIR / filename
            ImageMetadata.save_with_metadata(image, output_path, metadata)
            
            images.append({
                "base64": image_to_base64(image),
                "filename": filename,
                "seed": current_seed,
                "path": f"/outputs/{filename}"
            })
        
        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        history_manager.add(
            prompt=request.prompt,
            settings={
                "width": request.width,
                "height": request.height,
                "steps": request.steps,
                "guidance_scale": request.guidance_scale,
                "seed": seed,
            }
        )
        
        await manager.broadcast({
            "type": "complete",
            "content": f"âœ… {len(images)}ì¥ ìƒì„± ì™„ë£Œ! (ì‹œë“œ: {seed})"
        })
        
        return {"success": True, "images": images, "seed": seed, "prompt": final_prompt}
        
    except Exception as e:
        await manager.broadcast({"type": "error", "content": f"âŒ ìƒì„± ì˜¤ë¥˜: {str(e)}"})
        raise HTTPException(500, str(e))
    
    finally:
        is_generating = False


@app.post("/api/preview")
async def generate_preview(request: GenerateRequest):
    """ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° (256x256)"""
    request.width = 256
    request.height = 256
    request.steps = min(request.steps, 4)
    request.num_images = 1
    return await generate_image(request)


@app.post("/api/translate")
async def translate_text(request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ë²ˆì—­ (í•œêµ­ì–´ â†’ ì˜ì–´)"""
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    translated, success = translator.translate(request.text)
    return {"success": success, "translated": translated}


@app.post("/api/translate-reverse")
async def reverse_translate_text(request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ì—­ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)"""
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    translated, success = translator.reverse_translate(request.text)
    return {"success": success, "translated": translated}


@app.post("/api/enhance")
async def enhance_prompt(request: EnhanceRequest):
    """í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì •ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    enhanced, success = prompt_enhancer.enhance(request.prompt, request.style)
    return {"success": success, "enhanced": enhanced}


@app.get("/api/templates")
async def get_templates():
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ëª©ë¡"""
    return {"templates": PROMPT_TEMPLATES}


@app.get("/api/model-status")
async def get_model_download_status():
    """ê° ëª¨ë¸ì˜ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸"""
    from huggingface_hub import try_to_load_from_cache, scan_cache_dir
    import os
    
    status = {}
    
    for option_name, option_info in QUANTIZATION_OPTIONS.items():
        is_downloaded = False
        
        try:
            if option_info.get("is_gguf", False):
                # GGUF ëª¨ë¸: íŠ¹ì • íŒŒì¼ì´ ìºì‹œì— ìˆëŠ”ì§€ í™•ì¸
                filename = option_info.get("filename", "")
                repo_id = option_info.get("repo", "")
                
                if filename and repo_id:
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename=filename
                    )
                    is_downloaded = cached_path is not None
            else:
                # BF16 ëª¨ë¸: diffusers ìºì‹œ í™•ì¸
                repo_id = option_info.get("repo", "")
                if repo_id:
                    # model_index.jsonì´ ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename="model_index.json"
                    )
                    is_downloaded = cached_path is not None
        except Exception as e:
            print(f"ìºì‹œ í™•ì¸ ì˜¤ë¥˜ ({option_name}): {e}")
            is_downloaded = False
        
        status[option_name] = is_downloaded
    
    return {"status": status}


@app.get("/api/history")
async def get_history():
    """íˆìŠ¤í† ë¦¬ ëª©ë¡"""
    entries = history_manager.get_all()
    return {"history": [e.to_dict() for e in entries[:50]]}


@app.delete("/api/history")
async def clear_history():
    """íˆìŠ¤í† ë¦¬ ì‚­ì œ"""
    history_manager.clear()
    return {"success": True}


@app.get("/api/favorites")
async def get_favorites():
    """ì¦ê²¨ì°¾ê¸° ëª©ë¡"""
    entries = favorites_manager.get_all()
    return {"favorites": [e.to_dict() for e in entries]}


@app.post("/api/favorites")
async def add_favorite(request: FavoriteRequest):
    """ì¦ê²¨ì°¾ê¸° ì¶”ê°€"""
    entry = favorites_manager.add(
        name=request.name,
        prompt=request.prompt,
        settings=request.settings
    )
    return {"success": True, "id": entry.id}


@app.delete("/api/favorites/{fav_id}")
async def delete_favorite(fav_id: str):
    """ì¦ê²¨ì°¾ê¸° ì‚­ì œ"""
    success = favorites_manager.delete(fav_id)
    return {"success": success}


@app.get("/api/gallery")
async def get_gallery():
    """ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ ëª©ë¡"""
    images = []
    if OUTPUTS_DIR.exists():
        for f in sorted(OUTPUTS_DIR.glob("*.png"), key=lambda x: x.stat().st_mtime, reverse=True)[:50]:
            metadata = ImageMetadata.read_metadata(f)
            images.append({
                "filename": f.name,
                "path": f"/outputs/{f.name}",
                "metadata": metadata
            })
    return {"images": images}


@app.post("/api/settings")
async def save_settings(request: SettingsRequest):
    """ì„¤ì • ì €ì¥"""
    from utils.llm_client import llm_client
    
    # ë ˆê±°ì‹œ í˜¸í™˜: openai_api_keyê°€ ìˆê³  llm_api_keyê°€ ì—†ìœ¼ë©´ ë™ê¸°í™”
    if request.openai_api_key:
        settings.set("openai_api_key", request.openai_api_key)
        # ë ˆê±°ì‹œ í˜¸í™˜ ìœ ì§€
        translator.set_api_key(request.openai_api_key)
        prompt_enhancer.set_api_key(request.openai_api_key)
    
    # LLM Provider ì„¤ì •
    if request.llm_provider:
        settings.set("llm_provider", request.llm_provider)
    
    if request.llm_api_key:
        settings.set("llm_api_key", request.llm_api_key)
        # ë ˆê±°ì‹œ í˜¸í™˜ ë™ê¸°í™”
        settings.set("openai_api_key", request.llm_api_key)
    
    if request.llm_base_url is not None:
        settings.set("llm_base_url", request.llm_base_url)
    
    if request.llm_model is not None:
        settings.set("llm_model", request.llm_model)
    
    # LLM í´ë¼ì´ì–¸íŠ¸ ìºì‹œ ë¬´íš¨í™” (ì„¤ì • ë³€ê²½ ë°˜ì˜)
    llm_client.invalidate()
    
    if request.output_path:
        settings.set("output_path", request.output_path)
    
    if request.filename_pattern:
        settings.set("filename_pattern", request.filename_pattern)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if request.translate_system_prompt is not None:
        settings.set("translate_system_prompt", request.translate_system_prompt)
    
    if request.enhance_system_prompt is not None:
        settings.set("enhance_system_prompt", request.enhance_system_prompt)
    
    return {"success": True}


@app.get("/api/settings")
async def get_settings():
    """ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    from utils.settings import LLM_PROVIDERS
    from utils.translator import Translator
    from utils.prompt_enhancer import PromptEnhancer
    
    return {
        # ë ˆê±°ì‹œ í˜¸í™˜
        "openai_api_key": "***" if settings.get("openai_api_key") else "",
        # LLM Provider ì„¤ì •
        "llm_provider": settings.get("llm_provider", "openai"),
        "llm_api_key": "***" if settings.get("llm_api_key") else "",
        "llm_base_url": settings.get("llm_base_url", ""),
        "llm_model": settings.get("llm_model", ""),
        "llm_providers": {
            pid: {
                "name": pinfo["name"],
                "base_url": pinfo["base_url"],
                "default_model": pinfo["default_model"],
                "models": pinfo["models"],
            }
            for pid, pinfo in LLM_PROVIDERS.items()
        },
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²ˆì—­/í–¥ìƒ)
        "translate_system_prompt": settings.get("translate_system_prompt") or Translator.DEFAULT_SYSTEM_PROMPT,
        "enhance_system_prompt": settings.get("enhance_system_prompt") or PromptEnhancer.DEFAULT_SYSTEM_PROMPT,
        "default_translate_system_prompt": Translator.DEFAULT_SYSTEM_PROMPT,
        "default_enhance_system_prompt": PromptEnhancer.DEFAULT_SYSTEM_PROMPT,
        # ê¸°íƒ€ ì„¤ì •
        "output_path": str(settings.get("output_path", OUTPUTS_DIR)),
        "filename_pattern": settings.get("filename_pattern", "{date}_{time}_{seed}"),
        "quantization_options": list(QUANTIZATION_OPTIONS.keys()),
        "resolution_presets": RESOLUTION_PRESETS,
    }


# ============= WebSocket =============
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """ì›¹ì†Œì¼“ ì—°ê²°"""
    await manager.connect(websocket)
    try:
        # ì—°ê²° ì‹œ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "connected",
            "content": "ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤."
        })
        
        while True:
            data = await websocket.receive_text()
            # í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬ (í•„ìš”ì‹œ)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)


# ============= ë©”ì¸ =============
if __name__ == "__main__":
    # ì¶œë ¥ í´ë” ìƒì„±
    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ¨ Z-Image WebUI ì‹œì‘...")
    print("ğŸ“ http://localhost:7860")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=7860,
        reload=False
    )
