"""Z-Image WebUI - FastAPI ê¸°ë°˜ ëŒ€í™”í˜• ì´ë¯¸ì§€ ìƒì„± ì›¹ì•± (ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›)"""

import os
import sys
import json
import asyncio
import base64
import random
import gc
import time
import inspect
from pathlib import Path
from datetime import datetime
from typing import Optional, List, Dict, Any
from io import BytesIO
from contextlib import asynccontextmanager

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
ROOT_DIR = Path(__file__).parent
sys.path.insert(0, str(ROOT_DIR))

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File, Form, Response, Cookie
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse, JSONResponse, FileResponse
from fastapi.requests import Request
from pydantic import BaseModel
import uvicorn

import torch
from PIL import Image

# ë¡œì»¬ ëª¨ë“ˆ
from config.defaults import (
    QUANTIZATION_OPTIONS,
    EDIT_QUANTIZATION_OPTIONS,
    RESOLUTION_PRESETS,
    OUTPUTS_DIR,
    MODELS_DIR,
    SERVER_HOST,
    SERVER_PORT,
    SERVER_RELOAD,
    LONGCAT_EDIT_AUTO_UNLOAD_TIMEOUT,
    DEFAULT_GPU_SETTINGS,
)
from config.templates import PROMPT_TEMPLATES
from utils.settings import settings
from utils.translator import translator
from utils.prompt_enhancer import prompt_enhancer
from utils.metadata import ImageMetadata, filename_generator
from utils.history import get_history_manager_sync, HistoryManager
from utils.favorites import get_favorites_manager_sync, FavoritesManager
from utils.upscaler import upscaler, REALESRGAN_AVAILABLE
from utils.session import session_manager, is_localhost, SessionManager, SessionInfo
from utils.queue_manager import generation_queue, GenerationQueueManager
from utils.auth import auth_manager, User
from utils.longcat_edit import longcat_edit_manager
from utils.edit_history import get_edit_history_manager_sync, EditHistoryManager
from utils.edit_llm import edit_translator, edit_enhancer, edit_suggester
from utils.gpu_monitor import gpu_monitor


# ============= ì „ì—­ ë³€ìˆ˜ =============
pipe = None
current_model = None
device = None
last_activity_time = time.time()  # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
auto_unload_task = None  # ìë™ ì–¸ë¡œë“œ ì²´í¬ íƒœìŠ¤í¬
model_lock = asyncio.Lock()  # ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ì ê¸ˆ

# LongCat-Image-Edit ê´€ë ¨
edit_last_activity_time = time.time()  # í¸ì§‘ ëª¨ë¸ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„
edit_auto_unload_task = None  # í¸ì§‘ ëª¨ë¸ ìë™ ì–¸ë¡œë“œ íƒœìŠ¤í¬
edit_model_lock = asyncio.Lock()  # í¸ì§‘ ëª¨ë¸ ë¡œë“œ/ì–¸ë¡œë“œ ì ê¸ˆ


# ============= ìë™ ì–¸ë¡œë“œ ê´€ë ¨ í•¨ìˆ˜ =============
def update_activity():
    """ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸"""
    global last_activity_time
    last_activity_time = time.time()


async def auto_unload_checker():
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ ì–¸ë¡œë“œ ì²´í¬"""
    global pipe, current_model, last_activity_time
    
    while True:
        await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
        
        # ìë™ ì–¸ë¡œë“œ ì„¤ì • í™•ì¸
        if not settings.get("auto_unload_enabled", True):
            continue
        
        # ëª¨ë¸ì´ ë¡œë“œë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ìŠ¤í‚µ
        if pipe is None:
            continue
        
        # ìƒì„± ì¤‘ì´ë©´ ìŠ¤í‚µ
        if generation_queue.is_processing():
            update_activity()  # ìƒì„± ì¤‘ì—ëŠ” í™œë™ìœ¼ë¡œ ê°„ì£¼
            continue
        
        # íƒ€ì„ì•„ì›ƒ ì²´í¬
        timeout_minutes = settings.get("auto_unload_timeout", 10)
        timeout_seconds = timeout_minutes * 60
        elapsed = time.time() - last_activity_time
        
        if elapsed >= timeout_seconds:
            print(f"â° ìë™ ì–¸ë¡œë“œ: {timeout_minutes}ë¶„ ë™ì•ˆ í™œë™ì´ ì—†ì–´ ëª¨ë¸ì„ ì–¸ë¡œë“œí•©ë‹ˆë‹¤.")
            
            try:
                # GPU ëª¨ë‹ˆí„°ì—ì„œ ëª¨ë¸ ë“±ë¡ í•´ì œ
                gpu_monitor.unregister_model("Z-Image-Turbo")
                
                # ëª¨ë¸ ì–¸ë¡œë“œ
                del pipe
                pipe = None
                current_model = None
                
                # GPU ìºì‹œ ì •ë¦¬ (ìƒì„± ëª¨ë¸ì´ ì˜¬ë ¤ì ¸ ìˆë˜ ë””ë°”ì´ìŠ¤ë§Œ ì •ë¦¬)
                # - í¸ì§‘ ëª¨ë¸ì´ ë‹¤ë¥¸ GPUì— ì˜¬ë¼ê°„ ê²½ìš°ê¹Œì§€ ì˜í–¥ì„ ì£¼ì§€ ì•Šë„ë¡ ë²”ìœ„ë¥¼ ì œí•œí•œë‹¤.
                gpu_monitor.clear_cache(device)
                gc.collect()
                
                # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì•Œë¦¼
                await ws_manager.broadcast({
                    "type": "system",
                    "content": f"â° {timeout_minutes}ë¶„ ë™ì•ˆ í™œë™ì´ ì—†ì–´ ëª¨ë¸ì´ ìë™ ì–¸ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. VRAMì„ ì ˆì•½í•©ë‹ˆë‹¤."
                })
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 100, 
                    "label": "â° ìë™ ì–¸ë¡œë“œ ì™„ë£Œ",
                    "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}",
                    "stage": "complete"
                })
                
                print(f"âœ… ìë™ ì–¸ë¡œë“œ ì™„ë£Œ. VRAM: {get_vram_info()}")
                
            except Exception as e:
                print(f"âŒ ìë™ ì–¸ë¡œë“œ ì‹¤íŒ¨: {e}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” lifespan í•¸ë“¤ëŸ¬"""
    global auto_unload_task
    
    # ì‹œì‘ ì‹œ: ìë™ ì–¸ë¡œë“œ ì²´í¬ íƒœìŠ¤í¬ ì‹œì‘
    auto_unload_task = asyncio.create_task(auto_unload_checker())
    print("ğŸ”„ ìë™ ì–¸ë¡œë“œ ì²´ì»¤ ì‹œì‘ë¨")
    
    # í ì›Œì»¤ ì‹œì‘
    await generation_queue.start_worker()
    print("ğŸ”„ ì´ë¯¸ì§€ ìƒì„± í ì›Œì»¤ ì‹œì‘ë¨")
    
    # í ì½œë°± ì„¤ì •
    generation_queue.set_callbacks(
        on_status_change=on_queue_status_change,
        on_broadcast=on_queue_broadcast,
        generate_func=execute_generation
    )
    
    yield
    
    # ì¢…ë£Œ ì‹œ: íƒœìŠ¤í¬ ì·¨ì†Œ
    if auto_unload_task:
        auto_unload_task.cancel()
        try:
            await auto_unload_task
        except asyncio.CancelledError:
            pass
    
    # í ì›Œì»¤ ì¤‘ì§€
    await generation_queue.stop_worker()


# ============= FastAPI ì•± ì„¤ì • =============
app = FastAPI(title="Z-Image WebUI", version="2.0.0", lifespan=lifespan)

# ì •ì  íŒŒì¼ ë° í…œí”Œë¦¿
app.mount("/static", StaticFiles(directory=ROOT_DIR / "static"), name="static")
templates = Jinja2Templates(directory=ROOT_DIR / "templates")


# ============= Pydantic ëª¨ë¸ =============
class GenerateRequest(BaseModel):
    prompt: str
    korean_prompt: str = ""  # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ (ì›ë³¸)
    width: int = 512
    height: int = 512
    steps: int = 8
    guidance_scale: float = 0.0
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


class ModelLoadRequest(BaseModel):
    quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"
    cpu_offload: bool = False
    target_device: str = "auto"  # ê´€ë¦¬ì ì „ìš©: "auto", "cuda:0", "cuda:1", "cpu", "mps"


class SettingsRequest(BaseModel):
    openai_api_key: str = ""  # ë ˆê±°ì‹œ í˜¸í™˜
    output_path: str = ""
    filename_pattern: str = "{date}_{time}_{seed}"
    # LLM Provider ì„¤ì •
    llm_provider: str = ""
    llm_api_key: str = ""
    # NOTE:
    # - /api/settings ëŠ” ë‹¤ì–‘í•œ ì„¤ì •(ìë™ ì–¸ë¡œë“œ ë“±) ì €ì¥ì—ë„ ì¬ì‚¬ìš©ëœë‹¤.
    # - ì•„ë˜ ê°’ë“¤ì„ ê¸°ë³¸ê°’ ""ë¡œ ë‘ë©´, ìš”ì²­ ë°”ë””ì— í•´ë‹¹ í•„ë“œê°€ ì—†ì–´ë„ Pydanticì´ ""ë¥¼ ì±„ì›Œë„£ì–´
    #   ì €ì¥ ì‹œ ê¸°ì¡´ ê°’ì´ ""ë¡œ ë®ì—¬ì„œ "ì„¤ì •ì´ í’€ë¦¬ëŠ”" ë¬¸ì œê°€ ë°œìƒí•œë‹¤.
    # - ë”°ë¼ì„œ Optionalë¡œ ë‘ê³ , ì‹¤ì œë¡œ ê°’ì´ ì „ë‹¬ëœ ê²½ìš°ì—ë§Œ(= Noneì´ ì•„ë‹ ë•Œë§Œ) ì €ì¥í•œë‹¤.
    llm_base_url: Optional[str] = None
    llm_model: Optional[str] = None
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²ˆì—­/í–¥ìƒ)
    translate_system_prompt: Optional[str] = None
    enhance_system_prompt: Optional[str] = None
    # ìë™ ì–¸ë¡œë“œ ì„¤ì •
    auto_unload_enabled: Optional[bool] = None
    auto_unload_timeout: Optional[int] = None
    # í¸ì§‘ ëª¨ë¸ ìë™ ì–¸ë¡œë“œ ì„¤ì •
    edit_auto_unload_enabled: Optional[bool] = None
    edit_auto_unload_timeout: Optional[int] = None

    # ëª¨ë¸ ì„¤ì • (ê´€ë¦¬ì ì „ìš©)
    quantization: Optional[str] = None
    cpu_offload: Optional[bool] = None
    # í¸ì§‘ ëª¨ë¸ ì„¤ì • (ê´€ë¦¬ì ì „ìš©)
    edit_quantization: Optional[str] = None
    edit_cpu_offload: Optional[bool] = None


class FavoriteRequest(BaseModel):
    name: str
    prompt: str
    settings: dict = {}


class TranslateRequest(BaseModel):
    text: str


class EnhanceRequest(BaseModel):
    prompt: str
    style: str = "ê¸°ë³¸"


class ConversationUpdateRequest(BaseModel):
    conversation: List[Dict[str, Any]]


# ============= í¸ì§‘ ê´€ë ¨ Pydantic ëª¨ë¸ =============
class EditModelLoadRequest(BaseModel):
    quantization: str = "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"
    model_path: str = ""
    cpu_offload: bool = True  # ê¸°ë³¸ í™œì„±í™” (VRAM ì ˆì•½)
    target_device: str = "auto"  # ê´€ë¦¬ì ì „ìš©: "auto", "cuda:0", "cuda:1", "cpu", "mps"


class EditGenerateRequest(BaseModel):
    prompt: str
    korean_prompt: str = ""
    steps: int = 50
    guidance_scale: float = 4.5
    seed: int = -1
    num_images: int = 1
    auto_translate: bool = True


class EditTranslateRequest(BaseModel):
    text: str


class EditEnhanceRequest(BaseModel):
    instruction: str


class EditSuggestRequest(BaseModel):
    context: str = ""
    image_description: str = ""


class EditConversationUpdateRequest(BaseModel):
    conversation: List[Dict[str, Any]]


# ============= ì¸ì¦ ê´€ë ¨ Pydantic ëª¨ë¸ =============
class RegisterRequest(BaseModel):
    username: str
    password: str
    password_confirm: str


class LoginRequest(BaseModel):
    username: str
    password: str


class ChangePasswordRequest(BaseModel):
    current_password: str
    new_password: str
    new_password_confirm: str


class ResetPasswordRequest(BaseModel):
    new_password: Optional[str] = None  # Noneì´ë©´ ì„ì‹œ ë¹„ë°€ë²ˆí˜¸ ìë™ ìƒì„±


# ============= ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =============
def get_device(target_device: str = "auto") -> str:
    """
    ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ë°˜í™˜
    
    Args:
        target_device: ëª©í‘œ ë””ë°”ì´ìŠ¤ ("auto", "cuda:0", "cuda:1", "cpu", "mps")
    
    Returns:
        ì‹¤ì œ ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
    """
    return gpu_monitor.resolve_device(target_device, prefer_empty=True)


def image_to_base64(image: Image.Image) -> str:
    """PIL ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜"""
    buffer = BytesIO()
    image.save(buffer, format="PNG")
    return base64.b64encode(buffer.getvalue()).decode()


def get_vram_info() -> str:
    """VRAM ì‚¬ìš©ëŸ‰ ì •ë³´"""
    return gpu_monitor.get_vram_summary()


async def get_session_from_request(request: Request, create_if_missing: bool = False) -> Optional[SessionInfo]:
    """
    ìš”ì²­ì—ì„œ ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°
    - ê¸°ë³¸: **ë¹„ë¡œê·¸ì¸(ì¿ í‚¤ ì—†ìŒ/ìœ íš¨í•˜ì§€ ì•ŠìŒ/ë©”ëª¨ë¦¬ì— ì—†ìŒ)ì—ì„œëŠ” ì„¸ì…˜ì„ ìƒì„±í•˜ì§€ ì•ŠìŒ**
    - ë¡œê·¸ì¸/íšŒì›ê°€ì… ë“± ì¼ë¶€ ì—”ë“œí¬ì¸íŠ¸ì—ì„œë§Œ create_if_missing=Trueë¡œ ì„¸ì…˜ ìƒì„±
    """
    session_id = request.cookies.get(SessionManager.COOKIE_NAME)

    # ì¿ í‚¤ê°€ ìˆê³  ë©”ëª¨ë¦¬ì— ì‚´ì•„ìˆëŠ” ì„¸ì…˜ì´ë©´ ë°˜í™˜
    if session_id and session_manager.validate_session_id(session_id):
        session = session_manager.get_session(session_id)
        if session:
            session.update_activity()
            return session

    # í•„ìš”í•œ ê²½ìš°ì—ë§Œ ìƒˆ ì„¸ì…˜ ìƒì„±
    if create_if_missing:
        return await session_manager.get_or_create_session(session_id)

    return None


def require_auth(session: Optional[SessionInfo]) -> None:
    """ì¸ì¦ í•„ìˆ˜ ì²´í¬ - ë¡œê·¸ì¸í•˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ"""
    if not session or not session.is_authenticated:
        raise HTTPException(401, "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")


def require_admin(request: Request) -> None:
    """ê´€ë¦¬ì ê¶Œí•œ ì²´í¬ - localhostê°€ ì•„ë‹ˆë©´ ì˜ˆì™¸ ë°œìƒ"""
    client_host = request.client.host if request.client else None
    if not is_localhost(client_host):
        raise HTTPException(403, "ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.")


def set_session_cookie(response: Response, session: Optional[SessionInfo]):
    """ì‘ë‹µì— ì„¸ì…˜ ì¿ í‚¤ ì„¤ì •"""
    if not session:
        return
    response.set_cookie(
        key=SessionManager.COOKIE_NAME,
        value=session.session_id,
        max_age=SessionManager.COOKIE_MAX_AGE,
        httponly=True,
        samesite="lax"
    )


def clear_session_cookie(response: Response):
    """ì„¸ì…˜ ì¿ í‚¤ ì œê±°"""
    response.delete_cookie(key=SessionManager.COOKIE_NAME)


# ============= ì›¹ì†Œì¼“ ì—°ê²° ê´€ë¦¬ (ì„¸ì…˜ë³„) =============
class SessionConnectionManager:
    """ì„¸ì…˜ë³„ WebSocket ì—°ê²° ê´€ë¦¬"""
    
    def __init__(self):
        # session_id -> List[WebSocket]
        self._connections: Dict[str, List[WebSocket]] = {}
        self._websocket_sessions: Dict[WebSocket, str] = {}  # ì—­ë°©í–¥ ë§¤í•‘
        self._lock = asyncio.Lock()
    
    async def connect(self, websocket: WebSocket, session_id: str):
        """ì—°ê²° ì¶”ê°€"""
        await websocket.accept()
        async with self._lock:
            if session_id not in self._connections:
                self._connections[session_id] = []
            self._connections[session_id].append(websocket)
            self._websocket_sessions[websocket] = session_id
    
    async def disconnect(self, websocket: WebSocket):
        """ì—°ê²° ì œê±°"""
        async with self._lock:
            session_id = self._websocket_sessions.get(websocket)
            if session_id:
                if session_id in self._connections:
                    if websocket in self._connections[session_id]:
                        self._connections[session_id].remove(websocket)
                    # ì„¸ì…˜ì˜ ëª¨ë“  ì—°ê²°ì´ ëŠì–´ì§€ë©´ íì—ì„œ ì œê±°
                    if not self._connections[session_id]:
                        del self._connections[session_id]
                        # íì—ì„œ í•´ë‹¹ ì„¸ì…˜ ìš”ì²­ ì œê±°
                        await generation_queue.remove_session_items(session_id)
                del self._websocket_sessions[websocket]
    
    async def send_to_session(self, session_id: str, message: dict):
        """íŠ¹ì • ì„¸ì…˜ì— ë©”ì‹œì§€ ì „ì†¡"""
        async with self._lock:
            connections = self._connections.get(session_id, [])
            for ws in connections:
                try:
                    await ws.send_json(message)
                except:
                    pass
    
    async def broadcast(self, message: dict):
        """ëª¨ë“  ì—°ê²°ì— ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
        async with self._lock:
            for connections in self._connections.values():
                for ws in connections:
                    try:
                        await ws.send_json(message)
                    except:
                        pass
    
    def get_connection_count(self) -> int:
        """ì´ ì—°ê²° ìˆ˜"""
        return sum(len(conns) for conns in self._connections.values())
    
    def get_session_count(self) -> int:
        """ì—°ê²°ëœ ì„¸ì…˜ ìˆ˜"""
        return len(self._connections)

    def get_connected_keys(self) -> List[str]:
        """í˜„ì¬ ì—°ê²°ëœ í‚¤ ëª©ë¡ (í˜„ì¬ëŠ” user_{id} í˜•íƒœ)"""
        return list(self._connections.keys())

    async def disconnect_key(self, key: str) -> int:
        """íŠ¹ì • í‚¤(user_{id})ì˜ ëª¨ë“  WebSocket ì—°ê²° ì¢…ë£Œ"""
        async with self._lock:
            connections = list(self._connections.get(key, []))
        closed = 0
        for ws in connections:
            try:
                await ws.close(code=4000)
                closed += 1
            except Exception:
                pass
        return closed
    
    def get_session_id(self, websocket: WebSocket) -> Optional[str]:
        """WebSocketì˜ ì„¸ì…˜ ID ê°€ì ¸ì˜¤ê¸°"""
        return self._websocket_sessions.get(websocket)


ws_manager = SessionConnectionManager()


def _format_bytes(total_size: int) -> str:
    """ë°”ì´íŠ¸ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë‹¨ìœ„ë¡œ ë³€í™˜"""
    if total_size < 1024:
        return f"{total_size} B"
    if total_size < 1024 * 1024:
        return f"{total_size / 1024:.1f} KB"
    if total_size < 1024 * 1024 * 1024:
        return f"{total_size / (1024 * 1024):.1f} MB"
    return f"{total_size / (1024 * 1024 * 1024):.2f} GB"


def _get_data_size_by_data_id(data_id: str) -> str:
    """data_id(user_{id}) ê¸°ì¤€ ë°ì´í„° í¬ê¸° ê³„ì‚° (ì„¸ì…˜ í™”ë©´ìš©)"""
    from config.defaults import DATA_DIR, OUTPUTS_DIR
    total_size = 0
    sessions_dir = DATA_DIR / "sessions" / data_id
    outputs_dir = OUTPUTS_DIR / data_id

    for d in (sessions_dir, outputs_dir):
        if d.exists():
            for f in d.rglob("*"):
                if f.is_file():
                    try:
                        total_size += f.stat().st_size
                    except Exception:
                        pass
    return _format_bytes(total_size)


def _parse_user_id_from_data_id(data_id: str) -> Optional[int]:
    """user_123 -> 123"""
    if not isinstance(data_id, str):
        return None
    if not data_id.startswith("user_"):
        return None
    try:
        return int(data_id.split("_", 1)[1])
    except Exception:
        return None


# ============= í ì½œë°± í•¨ìˆ˜ë“¤ =============
async def on_queue_status_change(session_id: str, event_type: str, data: dict):
    """í ìƒíƒœ ë³€ê²½ ì‹œ ì„¸ì…˜ì— ì•Œë¦¼"""
    if event_type == "generation_start":
        await ws_manager.send_to_session(session_id, {
            "type": "queue_status",
            "status": "processing",
            "position": 0,
            "message": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
        })
    elif event_type == "queue_position":
        await ws_manager.send_to_session(session_id, {
            "type": "queue_status",
            "status": "waiting",
            "position": data["position"],
            "message": f"â³ ëŒ€ê¸° ì¤‘... (ìˆœì„œ: {data['position']})"
        })
    elif event_type == "generation_error":
        await ws_manager.send_to_session(session_id, {
            "type": "error",
            "content": f"âŒ ìƒì„± ì˜¤ë¥˜: {data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        })
    elif event_type == "generation_complete":
        # ê²°ê³¼ëŠ” execute_generationì—ì„œ ì§ì ‘ ì „ì†¡
        pass


async def on_queue_broadcast(data: dict):
    """í ìƒíƒœ ì „ì²´ ë¸Œë¡œë“œìºìŠ¤íŠ¸"""
    await ws_manager.broadcast(data)


async def execute_generation(request_data: dict) -> dict:
    """ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„± ì‹¤í–‰"""
    global pipe, current_model
    
    session_id = request_data.get("session_id")
    # ê³„ì • ë‹¨ìœ„(data_id)ë¡œ ì‹¤í–‰ ì‹œ SessionInfoê°€ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜µì…˜ ì²˜ë¦¬
    session = session_manager.get_session(session_id)
    
    if pipe is None:
        raise Exception("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    prompt = request_data.get("prompt", "")
    korean_prompt = request_data.get("korean_prompt", "")
    width = request_data.get("width", 512)
    height = request_data.get("height", 512)
    steps = request_data.get("steps", 8)
    guidance_scale = request_data.get("guidance_scale", 0.0)
    seed = request_data.get("seed", -1)
    num_images = request_data.get("num_images", 1)
    auto_translate = request_data.get("auto_translate", True)
    
    # ë²ˆì—­
    final_prompt = prompt
    if auto_translate and translator.is_korean(prompt):
        await ws_manager.send_to_session(session_id, {
            "type": "system",
            "content": "ğŸŒ í”„ë¡¬í”„íŠ¸ ë²ˆì—­ ì¤‘..."
        })
        final_prompt, success = translator.translate(prompt)
        if not success:
            await ws_manager.send_to_session(session_id, {
                "type": "warning",
                "content": "âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"
            })
    
    # ì‹œë“œ ì„¤ì •
    if seed == -1:
        seed = random.randint(0, 2147483647)
    
    # ìƒì„± ì‹œì‘ ë©”ì‹œì§€
    await ws_manager.send_to_session(session_id, {
        "type": "system",
        "content": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ì¤‘..."
    })
    
    # ìƒì„± ì‹œì‘ ì „ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    
    # ì„¸ì…˜ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
    if session:
        outputs_dir = session.get_outputs_dir()
    else:
        # session_idê°€ user_{id} í˜•íƒœë©´ outputs/user_{id}ì— ì €ì¥
        if isinstance(session_id, str) and session_id.startswith("user_"):
            outputs_dir = OUTPUTS_DIR / session_id
        else:
            outputs_dir = OUTPUTS_DIR
    
    images = []
    for i in range(num_images):
        current_seed = seed + i
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì—…ë°ì´íŠ¸
        percent = ((i) / num_images) * 100
        await ws_manager.send_to_session(session_id, {
            "type": "image_progress",
            "progress": percent,
            "current": i + 1,
            "total": num_images
        })
        await asyncio.sleep(0.05)
        
        generator = torch.Generator(device).manual_seed(current_seed)
        loop = asyncio.get_running_loop()
        last_sent_step = {"value": -1}  # í´ë¡œì €ì—ì„œ mutableë¡œ ì‚¬ìš©

        def _send_generation_progress_from_thread(current_step: int, total_steps: int):
            """diffusers ì½œë°±(ë³„ë„ ìŠ¤ë ˆë“œ)ì—ì„œ WebSocket ì§„í–‰ìƒí™© ì „ì†¡"""
            # ë„ˆë¬´ ì¦ì€ ì¤‘ë³µ ì „ì†¡ ë°©ì§€
            if current_step == last_sent_step["value"]:
                return
            last_sent_step["value"] = current_step

            # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚° (ì´ë¯¸ì§€ + ìŠ¤í… ê¸°ì¤€)
            image_progress = (i) / num_images
            step_progress = (current_step / max(total_steps, 1)) / num_images
            overall_progress = int((image_progress + step_progress) * 100)

            payload = {
                "type": "generation_progress",
                "current_image": i + 1,
                "total_images": num_images,
                "current_step": current_step,
                "total_steps": total_steps,
                "progress": overall_progress,
            }

            try:
                fut = asyncio.run_coroutine_threadsafe(
                    ws_manager.send_to_session(session_id, payload),
                    loop
                )
                # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ì‘ì—…ì„ ê¹¨ì§€ ì•Šë„ë¡ í¡ìˆ˜
                fut.add_done_callback(lambda f: f.exception())
            except Exception:
                pass
        
        # ë™ê¸° pipe í˜¸ì¶œì„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        def run_pipe():
            call_sig = inspect.signature(pipe.__call__)

            # diffusers ìµœì‹ : callback_on_step_end ì§€ì›
            if "callback_on_step_end" in call_sig.parameters:
                def callback_on_step_end(_pipeline, step_index, _timestep, callback_kwargs):
                    # step_indexëŠ” 0-basedì¸ ê²½ìš°ê°€ ëŒ€ë¶€ë¶„
                    _send_generation_progress_from_thread(step_index + 1, steps)
                    return callback_kwargs

                return pipe(
                    prompt=final_prompt,
                    height=height,
                    width=width,
                    num_inference_steps=steps,
                    guidance_scale=guidance_scale,
                    generator=generator,
                    callback_on_step_end=callback_on_step_end,
                ).images[0]

            # diffusers êµ¬ë²„ì „: callback/callback_steps ì§€ì›
            if "callback" in call_sig.parameters:
                def callback(step_index, _timestep, _latents):
                    _send_generation_progress_from_thread(step_index + 1, steps)

                return pipe(
                    prompt=final_prompt,
                    height=height,
                    width=width,
                    num_inference_steps=steps,
                    guidance_scale=guidance_scale,
                    generator=generator,
                    callback=callback,
                    callback_steps=1,
                ).images[0]

            # ì½œë°± ë¯¸ì§€ì›(ì˜ˆì™¸ ì¼€ì´ìŠ¤): ê¸°ì¡´ ë™ì‘
            return pipe(
                prompt=final_prompt,
                height=height,
                width=width,
                num_inference_steps=steps,
                guidance_scale=guidance_scale,
                generator=generator,
            ).images[0]
        
        image = await asyncio.to_thread(run_pipe)
        
        # ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥
        metadata = ImageMetadata.create_metadata(
            prompt=final_prompt,
            seed=current_seed,
            width=width,
            height=height,
            steps=steps,
            guidance_scale=guidance_scale,
            model=current_model or "unknown",
        )
        
        outputs_dir.mkdir(parents=True, exist_ok=True)
        filename = filename_generator.generate(
            pattern=settings.get("filename_pattern", "{date}_{time}_{seed}"),
            prompt=final_prompt,
            seed=current_seed
        )
        output_path = outputs_dir / filename
        ImageMetadata.save_with_metadata(image, output_path, metadata)
        
        images.append({
            "base64": image_to_base64(image),
            "filename": filename,
            "seed": current_seed,
            "path": (
                f"/outputs/{session.data_id}/{filename}"
                if session
                else (f"/outputs/{session_id}/{filename}" if isinstance(session_id, str) and session_id.startswith("user_") else f"/outputs/{filename}")
            )
        })
        
        # ê° ì´ë¯¸ì§€ ìƒì„± í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ (ì—¬ëŸ¬ ì¥ ìƒì„± ì‹œ OOM ë°©ì§€)
        if num_images > 1 and torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # ìƒì„± ì™„ë£Œ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()
    
    # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ì‚¬ìš©ìë³„)
    if session:
        history_mgr = get_history_manager_sync(session.data_id)
    else:
        # ê³„ì • ë‹¨ìœ„ë¡œ ì‹¤í–‰ëœ ê²½ìš°(user_{id})ì—ëŠ” ê·¸ ê³„ì • íˆìŠ¤í† ë¦¬ì— ì €ì¥
        if isinstance(session_id, str) and session_id.startswith("user_"):
            history_mgr = get_history_manager_sync(session_id)
        else:
            from utils.history import history_manager
            history_mgr = history_manager
    
    history_entry = history_mgr.add(
        prompt=prompt,
        korean_prompt=korean_prompt,
        settings={
            "width": width,
            "height": height,
            "steps": steps,
            "guidance_scale": guidance_scale,
            "seed": seed,
        }
    )
    
    # ì™„ë£Œ ë©”ì‹œì§€
    await ws_manager.send_to_session(session_id, {
        "type": "complete",
        "content": f"âœ… {len(images)}ì¥ ìƒì„± ì™„ë£Œ! (ì‹œë“œ: {seed})"
    })
    
    # ì´ë¯¸ì§€ ê²°ê³¼ ì „ì†¡
    await ws_manager.send_to_session(session_id, {
        "type": "generation_result",
        "images": images,
        "seed": seed,
        "prompt": final_prompt,
        "history_id": history_entry.id
    })
    
    return {
        "success": True,
        "images": images,
        "seed": seed,
        "prompt": final_prompt,
        "history_id": history_entry.id
    }


# ============= ì„¸ì…˜ë³„ ì¶œë ¥ í´ë” ì •ì  íŒŒì¼ ì œê³µ =============
@app.get("/outputs/{session_id}/{filename:path}")
async def serve_session_output(session_id: str, filename: str, request: Request):
    """ì„¸ì…˜ë³„ ì¶œë ¥ íŒŒì¼ ì œê³µ"""
    # ì„¸ì…˜ ID ê²€ì¦
    if not session_manager.validate_session_id(session_id):
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    file_path = OUTPUTS_DIR / session_id / filename
    if not file_path.exists():
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(file_path)


# ë ˆê±°ì‹œ ì¶œë ¥ í´ë” (ì„¸ì…˜ ì—†ëŠ” ê¸°ì¡´ ì´ë¯¸ì§€ìš©)
@app.get("/outputs/{filename:path}")
async def serve_legacy_output(filename: str):
    """ë ˆê±°ì‹œ ì¶œë ¥ íŒŒì¼ ì œê³µ"""
    # ì„¸ì…˜ IDì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ í™•ì¸ (UUID í˜•ì‹)
    if "/" in filename or session_manager.validate_session_id(filename.split("/")[0] if "/" in filename else ""):
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    file_path = OUTPUTS_DIR / filename
    if not file_path.exists():
        raise HTTPException(404, "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return FileResponse(file_path)


# ============= API ì—”ë“œí¬ì¸íŠ¸ =============

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€ - ë¡œê·¸ì¸ í•„ìˆ˜"""
    update_activity()
    session = await get_session_from_request(request)
    
    # ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ê²½ìš° ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    if not session or not session.is_authenticated:
        from fastapi.responses import RedirectResponse
        response = RedirectResponse(url="/login", status_code=302)
        # ë¹„ë¡œê·¸ì¸ ì„¸ì…˜ì€ ìƒì„±/ìœ ì§€í•˜ì§€ ì•ŠìŒ (ì¿ í‚¤ë„ ì œê±°)
        clear_session_cookie(response)
        return response
    
    response = templates.TemplateResponse("index.html", {
        "request": request,
        "user": {
            "id": session.user_id,
            "username": session.username,
        }
    })
    set_session_cookie(response, session)
    
    return response


@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    """ë¡œê·¸ì¸ í˜ì´ì§€"""
    session = await get_session_from_request(request)
    
    # ì´ë¯¸ ë¡œê·¸ì¸ëœ ê²½ìš° ë©”ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    if session and session.is_authenticated:
        from fastapi.responses import RedirectResponse
        response = RedirectResponse(url="/", status_code=302)
        set_session_cookie(response, session)
        return response
    
    response = templates.TemplateResponse("login.html", {"request": request})
    # ë¹„ë¡œê·¸ì¸ì—ì„œëŠ” ì„¸ì…˜/ì¿ í‚¤ë¥¼ ë§Œë“¤ì§€ ì•ŠìŒ
    clear_session_cookie(response)
    
    return response


@app.get("/api/status")
async def get_status(request: Request):
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    global pipe, current_model, device
    update_activity()
    
    session = await get_session_from_request(request)
    queue_status = generation_queue.get_queue_status()
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)
    
    status = {
        "model_loaded": pipe is not None,
        "current_model": current_model,
        "device": device or get_device(),
        "vram": get_vram_info(),
        "is_generating": queue_status["is_processing"],
        "upscaler_available": REALESRGAN_AVAILABLE,
        "queue_length": queue_status["queue_length"],
        "connected_users": ws_manager.get_session_count(),
        # í”„ë¡ íŠ¸ í˜¸í™˜ í•„ë“œ: ë¡œê·¸ì¸ ì‹œ ê³„ì • í‚¤(user_{id}), ë¹„ë¡œê·¸ì¸ ì‹œ None
        "session_id": (session.data_id if session else None),
        "is_admin": is_admin,
    }
    
    # ê´€ë¦¬ìì¸ ê²½ìš° GPU ì •ë³´ ì¶”ê°€
    if is_admin:
        status["gpu_info"] = {
            "gpu_count": gpu_monitor.gpu_count,
            "cuda_available": gpu_monitor.cuda_available,
            "available_devices": gpu_monitor.get_available_devices(),
            "gpus": gpu_monitor.get_all_gpu_info(),
        }
    
    return status


@app.post("/api/model/load")
async def load_model(request: Request, model_request: ModelLoadRequest):
    """ëª¨ë¸ ë¡œë“œ"""
    global pipe, current_model, device, model_lock
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    
    async with model_lock:
        # GPU ì„ íƒ (ê´€ë¦¬ìë§Œ íŠ¹ì • GPU ì§€ì • ê°€ëŠ¥)
        target_device = model_request.target_device
        client_host = request.client.host if request.client else None
        is_admin = is_localhost(client_host)

        # UIê°€ target_device="auto"ë¡œ ë³´ë‚´ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ,
        # ê´€ë¦¬ìê°€ ì„¤ì •í•œ ê¸°ë³¸ GPU(ì„¤ì • -> GPU ì„¤ì •/ëª¨ë‹ˆí„°ë§)ë¥¼ ìë™ ì ìš©í•œë‹¤.
        if target_device == "auto":
            target_device = settings.get("generation_gpu", DEFAULT_GPU_SETTINGS["generation_gpu"])

        if not is_admin and target_device != "auto":
            # ê´€ë¦¬ìê°€ ì•„ë‹Œ ê²½ìš° autoë¡œ ê°•ì œ
            target_device = "auto"
        
        device = get_device(target_device)

        # ì–‘ìí™”/CPU ì˜¤í”„ë¡œë”©ì€ ê´€ë¦¬ìë§Œ ë³€ê²½ ê°€ëŠ¥
        requested_quantization = model_request.quantization
        requested_cpu_offload = model_request.cpu_offload
        if not is_admin:
            requested_quantization = settings.get("quantization", requested_quantization)
            requested_cpu_offload = settings.get("cpu_offload", requested_cpu_offload)

        quant_info = QUANTIZATION_OPTIONS.get(requested_quantization)
        
        if not quant_info:
            raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {requested_quantization}")
        
        repo_id = quant_info["repo"]
        dtype = quant_info["type"]
        is_gguf = quant_info.get("is_gguf", False)
        
        try:
            # 1ë‹¨ê³„: ë¡œë”© ì¤€ë¹„
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 5, 
                "label": "ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...",
                "detail": f"ì–‘ìí™”: {dtype}, ë””ë°”ì´ìŠ¤: {device}",
                "stage": "init"
            })
            await asyncio.sleep(0.1)
            
            from diffusers import ZImagePipeline
            
            if is_gguf:
                # GGUF ì–‘ìí™” ëª¨ë¸ ë¡œë“œ
                from diffusers import ZImageTransformer2DModel, GGUFQuantizationConfig
                from huggingface_hub import hf_hub_download
                
                filename = quant_info["filename"]
                
                # 2ë‹¨ê³„: GGUF ë‹¤ìš´ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 10, 
                    "label": "ğŸ“¥ GGUF ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                    "detail": f"íŒŒì¼: {filename} (ìºì‹œ í™•ì¸ ì¤‘...)",
                    "stage": "download"
                })
                await asyncio.sleep(0.1)
                
                gguf_path = await asyncio.to_thread(
                    hf_hub_download,
                    repo_id=repo_id, 
                    filename=filename,
                    cache_dir=str(MODELS_DIR)
                )
                
                # 3ë‹¨ê³„: GGUF Transformer ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 30, 
                    "label": "ğŸ”„ GGUF Transformer ë¡œë”© ì¤‘...",
                    "detail": f"ì–‘ìí™” íƒ€ì…: {dtype} (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)",
                    "stage": "load_transformer"
                })
                await asyncio.sleep(0.1)
                
                transformer = await asyncio.to_thread(
                    ZImageTransformer2DModel.from_single_file,
                    gguf_path,
                    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),
                    torch_dtype=torch.bfloat16,
                )
                
                # 4ë‹¨ê³„: íŒŒì´í”„ë¼ì¸ êµ¬ì„±
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 55, 
                    "label": "ğŸ”— íŒŒì´í”„ë¼ì¸ êµ¬ì„± ì¤‘...",
                    "detail": "ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ/ë¡œë“œ ë° GGUF Transformer ê²°í•©",
                    "stage": "load_pipeline"
                })
                await asyncio.sleep(0.1)
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    "Tongyi-MAI/Z-Image-Turbo",
                    transformer=transformer,
                    torch_dtype=torch.bfloat16,
                )
            else:
                # ê¸°ë³¸ BF16 ëª¨ë¸ ë¡œë“œ
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 15, 
                    "label": "ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í™•ì¸ ì¤‘...",
                    "detail": f"ì €ì¥ì†Œ: {repo_id} (ìºì‹œì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤)",
                    "stage": "download"
                })
                await asyncio.sleep(0.1)
                
                load_kwargs = {
                    "torch_dtype": torch.bfloat16,
                    "cache_dir": str(MODELS_DIR),
                }
                
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 30, 
                    "label": "ğŸ”„ ëª¨ë¸ íŒŒì¼ ë¡œë”© ì¤‘...",
                    "detail": "ë‹¤ìš´ë¡œë“œ ë˜ëŠ” ìºì‹œì—ì„œ ë¡œë“œ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª‡ ë¶„ ì†Œìš”)",
                    "stage": "load_model"
                })
                await asyncio.sleep(0.1)
                
                pipe = await asyncio.to_thread(
                    ZImagePipeline.from_pretrained,
                    repo_id,
                    **load_kwargs
                )
            
            # 5ë‹¨ê³„: ë””ë°”ì´ìŠ¤ ì „ì†¡
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 75, 
                "label": f"ğŸš€ {device.upper()}ë¡œ ëª¨ë¸ ì „ì†¡ ì¤‘...",
                "detail": "VRAMìœ¼ë¡œ ëª¨ë¸ ë³µì‚¬ ì¤‘...",
                "stage": "to_device"
            })
            await asyncio.sleep(0.1)
            
            if requested_cpu_offload:
                await asyncio.to_thread(pipe.enable_model_cpu_offload)
                await ws_manager.broadcast({
                    "type": "model_progress", 
                    "progress": 95, 
                    "label": "âš™ï¸ CPU ì˜¤í”„ë¡œë”© ì„¤ì • ì¤‘...",
                    "detail": "VRAM ë¶€ì¡± ì‹œ ìë™ìœ¼ë¡œ RAM ì‚¬ìš©",
                    "stage": "cpu_offload"
                })
            else:
                await asyncio.to_thread(pipe.to, device)
            
            current_model = requested_quantization
            
            # GPU ëª¨ë‹ˆí„°ì— ëª¨ë¸ ë“±ë¡
            gpu_monitor.register_model("Z-Image-Turbo", device)
            
            # 6ë‹¨ê³„: ì™„ë£Œ
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 100, 
                "label": "âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}",
                "stage": "complete"
            })
            
            await ws_manager.broadcast({
                "type": "model_status_change",
                "model_loaded": True,
                "current_model": current_model,
                "device": device
            })
            
            await ws_manager.broadcast({
                "type": "complete",
                "content": f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({dtype}, {device})"
            })
            
            return {"success": True, "message": f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {repo_id} ({dtype})", "device": device}
            
        except Exception as e:
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 0, 
                "label": "âŒ ë¡œë“œ ì‹¤íŒ¨",
                "detail": str(e),
                "stage": "error"
            })
            await ws_manager.broadcast({"type": "error", "content": f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"})
            raise HTTPException(500, str(e))


@app.post("/api/model/unload")
async def unload_model(request: Request):
    """ëª¨ë¸ ì–¸ë¡œë“œ"""
    global pipe, current_model, model_lock
    
    # ëª¨ë¸ ì ê¸ˆ í™•ì¸
    if model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    async with model_lock:
        if pipe is None:
            return {"success": True, "message": "ë¡œë“œëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 30, 
                "label": "ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ ì¤‘...",
                "detail": ""
            })
            
            # GPU ëª¨ë‹ˆí„°ì—ì„œ ëª¨ë¸ ë“±ë¡ í•´ì œ
            gpu_monitor.unregister_model("Z-Image-Turbo")
            
            del pipe
            pipe = None
            current_model = None
            
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 60, 
                "label": "VRAM ì •ë¦¬ ì¤‘...",
                "detail": ""
            })
            
            # GPU ìºì‹œ ì •ë¦¬ (ìƒì„± ëª¨ë¸ ë””ë°”ì´ìŠ¤ë§Œ ì •ë¦¬)
            gpu_monitor.clear_cache(device)
            gc.collect()
            
            await ws_manager.broadcast({
                "type": "model_progress", 
                "progress": 100, 
                "label": "ì–¸ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM ì‚¬ìš©ëŸ‰: {get_vram_info()}"
            })
            
            await ws_manager.broadcast({
                "type": "model_status_change",
                "model_loaded": False,
                "current_model": None
            })
            
            await ws_manager.broadcast({"type": "complete", "content": "âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!"})
            return {"success": True, "message": "ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ"}
            
        except Exception as e:
            raise HTTPException(500, str(e))


@app.post("/api/generate")
async def generate_image(request: Request, gen_request: GenerateRequest):
    """ì´ë¯¸ì§€ ìƒì„± ìš”ì²­ (íì— ì¶”ê°€)"""
    update_activity()
    
    session = await get_session_from_request(request)
    require_auth(session)
    
    if pipe is None:
        raise HTTPException(400, "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not gen_request.prompt.strip():
        raise HTTPException(400, "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # Rate limit ì²´í¬
    # ê³„ì • ë‹¨ìœ„ë¡œ ì œí•œ(ì„¸ì…˜ ë‹¨ìœ„ êµ¬ë¶„ ì œê±°)
    exceeded, count = session_manager.check_rate_limit(session.session_id)
    if exceeded:
        await ws_manager.send_to_session(session.data_id, {
            "type": "warning",
            "content": f"âš ï¸ ìš”ì²­ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. (ë¶„ë‹¹ {count}íšŒ) ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        })
    
    # ìš”ì²­ ë°ì´í„° ì¤€ë¹„
    request_data = {
        # ì‹¤í–‰/ì•Œë¦¼/í ëª¨ë‘ ê³„ì •(data_id) ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        "session_id": session.data_id,
        "prompt": gen_request.prompt,
        "korean_prompt": gen_request.korean_prompt,
        "width": gen_request.width,
        "height": gen_request.height,
        "steps": gen_request.steps,
        "guidance_scale": gen_request.guidance_scale,
        "seed": gen_request.seed,
        "num_images": gen_request.num_images,
        "auto_translate": gen_request.auto_translate,
    }
    
    # íì— ì¶”ê°€
    item_id, position = await generation_queue.add_to_queue(session.data_id, request_data)
    
    # í ìƒíƒœ ì•Œë¦¼
    if position > 1:
        await ws_manager.send_to_session(session.data_id, {
            "type": "queue_status",
            "status": "queued",
            "position": position,
            "message": f"â³ ëŒ€ê¸°ì—´ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆœì„œ: {position})"
        })
    else:
        await ws_manager.send_to_session(session.data_id, {
            "type": "queue_status",
            "status": "processing",
            "position": 0,
            "message": "ğŸ¨ ì´ë¯¸ì§€ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
        })
    
    response = JSONResponse(content={
        "success": True,
        "queued": True,
        "item_id": item_id,
        "position": position,
        "message": f"ìš”ì²­ì´ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. (ìˆœì„œ: {position})"
    })
    set_session_cookie(response, session)
    
    return response


@app.post("/api/preview")
async def generate_preview(request: Request, gen_request: GenerateRequest):
    """ë¹ ë¥¸ ë¯¸ë¦¬ë³´ê¸° (256x256)"""
    gen_request.width = 256
    gen_request.height = 256
    gen_request.steps = min(gen_request.steps, 4)
    gen_request.num_images = 1
    return await generate_image(request, gen_request)


@app.post("/api/translate")
async def translate_text(request: Request, trans_request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ë²ˆì—­ (í•œêµ­ì–´ â†’ ì˜ì–´)"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = translator.translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/translate-reverse")
async def reverse_translate_text(request: Request, trans_request: TranslateRequest):
    """í”„ë¡¬í”„íŠ¸ ì—­ë²ˆì—­ (ì˜ì–´ â†’ í•œêµ­ì–´)"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = translator.reverse_translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/enhance")
async def enhance_prompt(request: Request, enhance_request: EnhanceRequest):
    """í”„ë¡¬í”„íŠ¸ í–¥ìƒ"""
    update_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    enhanced, success = prompt_enhancer.enhance(enhance_request.prompt, enhance_request.style)
    return {"success": success, "enhanced": enhanced}


@app.get("/api/templates")
async def get_templates():
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ëª©ë¡"""
    return {"templates": PROMPT_TEMPLATES}


@app.get("/api/model-status")
async def get_model_download_status():
    """ê° ëª¨ë¸ì˜ ë‹¤ìš´ë¡œë“œ ìƒíƒœ í™•ì¸"""
    from huggingface_hub import try_to_load_from_cache
    
    status = {}
    
    for option_name, option_info in QUANTIZATION_OPTIONS.items():
        is_downloaded = False
        
        try:
            if option_info.get("is_gguf", False):
                filename = option_info.get("filename", "")
                repo_id = option_info.get("repo", "")
                
                if filename and repo_id:
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename=filename
                    )
                    is_downloaded = cached_path is not None
            else:
                repo_id = option_info.get("repo", "")
                if repo_id:
                    cached_path = try_to_load_from_cache(
                        repo_id=repo_id,
                        filename="model_index.json"
                    )
                    is_downloaded = cached_path is not None
        except Exception as e:
            print(f"ìºì‹œ í™•ì¸ ì˜¤ë¥˜ ({option_name}): {e}")
            is_downloaded = False
        
        status[option_name] = is_downloaded
    
    return {"status": status}


# ============= ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ API =============
@app.get("/api/history")
async def get_history(request: Request):
    """íˆìŠ¤í† ë¦¬ ëª©ë¡ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    history_mgr = get_history_manager_sync(session.data_id)
    entries = history_mgr.get_all()
    
    response = JSONResponse(content={"history": [e.to_dict() for e in entries[:50]]})
    set_session_cookie(response, session)
    return response


@app.get("/api/history/{history_id}")
async def get_history_detail(history_id: str, request: Request):
    """íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    history_mgr = get_history_manager_sync(session.data_id)
    entry = history_mgr.get_by_id(history_id)
    
    if not entry:
        raise HTTPException(404, "íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"history": entry.to_dict()}


@app.patch("/api/history/{history_id}/conversation")
async def update_history_conversation(history_id: str, request: Request, conv_request: ConversationUpdateRequest):
    """íˆìŠ¤í† ë¦¬ì˜ ëŒ€í™” ë‚´ìš© ì—…ë°ì´íŠ¸ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    history_mgr = get_history_manager_sync(session.data_id)
    entry = history_mgr.get_by_id(history_id)
    
    if not entry:
        raise HTTPException(404, "íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    entry.conversation = conv_request.conversation
    history_mgr._save()
    
    return {"success": True}


@app.delete("/api/history")
async def clear_history(request: Request):
    """íˆìŠ¤í† ë¦¬ ì‚­ì œ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    history_mgr = get_history_manager_sync(session.data_id)
    history_mgr.clear()
    return {"success": True}


# ============= ì‚¬ìš©ìë³„ ì¦ê²¨ì°¾ê¸° API =============
@app.get("/api/favorites")
async def get_favorites(request: Request):
    """ì¦ê²¨ì°¾ê¸° ëª©ë¡ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    fav_mgr = get_favorites_manager_sync(session.data_id)
    entries = fav_mgr.get_all()
    
    response = JSONResponse(content={"favorites": [e.to_dict() for e in entries]})
    set_session_cookie(response, session)
    return response


@app.post("/api/favorites")
async def add_favorite(request: Request, fav_request: FavoriteRequest):
    """ì¦ê²¨ì°¾ê¸° ì¶”ê°€ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    fav_mgr = get_favorites_manager_sync(session.data_id)
    entry = fav_mgr.add(
        name=fav_request.name,
        prompt=fav_request.prompt,
        settings=fav_request.settings
    )
    return {"success": True, "id": entry.id}


@app.delete("/api/favorites/{fav_id}")
async def delete_favorite(fav_id: str, request: Request):
    """ì¦ê²¨ì°¾ê¸° ì‚­ì œ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    fav_mgr = get_favorites_manager_sync(session.data_id)
    success = fav_mgr.delete(fav_id)
    return {"success": success}


# ============= ì„¸ì…˜ë³„ ê°¤ëŸ¬ë¦¬ API =============
@app.get("/api/gallery")
async def get_gallery(request: Request):
    """ê°¤ëŸ¬ë¦¬ ì´ë¯¸ì§€ ëª©ë¡ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    outputs_dir = session.get_outputs_dir()
    
    images = []
    if outputs_dir.exists():
        for f in sorted(outputs_dir.glob("*.png"), key=lambda x: x.stat().st_mtime, reverse=True)[:50]:
            metadata = ImageMetadata.read_metadata(f)
            images.append({
                "filename": f.name,
                "path": f"/outputs/{session.data_id}/{f.name}",
                "metadata": metadata
            })
    
    response = JSONResponse(content={"images": images})
    set_session_cookie(response, session)
    return response


# ============= ì„¤ì • API (localhost ì „ìš© ì“°ê¸°) =============
@app.post("/api/settings")
async def save_settings(request: Request, settings_request: SettingsRequest):
    """ì„¤ì • ì €ì¥ (localhostë§Œ í—ˆìš©)"""
    # localhost ì²´í¬
    client_host = request.client.host if request.client else None
    if not is_localhost(client_host):
        raise HTTPException(403, "ì„¤ì • ë³€ê²½ì€ localhostì—ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    from utils.llm_client import llm_client
    
    # ë ˆê±°ì‹œ í˜¸í™˜
    if settings_request.openai_api_key:
        settings.set("openai_api_key", settings_request.openai_api_key)
        translator.set_api_key(settings_request.openai_api_key)
        prompt_enhancer.set_api_key(settings_request.openai_api_key)
    
    # LLM Provider ì„¤ì •
    if settings_request.llm_provider:
        settings.set("llm_provider", settings_request.llm_provider)
    
    if settings_request.llm_api_key:
        settings.set("llm_api_key", settings_request.llm_api_key)
        settings.set("openai_api_key", settings_request.llm_api_key)
    
    if settings_request.llm_base_url is not None:
        settings.set("llm_base_url", settings_request.llm_base_url)
    
    if settings_request.llm_model is not None:
        settings.set("llm_model", settings_request.llm_model)
    
    llm_client.invalidate()
    
    if settings_request.output_path:
        settings.set("output_path", settings_request.output_path)
    
    if settings_request.filename_pattern:
        settings.set("filename_pattern", settings_request.filename_pattern)
    
    if settings_request.translate_system_prompt is not None:
        settings.set("translate_system_prompt", settings_request.translate_system_prompt)
    
    if settings_request.enhance_system_prompt is not None:
        settings.set("enhance_system_prompt", settings_request.enhance_system_prompt)
    
    if settings_request.auto_unload_enabled is not None:
        settings.set("auto_unload_enabled", settings_request.auto_unload_enabled)
    
    if settings_request.auto_unload_timeout is not None:
        timeout = max(1, min(1440, settings_request.auto_unload_timeout))
        settings.set("auto_unload_timeout", timeout)
    
    if settings_request.edit_auto_unload_enabled is not None:
        settings.set("edit_auto_unload_enabled", settings_request.edit_auto_unload_enabled)
    
    if settings_request.edit_auto_unload_timeout is not None:
        timeout = max(1, min(1440, settings_request.edit_auto_unload_timeout))
        settings.set("edit_auto_unload_timeout", timeout)

    # ëª¨ë¸ ì„¤ì • (ê´€ë¦¬ì ì „ìš©)
    if settings_request.quantization is not None:
        if settings_request.quantization not in QUANTIZATION_OPTIONS:
            raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {settings_request.quantization}")
        settings.set("quantization", settings_request.quantization)

    if settings_request.cpu_offload is not None:
        settings.set("cpu_offload", bool(settings_request.cpu_offload))

    # í¸ì§‘ ëª¨ë¸ ì„¤ì • (ê´€ë¦¬ì ì „ìš©)
    if settings_request.edit_quantization is not None:
        if settings_request.edit_quantization not in EDIT_QUANTIZATION_OPTIONS:
            raise HTTPException(400, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í¸ì§‘ ì–‘ìí™”: {settings_request.edit_quantization}")
        settings.set("edit_quantization", settings_request.edit_quantization)

    if settings_request.edit_cpu_offload is not None:
        settings.set("edit_cpu_offload", bool(settings_request.edit_cpu_offload))
    
    return {"success": True}


@app.get("/api/settings")
async def get_settings(request: Request):
    """ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    from utils.settings import LLM_PROVIDERS
    from utils.translator import Translator
    from utils.prompt_enhancer import PromptEnhancer
    from utils.edit_llm import EditTranslator, EditEnhancer, EditSuggester
    
    session = await get_session_from_request(request)
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)
    
    # ì„¸ì…˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê°œì¸í™”)
    session_translate_prompt = session.get_setting("translate_system_prompt")
    session_enhance_prompt = session.get_setting("enhance_system_prompt")
    
    # ì„¸ì…˜ì— ì„¤ì •ì´ ì—†ìœ¼ë©´ ì „ì—­ ì„¤ì • ì‚¬ìš©, ì „ì—­ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
    translate_prompt = session_translate_prompt or settings.get("translate_system_prompt") or Translator.DEFAULT_SYSTEM_PROMPT
    enhance_prompt = session_enhance_prompt or settings.get("enhance_system_prompt") or PromptEnhancer.DEFAULT_SYSTEM_PROMPT
    
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„¸ì…˜ë³„ ê°œì¸í™”)
    session_edit_translate = session.get_setting("edit_translate_system_prompt")
    session_edit_enhance = session.get_setting("edit_enhance_system_prompt")
    session_edit_suggest = session.get_setting("edit_suggest_system_prompt")
    
    edit_translate_prompt = session_edit_translate or settings.get("edit_translate_system_prompt") or EditTranslator.DEFAULT_SYSTEM_PROMPT
    edit_enhance_prompt = session_edit_enhance or settings.get("edit_enhance_system_prompt") or EditEnhancer.DEFAULT_SYSTEM_PROMPT
    edit_suggest_prompt = session_edit_suggest or settings.get("edit_suggest_system_prompt") or EditSuggester.DEFAULT_SYSTEM_PROMPT
    
    return {
        # ê´€ë¦¬ì ì—¬ë¶€
        "is_admin": is_admin,
        # ë ˆê±°ì‹œ í˜¸í™˜
        "openai_api_key": "***" if settings.get("openai_api_key") else "",
        # LLM Provider ì„¤ì •
        "llm_provider": settings.get("llm_provider", "env"),
        "llm_api_key": "***" if settings.get("llm_api_key") else "",
        "llm_base_url": settings.get("llm_base_url", ""),
        "llm_model": settings.get("llm_model", ""),
        "llm_providers": {
            pid: {
                "name": pinfo["name"],
                "base_url": pinfo["base_url"],
                "default_model": pinfo["default_model"],
                "models": pinfo["models"],
            }
            for pid, pinfo in LLM_PROVIDERS.items()
        },
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„¸ì…˜ë³„ ê°œì¸í™”)
        "translate_system_prompt": translate_prompt,
        "enhance_system_prompt": enhance_prompt,
        "default_translate_system_prompt": Translator.DEFAULT_SYSTEM_PROMPT,
        "default_enhance_system_prompt": PromptEnhancer.DEFAULT_SYSTEM_PROMPT,
        # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„¸ì…˜ë³„ ê°œì¸í™”)
        "edit_translate_system_prompt": edit_translate_prompt,
        "edit_enhance_system_prompt": edit_enhance_prompt,
        "edit_suggest_system_prompt": edit_suggest_prompt,
        "default_edit_translate_system_prompt": EditTranslator.DEFAULT_SYSTEM_PROMPT,
        "default_edit_enhance_system_prompt": EditEnhancer.DEFAULT_SYSTEM_PROMPT,
        "default_edit_suggest_system_prompt": EditSuggester.DEFAULT_SYSTEM_PROMPT,
        # ê¸°íƒ€ ì„¤ì •
        "output_path": str(settings.get("output_path", OUTPUTS_DIR)),
        "filename_pattern": settings.get("filename_pattern", "{date}_{time}_{seed}"),
        # ëª¨ë¸ ì„¤ì • (ê´€ë¦¬ìë§Œ ë³€ê²½ ê°€ëŠ¥ - ëª¨ë“  ì‚¬ìš©ìì—ê²ŒëŠ” í˜„ì¬ ê°’ë§Œ ì œê³µ)
        "quantization": settings.get("quantization", "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"),
        "cpu_offload": settings.get("cpu_offload", False),
        "edit_quantization": settings.get("edit_quantization", "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"),
        "edit_cpu_offload": settings.get("edit_cpu_offload", True),
        "quantization_options": list(QUANTIZATION_OPTIONS.keys()),
        "resolution_presets": RESOLUTION_PRESETS,
        # ìë™ ì–¸ë¡œë“œ ì„¤ì •
        "auto_unload_enabled": settings.get("auto_unload_enabled", True),
        "auto_unload_timeout": settings.get("auto_unload_timeout", 10),
        # í¸ì§‘ ëª¨ë¸ ìë™ ì–¸ë¡œë“œ ì„¤ì •
        "edit_auto_unload_enabled": settings.get("edit_auto_unload_enabled", True),
        "edit_auto_unload_timeout": settings.get("edit_auto_unload_timeout", LONGCAT_EDIT_AUTO_UNLOAD_TIMEOUT),
    }


# ============= ì„¸ì…˜ë³„ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ API =============
class SystemPromptsRequest(BaseModel):
    translate_system_prompt: Optional[str] = None
    enhance_system_prompt: Optional[str] = None
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ê°œì¸í™”)
    edit_translate_system_prompt: Optional[str] = None
    edit_enhance_system_prompt: Optional[str] = None
    edit_suggest_system_prompt: Optional[str] = None


@app.post("/api/settings/prompts")
async def save_session_prompts(request: Request, prompts_request: SystemPromptsRequest):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì €ì¥ (ì„¸ì…˜ë³„ ê°œì¸í™”, ëª¨ë“  ì‚¬ìš©ì ì ‘ê·¼ ê°€ëŠ¥)"""
    session = await get_session_from_request(request)
    session_settings = session.get_settings()
    
    # ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if prompts_request.translate_system_prompt is not None:
        if prompts_request.translate_system_prompt == '':
            # ë¹ˆ ë¬¸ìì—´ì´ë©´ ì„¤ì • ì‚­ì œ (ê¸°ë³¸ê°’ ì‚¬ìš©)
            session_settings.pop("translate_system_prompt", None)
        else:
            session_settings["translate_system_prompt"] = prompts_request.translate_system_prompt
    
    if prompts_request.enhance_system_prompt is not None:
        if prompts_request.enhance_system_prompt == '':
            # ë¹ˆ ë¬¸ìì—´ì´ë©´ ì„¤ì • ì‚­ì œ (ê¸°ë³¸ê°’ ì‚¬ìš©)
            session_settings.pop("enhance_system_prompt", None)
        else:
            session_settings["enhance_system_prompt"] = prompts_request.enhance_system_prompt
    
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if prompts_request.edit_translate_system_prompt is not None:
        if prompts_request.edit_translate_system_prompt == '':
            session_settings.pop("edit_translate_system_prompt", None)
        else:
            session_settings["edit_translate_system_prompt"] = prompts_request.edit_translate_system_prompt
    
    if prompts_request.edit_enhance_system_prompt is not None:
        if prompts_request.edit_enhance_system_prompt == '':
            session_settings.pop("edit_enhance_system_prompt", None)
        else:
            session_settings["edit_enhance_system_prompt"] = prompts_request.edit_enhance_system_prompt
    
    if prompts_request.edit_suggest_system_prompt is not None:
        if prompts_request.edit_suggest_system_prompt == '':
            session_settings.pop("edit_suggest_system_prompt", None)
        else:
            session_settings["edit_suggest_system_prompt"] = prompts_request.edit_suggest_system_prompt
    
    session.save_settings(session_settings)
    
    response = JSONResponse(content={"success": True})
    set_session_cookie(response, session)
    return response


@app.delete("/api/settings/prompts")
async def reset_session_prompts(request: Request):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™” (ì„¸ì…˜ë³„ ì„¤ì • ì‚­ì œ, ì „ì—­/ê¸°ë³¸ê°’ ì‚¬ìš©)"""
    session = await get_session_from_request(request)
    
    session_settings = session.get_settings()
    # ìƒì„± ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if "translate_system_prompt" in session_settings:
        del session_settings["translate_system_prompt"]
    if "enhance_system_prompt" in session_settings:
        del session_settings["enhance_system_prompt"]
    # í¸ì§‘ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    if "edit_translate_system_prompt" in session_settings:
        del session_settings["edit_translate_system_prompt"]
    if "edit_enhance_system_prompt" in session_settings:
        del session_settings["edit_enhance_system_prompt"]
    if "edit_suggest_system_prompt" in session_settings:
        del session_settings["edit_suggest_system_prompt"]
    session.save_settings(session_settings)
    
    response = JSONResponse(content={"success": True})
    set_session_cookie(response, session)
    return response


# ============= ì¸ì¦ API =============
@app.post("/api/auth/register")
async def register(request: Request, data: RegisterRequest):
    """íšŒì›ê°€ì…"""
    # íšŒì›ê°€ì…ì€ ì„¸ì…˜(ë¡œê·¸ì¸ ì¿ í‚¤) ë°œê¸‰ì´ í•„ìš”í•˜ë¯€ë¡œ ìƒì„± í—ˆìš©
    session = await get_session_from_request(request, create_if_missing=True)
    
    # ë¹„ë°€ë²ˆí˜¸ í™•ì¸
    if data.password != data.password_confirm:
        raise HTTPException(400, "ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # íšŒì›ê°€ì…
    success, message, user = auth_manager.create_user(data.username, data.password)
    
    if not success:
        raise HTTPException(400, message)
    
    response = JSONResponse(content={
        "success": True,
        "message": message,
        "user": user.to_dict() if user else None
    })
    set_session_cookie(response, session)
    return response


@app.post("/api/auth/login")
async def login(request: Request, data: LoginRequest):
    """ë¡œê·¸ì¸"""
    # ë¡œê·¸ì¸ì€ ì„¸ì…˜(ë¡œê·¸ì¸ ì¿ í‚¤) ë°œê¸‰ì´ í•„ìš”í•˜ë¯€ë¡œ ìƒì„± í—ˆìš©
    session = await get_session_from_request(request, create_if_missing=True)
    
    # ì¸ì¦
    success, message, user = auth_manager.authenticate(data.username, data.password)
    
    if not success or not user:
        raise HTTPException(401, message)
    
    # ì„¸ì…˜ì— ë¡œê·¸ì¸ ì •ë³´ ì—°ê²°
    await session_manager.login_session(session.session_id, user.id, user.username)
    
    response = JSONResponse(content={
        "success": True,
        "message": message,
        "user": user.to_dict()
    })
    set_session_cookie(response, session)
    return response


@app.post("/api/auth/logout")
async def logout(request: Request):
    """ë¡œê·¸ì•„ì›ƒ"""
    session = await get_session_from_request(request)
    
    if session and session.is_authenticated:
        await session_manager.logout_session(session.session_id)
    
    response = JSONResponse(content={
        "success": True,
        "message": "ë¡œê·¸ì•„ì›ƒë˜ì—ˆìŠµë‹ˆë‹¤."
    })
    # ë¡œê·¸ì•„ì›ƒì€ ì¿ í‚¤ ì œê±°
    clear_session_cookie(response)
    return response


@app.get("/api/auth/me")
async def get_current_user(request: Request):
    """í˜„ì¬ ë¡œê·¸ì¸ëœ ì‚¬ìš©ì ì •ë³´"""
    session = await get_session_from_request(request)
    
    # ê´€ë¦¬ì ì—¬ë¶€ í™•ì¸
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)
    
    if not session or not session.is_authenticated:
        response = JSONResponse(content={
            "authenticated": False,
            "user": None,
            "is_admin": is_admin
        })
        clear_session_cookie(response)
    else:
        user = auth_manager.get_user_by_id(session.user_id)
        response = JSONResponse(content={
            "authenticated": True,
            "user": user.to_dict() if user else {
                "id": session.user_id,
                "username": session.username
            },
            "is_admin": is_admin
        })
        set_session_cookie(response, session)
    return response


@app.post("/api/auth/change-password")
async def change_password(request: Request, data: ChangePasswordRequest):
    """ë¹„ë°€ë²ˆí˜¸ ë³€ê²½ (ë³¸ì¸)"""
    session = await get_session_from_request(request)
    require_auth(session)
    
    # ë¹„ë°€ë²ˆí˜¸ í™•ì¸
    if data.new_password != data.new_password_confirm:
        raise HTTPException(400, "ìƒˆ ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # ë¹„ë°€ë²ˆí˜¸ ë³€ê²½
    success, message = auth_manager.change_password(
        session.user_id,
        data.current_password,
        data.new_password
    )
    
    if not success:
        raise HTTPException(400, message)
    
    response = JSONResponse(content={
        "success": True,
        "message": message
    })
    set_session_cookie(response, session)
    return response


# ============= ê´€ë¦¬ì API (localhost ì „ìš©) =============
@app.get("/api/admin/users")
async def get_all_users(request: Request):
    """ëª¨ë“  ì‚¬ìš©ì ëª©ë¡ (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)
    
    users = auth_manager.get_all_users()
    return {"users": users}


@app.post("/api/admin/users/{user_id}/reset-password")
async def admin_reset_password(request: Request, user_id: int, data: ResetPasswordRequest):
    """ì‚¬ìš©ì ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™” (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)
    
    success, message, new_password = auth_manager.reset_password(user_id, data.new_password)
    
    if not success:
        raise HTTPException(400, message)
    
    return {
        "success": True,
        "message": message,
        "new_password": new_password  # ê´€ë¦¬ìì—ê²Œ ì„ì‹œ ë¹„ë°€ë²ˆí˜¸ í‘œì‹œ
    }


@app.delete("/api/admin/users/{user_id}")
async def admin_delete_user(request: Request, user_id: int):
    """ì‚¬ìš©ì ì‚­ì œ (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)
    
    # ì‚¬ìš©ì ì‚­ì œ
    success, message = auth_manager.delete_user(user_id)
    
    if not success:
        raise HTTPException(400, message)
    
    # ì‚¬ìš©ì ë°ì´í„°ë„ ì‚­ì œ
    await session_manager.delete_user_data(user_id)
    
    return {
        "success": True,
        "message": message
    }


@app.get("/api/admin/sessions")
async def get_all_sessions(request: Request):
    """ì ‘ì† ì‚¬ìš©ì(ê³„ì •) ëª©ë¡ (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)

    users = []
    # WebSocket ì—°ê²° í‚¤ëŠ” í˜„ì¬ ê³„ì •(data_id)ë¡œ í†µì¼ë˜ì–´ ìˆìŒ
    for data_id in sorted(ws_manager.get_connected_keys()):
        user_id = _parse_user_id_from_data_id(data_id)
        user = auth_manager.get_user_by_id(user_id) if user_id is not None else None
        username = user.username if user else None

        # last_activityëŠ” (ìˆë‹¤ë©´) í•´ë‹¹ ìœ ì €ì˜ í™œì„± ì„¸ì…˜ì—ì„œ ê°€ì ¸ì˜´
        session = session_manager.get_session_by_user(user_id) if user_id is not None else None
        last_activity = session.last_activity if session else None

        users.append({
            "data_id": data_id,           # user_{id}
            "user_id": user_id,
            "username": username,
            "last_activity": last_activity,
            "data_size": _get_data_size_by_data_id(data_id),
            "connected": True,
        })

    return {"users": users}


@app.delete("/api/admin/sessions/{session_id}")
async def delete_session(session_id: str, request: Request):
    """ì‚¬ìš©ì(ê³„ì •) ì ‘ì† ì¢…ë£Œ/ì •ë¦¬ (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)

    # í”„ë¡ íŠ¸ì—ì„œ ë„˜ì–´ì˜¤ëŠ” ê°’ì€ ì´ì œ data_id(user_{id})ë¡œ ì‚¬ìš©
    data_id = session_id
    user_id = _parse_user_id_from_data_id(data_id)

    # WebSocket ê°•ì œ ì¢…ë£Œ + ëŒ€ê¸°ì—´ ì œê±°
    closed = await ws_manager.disconnect_key(data_id)
    await generation_queue.remove_session_items(data_id)

    # ì„¸ì…˜ ë§¤í•‘ ì •ë¦¬(ê°€ëŠ¥í•œ ê²½ìš°)
    if user_id is not None:
        existing = session_manager.get_session_by_user(user_id)
        if existing:
            await session_manager.delete_session(existing.session_id)

    return {"success": True, "closed_connections": closed}


# ============= GPU ê´€ë¦¬ API (ê´€ë¦¬ì ì „ìš©) =============
class GPUSettingsRequest(BaseModel):
    generation_gpu: Optional[str] = None  # "auto", "cuda:0", "cuda:1", "cpu"
    edit_gpu: Optional[str] = None        # "auto", "cuda:0", "cuda:1", "cpu"


@app.get("/api/admin/gpu-status")
async def get_gpu_status(request: Request):
    """GPU ìƒíƒœ ì¡°íšŒ (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)
    
    gpu_info = gpu_monitor.get_system_info()
    
    # í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì¶”ê°€
    gpu_info["models"] = {
        "generation": {
            "loaded": pipe is not None,
            "name": current_model,
            "device": device,
        },
        "edit": {
            "loaded": longcat_edit_manager.is_loaded,
            "name": longcat_edit_manager.current_model,
            "device": longcat_edit_manager.device,
            "quantization": longcat_edit_manager.current_quantization,
            "cpu_offload": longcat_edit_manager.cpu_offload_enabled,
        }
    }
    
    # í˜„ì¬ GPU ì„¤ì • ì¶”ê°€
    gpu_info["current_settings"] = {
        "generation_gpu": settings.get("generation_gpu", DEFAULT_GPU_SETTINGS["generation_gpu"]),
        "edit_gpu": settings.get("edit_gpu", DEFAULT_GPU_SETTINGS["edit_gpu"]),
    }
    
    return gpu_info


@app.post("/api/admin/gpu-settings")
async def update_gpu_settings(request: Request, gpu_settings: GPUSettingsRequest):
    """GPU ì„¤ì • ì—…ë°ì´íŠ¸ (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)
    
    # ìœ íš¨í•œ ë””ë°”ì´ìŠ¤ì¸ì§€ í™•ì¸
    available_devices = gpu_monitor.get_available_devices()
    
    if gpu_settings.generation_gpu is not None:
        if gpu_settings.generation_gpu not in available_devices:
            raise HTTPException(400, f"ìœ íš¨í•˜ì§€ ì•Šì€ ë””ë°”ì´ìŠ¤: {gpu_settings.generation_gpu}")
        settings.set("generation_gpu", gpu_settings.generation_gpu)
    
    if gpu_settings.edit_gpu is not None:
        if gpu_settings.edit_gpu not in available_devices:
            raise HTTPException(400, f"ìœ íš¨í•˜ì§€ ì•Šì€ ë””ë°”ì´ìŠ¤: {gpu_settings.edit_gpu}")
        settings.set("edit_gpu", gpu_settings.edit_gpu)
    
    return {
        "success": True,
        "message": "GPU ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.",
        "settings": {
            "generation_gpu": settings.get("generation_gpu", DEFAULT_GPU_SETTINGS["generation_gpu"]),
            "edit_gpu": settings.get("edit_gpu", DEFAULT_GPU_SETTINGS["edit_gpu"]),
        }
    }


@app.get("/api/admin/available-devices")
async def get_available_devices(request: Request):
    """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ëª©ë¡ (ê´€ë¦¬ì ì „ìš©)"""
    require_admin(request)
    
    return {
        "devices": gpu_monitor.get_available_devices(),
        "gpu_count": gpu_monitor.gpu_count,
        "cuda_available": gpu_monitor.cuda_available,
    }


# ============= WebSocket =============
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket, z_image_session: Optional[str] = Cookie(default=None)):
    """ì›¹ì†Œì¼“ ì—°ê²° (ì„¸ì…˜ë³„)"""
    # ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°
    # ë¹„ë¡œê·¸ì¸ì—ì„œ ì„¸ì…˜ì„ ë§Œë“¤ì§€ ì•Šê¸° ìœ„í•´ "ê¸°ì¡´ ì„¸ì…˜ ì¡°íšŒ"ë§Œ ì‹œë„
    session = session_manager.get_session(z_image_session) if z_image_session else None
    
    # ë¡œê·¸ì¸í•˜ì§€ ì•Šì€ ê²½ìš° ì—°ê²° ê±°ë¶€
    if not session or not session.is_authenticated:
        await websocket.close(code=4001, reason="ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ê³„ì •(data_id) ë‹¨ìœ„ë¡œ WebSocket ë£¸ì„ í†µì¼ (ì„¸ì…˜ êµ¬ë¶„ ì œê±°)
    await ws_manager.connect(websocket, session.data_id)
    update_activity()
    
    try:
        # ì—°ê²° ì‹œ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "connected",
            "content": "ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.",
            # í”„ë¡ íŠ¸ì—ì„œ ì“°ëŠ” ê°’ë„ ê³„ì • í‚¤ë¡œ í†µì¼
            "session_id": session.data_id,
            "connected_users": ws_manager.get_session_count()
        })
        
        # í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "model_status_change",
            "model_loaded": pipe is not None,
            "current_model": current_model
        })
        
        # í¸ì§‘ ëª¨ë¸ ìƒíƒœ ì „ì†¡
        await websocket.send_json({
            "type": "edit_model_status_change",
            "model_loaded": longcat_edit_manager.is_loaded,
            "current_model": longcat_edit_manager.current_model
        })
        
        # ì ‘ì†ì ìˆ˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        await ws_manager.broadcast({
            "type": "user_count",
            "count": ws_manager.get_session_count()
        })
        
        while True:
            data = await websocket.receive_text()
            update_activity()
            
            # í´ë¼ì´ì–¸íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬
            try:
                message = json.loads(data)
                # í•‘/í ì²˜ë¦¬
                if message.get("type") == "ping":
                    await websocket.send_json({"type": "pong"})
            except:
                pass
            
    except WebSocketDisconnect:
        await ws_manager.disconnect(websocket)
        
        # ì ‘ì†ì ìˆ˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        await ws_manager.broadcast({
            "type": "user_count",
            "count": ws_manager.get_session_count()
        })


# ============= LongCat-Image-Edit API =============

def update_edit_activity():
    """í¸ì§‘ ëª¨ë¸ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸"""
    global edit_last_activity_time
    edit_last_activity_time = time.time()


@app.get("/api/edit/status")
async def get_edit_status(request: Request):
    """í¸ì§‘ ëª¨ë¸ ìƒíƒœ"""
    update_edit_activity()
    
    session = await get_session_from_request(request)
    require_auth(session)
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)
    
    status = {
        "model_loaded": longcat_edit_manager.is_loaded,
        "current_model": longcat_edit_manager.current_model,
        "current_quantization": longcat_edit_manager.current_quantization,
        "cpu_offload_enabled": longcat_edit_manager.cpu_offload_enabled,
        # ì €ì¥ëœ(ê¸°ë³¸) í¸ì§‘ ëª¨ë¸ ì„¤ì •ê°’ - ìƒˆë¡œê³ ì¹¨/ì¬ì‹œì‘ í›„ UIì—ì„œ ìœ ì§€ë˜ë„ë¡ ì œê³µ
        "saved_edit_quantization": settings.get("edit_quantization", "BF16 (ê¸°ë³¸, ìµœê³ í’ˆì§ˆ)"),
        "saved_edit_cpu_offload": settings.get("edit_cpu_offload", True),
        "device": longcat_edit_manager.device or longcat_edit_manager.get_device(),
        "vram": get_vram_info(),
        "session_id": session.data_id,
        "is_admin": is_admin,
        "quantization_options": list(EDIT_QUANTIZATION_OPTIONS.keys()),
        # ì–‘ìí™” ì˜µì…˜ ìƒì„¸ ì •ë³´ (ì˜ˆìƒ VRAM í¬í•¨)
        "quantization_details": {
            name: {
                "type": info.get("type"),
                "estimated_vram": info.get("estimated_vram", "N/A"),
            }
            for name, info in EDIT_QUANTIZATION_OPTIONS.items()
        },
    }
    
    # ê´€ë¦¬ìì¸ ê²½ìš° ì¶”ê°€ ì •ë³´
    if is_admin:
        status["available_devices"] = gpu_monitor.get_available_devices()
    
    return status


@app.post("/api/edit/model/load")
async def load_edit_model(request: Request, model_request: EditModelLoadRequest):
    """LongCat-Image-Edit ëª¨ë¸ ë¡œë“œ"""
    global edit_model_lock
    
    if edit_model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    # GPU ì„ íƒ (ê´€ë¦¬ìë§Œ íŠ¹ì • GPU ì§€ì • ê°€ëŠ¥)
    target_device = model_request.target_device
    client_host = request.client.host if request.client else None
    is_admin = is_localhost(client_host)

    # UIê°€ target_device="auto"ë¡œ ë³´ë‚´ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ,
    # ê´€ë¦¬ìê°€ ì„¤ì •í•œ í¸ì§‘ ê¸°ë³¸ GPUë¥¼ ìë™ ì ìš©í•œë‹¤.
    if target_device == "auto":
        target_device = settings.get("edit_gpu", DEFAULT_GPU_SETTINGS["edit_gpu"])

    if not is_admin and target_device != "auto":
        # ê´€ë¦¬ìê°€ ì•„ë‹Œ ê²½ìš° autoë¡œ ê°•ì œ
        target_device = "auto"
    
    async with edit_model_lock:
        async def progress_callback(percent, label, detail):
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": percent,
                "label": label,
                "detail": detail,
                "stage": "loading" if percent < 100 else "complete"
            })
        
        try:
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 0,
                "label": "ğŸ”§ í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì‹œì‘...",
                "detail": "",
                "stage": "init"
            })
            
            # ì–‘ìí™”/CPU ì˜¤í”„ë¡œë”©ì€ ê´€ë¦¬ìë§Œ ë³€ê²½ ê°€ëŠ¥
            requested_quantization = model_request.quantization
            requested_cpu_offload = model_request.cpu_offload
            if not is_admin:
                requested_quantization = settings.get("edit_quantization", requested_quantization)
                requested_cpu_offload = settings.get("edit_cpu_offload", requested_cpu_offload)

            success, message = await longcat_edit_manager.load_model(
                quantization=requested_quantization,
                cpu_offload=requested_cpu_offload,
                model_path=model_request.model_path if model_request.model_path else None,
                target_device=target_device,
                progress_callback=progress_callback
            )
            
            if success:
                await ws_manager.broadcast({
                    "type": "edit_model_status_change",
                    "model_loaded": True,
                    "current_model": longcat_edit_manager.current_model,
                    "device": longcat_edit_manager.device
                })
                await ws_manager.broadcast({
                    "type": "edit_system",
                    "content": f"âœ… í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ({longcat_edit_manager.device})"
                })
                return {"success": True, "message": message, "device": longcat_edit_manager.device}
            else:
                await ws_manager.broadcast({
                    "type": "edit_model_progress",
                    "progress": 0,
                    "label": "âŒ ë¡œë“œ ì‹¤íŒ¨",
                    "detail": message,
                    "stage": "error"
                })
                raise HTTPException(500, message)
                
        except Exception as e:
            await ws_manager.broadcast({
                "type": "edit_system",
                "content": f"âŒ í¸ì§‘ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
            })
            raise HTTPException(500, str(e))


@app.post("/api/edit/model/unload")
async def unload_edit_model(request: Request):
    """LongCat-Image-Edit ëª¨ë¸ ì–¸ë¡œë“œ"""
    global edit_model_lock
    
    if edit_model_lock.locked():
        raise HTTPException(409, "ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ëª¨ë¸ì„ ë¡œë“œ/ì–¸ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
    
    async with edit_model_lock:
        try:
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 50,
                "label": "í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...",
                "detail": ""
            })
            
            success, message = await longcat_edit_manager.unload_model()
            
            await ws_manager.broadcast({
                "type": "edit_model_progress",
                "progress": 100,
                "label": "ì–¸ë¡œë“œ ì™„ë£Œ!",
                "detail": f"VRAM: {get_vram_info()}"
            })
            
            await ws_manager.broadcast({
                "type": "edit_model_status_change",
                "model_loaded": False,
                "current_model": None
            })
            
            await ws_manager.broadcast({
                "type": "complete",
                "content": "âœ… í¸ì§‘ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!"
            })
            
            return {"success": success, "message": message}
            
        except Exception as e:
            raise HTTPException(500, str(e))


@app.post("/api/edit/generate")
async def edit_image(
    request: Request,
    image: UploadFile = File(...),
    prompt: str = Form(...),
    korean_prompt: str = Form(""),
    steps: int = Form(50),
    guidance_scale: float = Form(4.5),
    seed: int = Form(-1),
    num_images: int = Form(1),
    auto_translate: str = Form("true"),
    reference_image: Optional[UploadFile] = File(None)
):
    """ì´ë¯¸ì§€ í¸ì§‘ ì‹¤í–‰"""
    update_edit_activity()
    
    session = await get_session_from_request(request)
    require_auth(session)
    
    if not longcat_edit_manager.is_loaded:
        raise HTTPException(400, "í¸ì§‘ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    if not prompt.strip():
        raise HTTPException(400, "í¸ì§‘ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # Formì—ì„œ ë°›ì€ auto_translate ë¬¸ìì—´ì„ boolë¡œ ë³€í™˜
    auto_translate_bool = auto_translate.lower() in ("true", "1", "yes")
    
    try:
        # ì´ë¯¸ì§€ ë¡œë“œ
        image_data = await image.read()
        pil_image = Image.open(BytesIO(image_data)).convert("RGB")
        
        # ì°¸ì¡° ì´ë¯¸ì§€ ë¡œë“œ (ìˆìœ¼ë©´)
        ref_image = None
        if reference_image:
            ref_data = await reference_image.read()
            ref_image = Image.open(BytesIO(ref_data)).convert("RGB")
        
        # í”„ë¡¬í”„íŠ¸ ë²ˆì—­
        final_prompt = prompt
        if auto_translate_bool and edit_translator.is_korean(prompt):
            await ws_manager.send_to_session(session.data_id, {
                "type": "edit_system",
                "content": "ğŸŒ í¸ì§‘ ì§€ì‹œì–´ ë²ˆì—­ ì¤‘..."
            })
            final_prompt, success = edit_translator.translate(prompt)
            if not success:
                await ws_manager.send_to_session(session.data_id, {
                    "type": "edit_system",
                    "content": "âš ï¸ ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©"
                })
        
        # í¸ì§‘ ì‹œì‘ ë©”ì‹œì§€
        await ws_manager.send_to_session(session.data_id, {
            "type": "edit_system",
            "content": "ğŸ¨ ì´ë¯¸ì§€ í¸ì§‘ ì¤‘..."
        })
        
        # ì§„í–‰ ìƒí™© ì½œë°± ì •ì˜
        async def edit_progress_callback(current_image: int, total_images: int, current_step: int, total_steps: int):
            # ì „ì²´ ì§„í–‰ë¥  ê³„ì‚° (ì´ë¯¸ì§€ + ìŠ¤í… ê¸°ì¤€)
            image_progress = (current_image - 1) / total_images
            step_progress = current_step / total_steps / total_images
            overall_progress = int((image_progress + step_progress) * 100)
            
            await ws_manager.send_to_session(session.data_id, {
                "type": "edit_progress",
                "current_image": current_image,
                "total_images": total_images,
                "current_step": current_step,
                "total_steps": total_steps,
                "progress": overall_progress
            })
        
        # ìƒíƒœ ë©”ì‹œì§€ ì½œë°± ì •ì˜ (ì°¸ì¡° ì´ë¯¸ì§€ ë¶„ì„ ë“±)
        async def edit_status_callback(message: str):
            await ws_manager.send_to_session(session.data_id, {
                "type": "edit_system",
                "content": message
            })
        
        # í¸ì§‘ ì‹¤í–‰
        success, results, message = await longcat_edit_manager.edit_image(
            image=pil_image,
            prompt=final_prompt,
            num_inference_steps=steps,
            guidance_scale=guidance_scale,
            seed=seed,
            num_images=num_images,
            reference_image=ref_image,
            progress_callback=edit_progress_callback,
            status_callback=edit_status_callback
        )
        
        if not success:
            raise HTTPException(500, message)
        
        # ì„¸ì…˜ë³„ ì¶œë ¥ ë””ë ‰í† ë¦¬
        outputs_dir = session.get_outputs_dir()
        outputs_dir.mkdir(parents=True, exist_ok=True)
        
        # ê²°ê³¼ ì €ì¥ ë° ë°˜í™˜
        images_response = []
        result_paths = []
        
        for i, result in enumerate(results):
            result_image = result["image"]
            seed = result["seed"]
            
            # íŒŒì¼ëª… ìƒì„±
            filename = filename_generator.generate(
                pattern=settings.get("filename_pattern", "{date}_{time}_{seed}"),
                prompt=final_prompt,
                seed=seed
            )
            filename = f"edit_{filename}"
            output_path = outputs_dir / filename
            
            # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ì €ì¥
            metadata = ImageMetadata.create_metadata(
                prompt=final_prompt,
                seed=seed,
                width=result_image.width,
                height=result_image.height,
                steps=steps,
                guidance_scale=guidance_scale,
                model="LongCat-Image-Edit",
            )
            ImageMetadata.save_with_metadata(result_image, output_path, metadata)
            
            result_paths.append(str(output_path))
            images_response.append({
                "base64": image_to_base64(result_image),
                "filename": filename,
                "seed": seed,
                "path": f"/outputs/{session.data_id}/{filename}"
            })
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        edit_history_mgr = get_edit_history_manager_sync(session.data_id)
        history_entry = edit_history_mgr.add(
            prompt=final_prompt,
            korean_prompt=korean_prompt,
            settings={
                "steps": steps,
                "guidance_scale": guidance_scale,
                "seed": results[0]["seed"] if results else -1,
            },
            result_image_paths=[img["path"] for img in images_response]
        )
        
        # ì™„ë£Œ ë©”ì‹œì§€
        await ws_manager.send_to_session(session.data_id, {
            "type": "edit_system",
            "content": f"âœ… í¸ì§‘ ì™„ë£Œ! (ì‹œë“œ: {results[0]['seed'] if results else 'N/A'})"
        })
        
        # ê²°ê³¼ ì „ì†¡
        await ws_manager.send_to_session(session.data_id, {
            "type": "edit_result",
            "images": images_response,
            "seed": results[0]["seed"] if results else -1,
            "prompt": final_prompt,
            "history_id": history_entry.id
        })
        
        return {
            "success": True,
            "images": images_response,
            "seed": results[0]["seed"] if results else -1,
            "prompt": final_prompt,
            "history_id": history_entry.id
        }
        
    except Exception as e:
        await ws_manager.send_to_session(session.data_id, {
            "type": "error",
            "content": f"âŒ í¸ì§‘ ì˜¤ë¥˜: {str(e)}"
        })
        raise HTTPException(500, str(e))


@app.post("/api/edit/translate")
async def translate_edit_instruction(request: Request, trans_request: EditTranslateRequest):
    """í¸ì§‘ ì§€ì‹œì–´ ë²ˆì—­"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    translated, success = edit_translator.translate(trans_request.text)
    return {"success": success, "translated": translated}


@app.post("/api/edit/enhance")
async def enhance_edit_instruction(request: Request, enhance_request: EditEnhanceRequest):
    """í¸ì§‘ ì§€ì‹œì–´ í–¥ìƒ"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    enhanced, success = edit_enhancer.enhance(enhance_request.instruction)
    return {"success": success, "enhanced": enhanced}


@app.post("/api/edit/suggest")
async def suggest_edits(request: Request, suggest_request: EditSuggestRequest):
    """í¸ì§‘ ì•„ì´ë””ì–´ ì œì•ˆ"""
    update_edit_activity()
    from utils.llm_client import llm_client
    
    if not llm_client.is_available:
        raise HTTPException(400, "LLM APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    suggestions, success = edit_suggester.suggest(
        context=suggest_request.context,
        image_description=suggest_request.image_description
    )
    
    # í•œêµ­ì–´ ì œì•ˆë„ í•¨ê»˜ ë°˜í™˜
    korean_suggestions, _ = edit_suggester.suggest_korean(
        context=suggest_request.context,
        image_description=suggest_request.image_description
    )
    
    return {
        "success": success,
        "suggestions": suggestions,
        "suggestions_korean": korean_suggestions
    }


# ============= í¸ì§‘ íˆìŠ¤í† ë¦¬ API =============
@app.get("/api/edit/history")
async def get_edit_history(request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ëª©ë¡ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    entries = edit_history_mgr.get_all()
    
    response = JSONResponse(content={"history": [e.to_dict() for e in entries[:50]]})
    set_session_cookie(response, session)
    return response


@app.get("/api/edit/history/{history_id}")
async def get_edit_history_detail(history_id: str, request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ìƒì„¸ ì •ë³´ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    entry = edit_history_mgr.get_by_id(history_id)
    
    if not entry:
        raise HTTPException(404, "í¸ì§‘ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"history": entry.to_dict()}


@app.get("/api/edit/history/{history_id}/chain")
async def get_edit_history_chain(history_id: str, request: Request):
    """ë©€í‹°í„´ í¸ì§‘ ì²´ì¸ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    chain = edit_history_mgr.get_chain(history_id)
    
    return {"chain": [e.to_dict() for e in chain]}


@app.patch("/api/edit/history/{history_id}/conversation")
async def update_edit_history_conversation(history_id: str, request: Request, conv_request: EditConversationUpdateRequest):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ëŒ€í™” ë‚´ìš© ì—…ë°ì´íŠ¸ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    
    success = edit_history_mgr.update_conversation(history_id, conv_request.conversation)
    if not success:
        raise HTTPException(404, "í¸ì§‘ íˆìŠ¤í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    return {"success": True}


@app.delete("/api/edit/history")
async def clear_edit_history(request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ ì‚­ì œ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    edit_history_mgr.clear()
    return {"success": True}


@app.delete("/api/edit/history/{history_id}")
async def delete_edit_history_entry(history_id: str, request: Request):
    """í¸ì§‘ íˆìŠ¤í† ë¦¬ í•­ëª© ì‚­ì œ (ì‚¬ìš©ìë³„)"""
    session = await get_session_from_request(request)
    require_auth(session)
    edit_history_mgr = get_edit_history_manager_sync(session.data_id)
    success = edit_history_mgr.delete(history_id)
    return {"success": success}


# ============= ë©”ì¸ =============
if __name__ == "__main__":
    # ì¶œë ¥ í´ë” ìƒì„±
    OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ¨ Z-Image WebUI ì‹œì‘...")
    print(f"ğŸ“ http://localhost:{SERVER_PORT}")
    print("ğŸŒ ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì› í™œì„±í™”")
    
    uvicorn.run(
        app,
        host=SERVER_HOST,
        port=SERVER_PORT,
        reload=SERVER_RELOAD
    )
