# Qwen-Image-Edit-2511 ì™„ë²½ ê°€ì´ë“œ

[![Hugging Face](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Qwen--Image--Edit--2511-blue)](https://huggingface.co/Qwen/Qwen-Image-Edit-2511)
[![License](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![arXiv](https://img.shields.io/badge/arXiv-2508.02324-b31b1b.svg)](https://arxiv.org/abs/2508.02324)

> **Qwen-Image-Edit-2511**ì€ Alibabaì˜ Qwen íŒ€ì—ì„œ ê°œë°œí•œ ê³ ê¸‰ ì´ë¯¸ì§€ í¸ì§‘ AI ëª¨ë¸ë¡œ, ì´ì „ ë²„ì „ì¸ Qwen-Image-Edit-2509ë¥¼ í¬ê²Œ ê°œì„ í•œ ë²„ì „ì…ë‹ˆë‹¤.

> ğŸ“Œ **ì´ ë¬¸ì„œëŠ” Windows í™˜ê²½ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.**

---

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì£¼ìš” ê°œì„  ì‚¬í•­](#ì£¼ìš”-ê°œì„ -ì‚¬í•­)
- [ëª¨ë¸ ì•„í‚¤í…ì²˜](#ëª¨ë¸-ì•„í‚¤í…ì²˜)
- [ì‹œìŠ¤í…œ ìš”êµ¬ ì‚¬í•­](#ì‹œìŠ¤í…œ-ìš”êµ¬-ì‚¬í•­)
- [ì„¤ì¹˜ ë°©ë²•](#ì„¤ì¹˜-ë°©ë²•)
- [ë¹ ë¥¸ ì‹œì‘](#ë¹ ë¥¸-ì‹œì‘)
- [íŒŒë¼ë¯¸í„° ìƒì„¸ ê°€ì´ë“œ](#íŒŒë¼ë¯¸í„°-ìƒì„¸-ê°€ì´ë“œ)
- [ë‹¤ì–‘í•œ ì‚¬ìš© ì˜ˆì œ](#ë‹¤ì–‘í•œ-ì‚¬ìš©-ì˜ˆì œ)
- [ìµœì í™” ë°©ë²•](#ìµœì í™”-ë°©ë²•)
- [LoRA í™œìš© ê°€ì´ë“œ](#lora-í™œìš©-ê°€ì´ë“œ)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
- [ë¼ì´ì„ ìŠ¤ ë° ì¸ìš©](#ë¼ì´ì„ ìŠ¤-ë°-ì¸ìš©)
- [ì°¸ê³  ìë£Œ](#ì°¸ê³ -ìë£Œ)

---

## ê°œìš”

**Qwen-Image-Edit-2511**ì€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ í¸ì§‘í•˜ëŠ” ìµœì²¨ë‹¨ AI ëª¨ë¸ì…ë‹ˆë‹¤. Diffusion Transformer ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì•½ **20B(200ì–µ)** ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ê¸°ëŠ¥

| ê¸°ëŠ¥ | ì„¤ëª… |
|------|------|
| **ì‹œë©˜í‹± í¸ì§‘** | ê°ì²´ ì¶”ê°€, ì œê±°, ìˆ˜ì •, íšŒì „, ìŠ¤íƒ€ì¼ ë³€í™˜ |
| **ì •í™•í•œ í…ìŠ¤íŠ¸ í¸ì§‘** | ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì • (ì˜ì–´, ì¤‘êµ­ì–´ ì§€ì›) |
| **ë‹¤ì¤‘ ì´ë¯¸ì§€ ì…ë ¥** | ìµœëŒ€ 3ê°œì˜ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ í™œìš©í•œ ë³µí•© í¸ì§‘ |
| **ìºë¦­í„° ì¼ê´€ì„±** | ì¸ë¬¼ì˜ ì •ì²´ì„±ê³¼ íŠ¹ì§•ì„ ìœ ì§€í•˜ë©´ì„œ í¸ì§‘ |
| **LoRA í†µí•©** | ë³„ë„ íŠœë‹ ì—†ì´ ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ ì ìš© ê°€ëŠ¥ |
| **ì‚°ì—… ë””ìì¸** | ì œí’ˆ ë””ìì¸ ë° ë§ˆì¼€íŒ… ë¹„ì£¼ì–¼ ì œì‘ ì§€ì› |

---

## ì£¼ìš” ê°œì„  ì‚¬í•­

Qwen-Image-Edit-2511ì€ ì´ì „ ë²„ì „(2509) ëŒ€ë¹„ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê°œì„ ì´ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤:

### 1. ğŸ¯ ì´ë¯¸ì§€ ë“œë¦¬í”„íŠ¸ ì™„í™” (Mitigate Image Drift)

- ë°˜ë³µì ì¸ í¸ì§‘ ê³¼ì •ì—ì„œ ì›ë³¸ ì´ë¯¸ì§€ì˜ ì •ì²´ì„±ì´ íë ¤ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°
- ì „ì²´ì ì¸ ì´ë¯¸ì§€ êµ¬ì¡°ì™€ ëŒ€ìƒì˜ íŠ¹ì§•ì´ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€
- ì—¬ëŸ¬ ì°¨ë¡€ ìˆ˜ì •ì„ ê±°ì³ë„ ì´ë¯¸ì§€ í’ˆì§ˆ ì €í•˜ ìµœì†Œí™”

### 2. ğŸ‘¥ ìºë¦­í„° ì¼ê´€ì„± í–¥ìƒ (Improved Character Consistency)

- ë‹¨ì¼ ì¸ë¬¼ í¸ì§‘ ì‹œ ì¸ë¬¼ì˜ ì •ì²´ì„±ê³¼ ì‹œê°ì  íŠ¹ì§• ë³´ì¡´
- **ë‹¤ì¤‘ ì¸ë¬¼ ì‚¬ì§„**ì—ì„œë„ ê° ì¸ë¬¼ì˜ ì¼ê´€ì„± ìœ ì§€
- ë‘ ê°œì˜ ë³„ë„ ì¸ë¬¼ ì´ë¯¸ì§€ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ê·¸ë£¹ ì‚¬ì§„ìœ¼ë¡œ í•©ì„± ê°€ëŠ¥

### 3. ğŸ¨ LoRA ê¸°ëŠ¥ í†µí•© (Integrated LoRA Capabilities)

ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ê°œë°œëœ ì¸ê¸° ìˆëŠ” LoRAë“¤ì´ ê¸°ë³¸ ëª¨ë¸ì— í†µí•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

- **ì¡°ëª… í–¥ìƒ LoRA**: í˜„ì‹¤ì ì¸ ì¡°ëª… ì œì–´
- **ë·°í¬ì¸íŠ¸ ìƒì„±**: ìƒˆë¡œìš´ ì‹œì ì—ì„œì˜ ì´ë¯¸ì§€ ìƒì„±
- ì¶”ê°€ì ì¸ íŠœë‹ ì—†ì´ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

### 4. ğŸ­ ì‚°ì—… ë””ìì¸ ìƒì„± ê°•í™” (Enhanced Industrial Design)

- ì‚°ì—… ì œí’ˆ ë°°ì¹˜ ë””ìì¸
- ì‚°ì—… ë¶€í’ˆì˜ ì¬ë£Œ êµì²´
- ë°˜ë³µì ì¸ ë””ìì¸ ì‹œì•ˆ ìƒì„±
- ë§ˆì¼€íŒ… ë¹„ì£¼ì–¼ ì œì‘

### 5. ğŸ“ ê¸°í•˜í•™ì  ì¶”ë¡  ëŠ¥ë ¥ ê°•í™” (Strengthened Geometric Reasoning)

- ì„¤ê³„ë‚˜ ì£¼ì„ì„ ìœ„í•œ ë³´ì¡° êµ¬ì„±ì„ (auxiliary construction lines) ì§ì ‘ ìƒì„±
- êµ¬ì¡°ì  ë³€í˜• ë° í˜•íƒœ ì¸ì‹ í¸ì§‘ì˜ ì •í™•ë„ í–¥ìƒ
- ê¸°í•˜í•™ì  êµ¬ì¡° ì´í•´ ëŠ¥ë ¥ ê°œì„ 

---

## ëª¨ë¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Qwen-Image-Edit-2511                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Base Architecture: Diffusion Transformer (DiT)                 â”‚
â”‚  Parameters: ~20B                                               â”‚
â”‚  Input: Text Prompt + Up to 3 Images                            â”‚
â”‚  Output: Edited Image                                           â”‚
â”‚  Pipeline: QwenImageEditPlusPipeline                            â”‚
â”‚  Precision: BFloat16 / Float16                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Components:                                                    â”‚
â”‚  â”œâ”€â”€ Text Encoder (Multilingual: EN, ZH)                        â”‚
â”‚  â”œâ”€â”€ Vision Encoder                                             â”‚
â”‚  â”œâ”€â”€ Diffusion Transformer                                      â”‚
â”‚  â””â”€â”€ VAE Decoder                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ì‹œìŠ¤í…œ ìš”êµ¬ ì‚¬í•­

### ìµœì†Œ ìš”êµ¬ ì‚¬í•­

| êµ¬ì„± ìš”ì†Œ | ìµœì†Œ ì‚¬ì–‘ | ê¶Œì¥ ì‚¬ì–‘ |
|-----------|-----------|-----------|
| **GPU** | NVIDIA RTX 3090 (24GB) | NVIDIA RTX 4090 / A100 |
| **VRAM** | 24GB | 40GB+ |
| **RAM** | 32GB | 64GB |
| **ì €ì¥ ê³µê°„** | 50GB | 100GB |
| **Python** | 3.10+ | 3.11+ |
| **CUDA** | 11.8+ | 12.1+ |

### ê¶Œì¥ í™˜ê²½ (Windows)

```powershell
# Python ë²„ì „ í™•ì¸
python --version  # 3.10 ì´ìƒ ê¶Œì¥

# CUDA ë²„ì „ í™•ì¸
nvcc --version    # 11.8 ì´ìƒ ê¶Œì¥

# GPU ë©”ëª¨ë¦¬ í™•ì¸
nvidia-smi
```

---

## ì„¤ì¹˜ ë°©ë²• (Windows)

### 1. ê¸°ë³¸ ì„¤ì¹˜

```powershell
# ê°€ìƒ í™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv

# ê°€ìƒ í™˜ê²½ í™œì„±í™” (Windows PowerShell)
.\venv\Scripts\Activate.ps1

# ë˜ëŠ” ëª…ë ¹ í”„ë¡¬í”„íŠ¸(CMD)ì—ì„œ
# qwen-edit-env\Scripts\activate.bat

# PyTorch ì„¤ì¹˜ (CUDA ë²„ì „ì— ë§ê²Œ)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126

# diffusers ìµœì‹  ë²„ì „ ì„¤ì¹˜ (í•„ìˆ˜)
pip install git+https://github.com/huggingface/diffusers

# ê¸°íƒ€ í•„ìˆ˜ íŒ¨í‚¤ì§€
pip install transformers accelerate pillow
```

> ğŸ’¡ **PowerShell ì‹¤í–‰ ì •ì±… ì˜¤ë¥˜ ì‹œ**: 
> `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser` ì‹¤í–‰ í›„ ë‹¤ì‹œ ì‹œë„

### 2. ì „ì²´ ì˜ì¡´ì„± ì„¤ì¹˜

```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
pip install git+https://github.com/huggingface/diffusers
pip install transformers>=4.40.0
pip install accelerate>=0.26.0
pip install safetensors
pip install pillow
pip install numpy
```

### requirements.txt

```txt
torch torchvision --index-url https://download.pytorch.org/whl/cu126
diffusers @ git+https://github.com/huggingface/diffusers
transformers>=4.40.0
accelerate>=0.26.0
safetensors>=0.4.0
pillow>=10.0.0
numpy>=1.24.0
```

---

## ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import os
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. íŒŒì´í”„ë¼ì¸ ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
print("âœ… íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ")

# GPUë¡œ ì´ë™
pipeline.to('cuda')
pipeline.set_progress_bar_config(disable=None)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. ì…ë ¥ ì´ë¯¸ì§€ ë¡œë“œ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
image = Image.open("input_image.png")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. í”„ë¡¬í”„íŠ¸ ì„¤ì • ë° í¸ì§‘ ìˆ˜í–‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
prompt = "Change the background to a sunset sky"

inputs = {
    "image": [image],
    "prompt": prompt,
    "generator": torch.manual_seed(42),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. ì´ë¯¸ì§€ ìƒì„± ë° ì €ì¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
with torch.inference_mode():
    output = pipeline(**inputs)
    output_image = output.images[0]
    output_image.save("edited_image.png")
    print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {os.path.abspath('edited_image.png')}")
```

---

## íŒŒë¼ë¯¸í„° ìƒì„¸ ê°€ì´ë“œ

### ì£¼ìš” íŒŒë¼ë¯¸í„°

| íŒŒë¼ë¯¸í„° | íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|------|--------|------|
| `image` | List[PIL.Image] | í•„ìˆ˜ | ì…ë ¥ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 3ê°œ) |
| `prompt` | str | í•„ìˆ˜ | í¸ì§‘ ì§€ì‹œ í…ìŠ¤íŠ¸ |
| `negative_prompt` | str | " " | ìƒì„±ì—ì„œ ì œì™¸í•  ìš”ì†Œ |
| `num_inference_steps` | int | 40 | ë””ë…¸ì´ì§• ìŠ¤í… ìˆ˜ |
| `guidance_scale` | float | 1.0 | í”„ë¡¬í”„íŠ¸ ê°€ì´ë˜ìŠ¤ ê°•ë„ |
| `true_cfg_scale` | float | 4.0 | True CFG ìŠ¤ì¼€ì¼ |
| `generator` | torch.Generator | None | ì‹œë“œ ì œì–´ìš© ìƒì„±ê¸° |
| `num_images_per_prompt` | int | 1 | í”„ë¡¬í”„íŠ¸ë‹¹ ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜ |

### íŒŒë¼ë¯¸í„° ìƒì„¸ ì„¤ëª…

#### `num_inference_steps` (ì¶”ë¡  ìŠ¤í… ìˆ˜)

```python
# ë¹ ë¥¸ ìƒì„± (í’ˆì§ˆ ë‚®ìŒ)
inputs["num_inference_steps"] = 20

# ê· í˜• ì¡íŒ ì„¤ì • (ê¶Œì¥)
inputs["num_inference_steps"] = 40

# ê³ í’ˆì§ˆ ìƒì„± (ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
inputs["num_inference_steps"] = 60
```

| ìŠ¤í… ìˆ˜ | í’ˆì§ˆ | ì†ë„ | ìš©ë„ |
|---------|------|------|------|
| 20-25 | ë‚®ìŒ | ë¹ ë¦„ | í…ŒìŠ¤íŠ¸/í”„ë¦¬ë·° |
| 35-45 | ì¤‘ê°„ | ë³´í†µ | ì¼ë°˜ ì‚¬ìš© |
| 50-60+ | ë†’ìŒ | ëŠë¦¼ | ìµœì¢… ê²°ê³¼ë¬¼ |

#### `true_cfg_scale` (True CFG ìŠ¤ì¼€ì¼)

í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ëª¨ë¸ì˜ ì¶©ì‹¤ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤:

```python
# ë‚®ì€ ê°’: ë” ìì—°ìŠ¤ëŸ½ì§€ë§Œ í”„ë¡¬í”„íŠ¸ ë°˜ì˜ ì•½í•¨
inputs["true_cfg_scale"] = 2.0

# ê¶Œì¥ ê°’
inputs["true_cfg_scale"] = 4.0

# ë†’ì€ ê°’: í”„ë¡¬í”„íŠ¸ ê°•í•˜ê²Œ ë°˜ì˜, ë•Œë•Œë¡œ ë¶€ìì—°ìŠ¤ëŸ¬ì›€
inputs["true_cfg_scale"] = 6.0
```

#### `negative_prompt` (ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸)

ìƒì„±ì—ì„œ ì œì™¸í•˜ê³  ì‹¶ì€ ìš”ì†Œë¥¼ ì§€ì •í•©ë‹ˆë‹¤:

```python
inputs["negative_prompt"] = "blurry, low quality, distorted, ugly, bad anatomy"
```

---

## ë‹¤ì–‘í•œ ì‚¬ìš© ì˜ˆì œ

### ì˜ˆì œ 1: ë‹¨ì¼ ì´ë¯¸ì§€ í¸ì§‘

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

# íŒŒì´í”„ë¼ì¸ ë¡œë“œ
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

# ë‹¨ì¼ ì´ë¯¸ì§€ í¸ì§‘
image = Image.open("portrait.png")
prompt = "Add a wizard hat and cloak to the person in this photo"

inputs = {
    "image": [image],
    "prompt": prompt,
    "generator": torch.manual_seed(42),
    "true_cfg_scale": 4.0,
    "negative_prompt": "blurry, distorted",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("wizard_portrait.png")
```

### ì˜ˆì œ 2: ë‘ ì´ë¯¸ì§€ í•©ì„± (ë‹¤ì¤‘ ì¸ë¬¼)

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

# ë‘ ê°œì˜ ì¸ë¬¼ ì´ë¯¸ì§€ ë¡œë“œ
person1 = Image.open("person1.png")
person2 = Image.open("person2.png")

# ë‘ ì¸ë¬¼ì„ í•˜ë‚˜ì˜ ì¥ë©´ì— í•©ì„±
prompt = "The person on the left and the person on the right are sitting across from each other in a coffee shop, having a conversation"

inputs = {
    "image": [person1, person2],
    "prompt": prompt,
    "generator": torch.manual_seed(123),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("coffee_shop_scene.png")
```

### ì˜ˆì œ 3: ìŠ¤íƒ€ì¼ ë³€í™˜ (ì§€ë¸Œë¦¬ ìŠ¤íƒ€ì¼)

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

image = Image.open("landscape.png")
prompt = "Transform this landscape photo into Studio Ghibli animation style"

inputs = {
    "image": [image],
    "prompt": prompt,
    "generator": torch.manual_seed(777),
    "true_cfg_scale": 5.0,
    "negative_prompt": "realistic, photograph",
    "num_inference_steps": 50,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("ghibli_landscape.png")
```

### ì˜ˆì œ 4: í…ìŠ¤íŠ¸ í¸ì§‘

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

# í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì´ë¯¸ì§€
image = Image.open("poster.png")
prompt = "Change the poster title text from 'SUMMER SALE' to 'WINTER COLLECTION'"

inputs = {
    "image": [image],
    "prompt": prompt,
    "generator": torch.manual_seed(42),
    "true_cfg_scale": 4.5,
    "negative_prompt": " ",
    "num_inference_steps": 45,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("updated_poster.png")
```

### ì˜ˆì œ 5: ì‚°ì—… ë””ìì¸ - ì¬ë£Œ ë³€ê²½

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

# ì œí’ˆ ì´ë¯¸ì§€
image = Image.open("product.png")
prompt = "Change this plastic product to wood material. Apply natural wood grain texture."

inputs = {
    "image": [image],
    "prompt": prompt,
    "generator": torch.manual_seed(99),
    "true_cfg_scale": 4.0,
    "negative_prompt": "plastic, shiny, artificial",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("wooden_product.png")
```

### ì˜ˆì œ 6: ê°ì²´ ì¶”ê°€

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

image = Image.open("room.png")
prompt = "Add a decorated Christmas tree in the center of the room. The tree has lights and ornaments."

inputs = {
    "image": [image],
    "prompt": prompt,
    "generator": torch.manual_seed(1225),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 40,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("room_with_tree.png")
```

### ì˜ˆì œ 7: ê°ì²´ ì œê±°

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

image = Image.open("beach.png")
prompt = "Remove all people from the beach photo and leave only the clean beach"

inputs = {
    "image": [image],
    "prompt": prompt,
    "generator": torch.manual_seed(42),
    "true_cfg_scale": 4.0,
    "negative_prompt": "people, person, crowd",
    "num_inference_steps": 45,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("empty_beach.png")
```

### ì˜ˆì œ 8: ë°°ì¹˜ ì²˜ë¦¬ (ì—¬ëŸ¬ ì´ë¯¸ì§€ ìƒì„±)

```python
import torch
from PIL import Image
from diffusers import QwenImageEditPlusPipeline

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

image = Image.open("character.png")
prompt = "Place this character in various seasonal backgrounds"

# ì—¬ëŸ¬ ì‹œë“œë¡œ ë‹¤ì–‘í•œ ê²°ê³¼ ìƒì„±
seeds = [42, 123, 456, 789]

for i, seed in enumerate(seeds):
    inputs = {
        "image": [image],
        "prompt": prompt,
        "generator": torch.manual_seed(seed),
        "true_cfg_scale": 4.0,
        "negative_prompt": " ",
        "num_inference_steps": 40,
        "guidance_scale": 1.0,
        "num_images_per_prompt": 1,
    }
    
    with torch.inference_mode():
        output = pipeline(**inputs)
        output.images[0].save(f"character_season_{i+1}.png")
        print(f"âœ… ì´ë¯¸ì§€ {i+1} ì €ì¥ ì™„ë£Œ")
```

---

## ìµœì í™” ë°©ë²•

### 1. ğŸš€ ë©”ëª¨ë¦¬ ìµœì í™”

#### CPU ì˜¤í”„ë¡œë”©

VRAMì´ ë¶€ì¡±í•œ ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
from diffusers import QwenImageEditPlusPipeline
import torch

# ëª¨ë¸ ë¡œë“œ
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)

# ë°©ë²• 1: Model CPU Offload (ê¶Œì¥)
# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ë¥¼ CPUë¡œ ì´ë™
pipeline.enable_model_cpu_offload()

# ë°©ë²• 2: Sequential CPU Offload
# ë” ì ì€ VRAM ì‚¬ìš©, í•˜ì§€ë§Œ ë” ëŠë¦¼
pipeline.enable_sequential_cpu_offload()
```

#### Attention Slicing

ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•:

```python
# Attention ìŠ¬ë¼ì´ì‹± í™œì„±í™”
pipeline.enable_attention_slicing("max")

# ë˜ëŠ” ìë™ ì„¤ì •
pipeline.enable_attention_slicing("auto")
```

#### VAE ìŠ¬ë¼ì´ì‹±

```python
# VAE ìŠ¬ë¼ì´ì‹± (ë©”ëª¨ë¦¬ ì ˆì•½)
pipeline.enable_vae_slicing()

# VAE íƒ€ì¼ë§ (ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€)
pipeline.enable_vae_tiling()
```

### 2. âš¡ ì†ë„ ìµœì í™”

#### torch.compile (PyTorch 2.0+)

> âš ï¸ **Windows ì£¼ì˜ì‚¬í•­**: `torch.compile`ì€ Windowsì—ì„œ **ì œí•œì ìœ¼ë¡œ ì§€ì›**ë©ë‹ˆë‹¤.
> - Triton ë°±ì—”ë“œê°€ Windowsë¥¼ ì§€ì›í•˜ì§€ ì•Šì•„ `mode="reduce-overhead"` ì‚¬ìš© ë¶ˆê°€
> - `mode="default"` ë˜ëŠ” `mode="max-autotune"` ì‚¬ìš© ì‹œì—ë„ ì¼ë¶€ ê¸°ëŠ¥ ì œí•œ
> - Linux í™˜ê²½ì—ì„œë§Œ ì™„ì „í•œ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŒ

```python
import torch

# Windowsì—ì„œëŠ” ì œí•œì ìœ¼ë¡œ ë™ì‘
# Linuxì—ì„œ ìµœì ì˜ ì„±ëŠ¥ ë°œíœ˜
try:
    pipeline.transformer = torch.compile(
        pipeline.transformer,
        mode="default",  # Windowsì—ì„œëŠ” "default" ê¶Œì¥
    )
    print("âœ… torch.compile ì ìš© ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ torch.compile ì‹¤íŒ¨ (Windowsì—ì„œ ì •ìƒ): {e}")
```

#### xFormers ì‚¬ìš©

> âš ï¸ **Windows ì„¤ì¹˜ ì‹œ ì£¼ì˜ì‚¬í•­**: xFormersëŠ” Windowsì—ì„œ ì„¤ì¹˜ê°€ ê¹Œë‹¤ë¡œìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> PyTorch ë²„ì „ê³¼ CUDA ë²„ì „ì´ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.

```powershell
# Windowsì—ì„œ xFormers ì„¤ì¹˜
# ë°©ë²• 1: pipë¡œ ì§ì ‘ ì„¤ì¹˜ (PyTorch/CUDA ë²„ì „ ì¼ì¹˜ í•„ìš”)
pip install xformers

# ë°©ë²• 2: íŠ¹ì • ë²„ì „ ëª…ì‹œ ì„¤ì¹˜ (ê¶Œì¥)
# PyTorch 2.5.1 + CUDA 12.4 ê¸°ì¤€
pip install xformers==0.0.28.post3
```

```python
# xFormers memory efficient attention í™œì„±í™”
try:
    pipeline.enable_xformers_memory_efficient_attention()
    print("âœ… xFormers í™œì„±í™” ì„±ê³µ")
except Exception as e:
    print(f"âš ï¸ xFormers ì‚¬ìš© ë¶ˆê°€: {e}")
    print("   â†’ Attention Slicingìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    pipeline.enable_attention_slicing("auto")
```

#### ~~Flash Attention 2~~ (Windows ë¯¸ì§€ì›)

> âŒ **Windowsì—ì„œ ì‚¬ìš© ë¶ˆê°€**: Flash Attention 2ëŠ” **Linux ì „ìš©**ì…ë‹ˆë‹¤.
> - Windowsì—ì„œëŠ” ê³µì‹ ì§€ì›ë˜ì§€ ì•ŠìŒ
> - WSL2(Windows Subsystem for Linux) í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ê°€ëŠ¥
> - Windowsì—ì„œëŠ” **xFormers** ë˜ëŠ” **Attention Slicing**ì„ ëŒ€ì•ˆìœ¼ë¡œ ì‚¬ìš©

```python
# âŒ Windowsì—ì„œëŠ” ì•„ë˜ ì½”ë“œê°€ ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
# from diffusers import QwenImageEditPlusPipeline
# import torch
# 
# pipeline = QwenImageEditPlusPipeline.from_pretrained(
#     "Qwen/Qwen-Image-Edit-2511",
#     torch_dtype=torch.bfloat16,
#     use_flash_attention_2=True  # Linux ì „ìš©
# )

# âœ… Windows ëŒ€ì•ˆ: xFormers ë˜ëŠ” Attention Slicing ì‚¬ìš©
pipeline.enable_attention_slicing("auto")
```

### 3. ğŸ”¢ ì–‘ìí™” (Quantization)

#### 4-bit ì–‘ìí™” (bitsandbytes)

> âš ï¸ **Windows ì£¼ì˜ì‚¬í•­**: `bitsandbytes`ëŠ” ê³µì‹ì ìœ¼ë¡œ **Linux ì „ìš©**ì…ë‹ˆë‹¤.
> - Windowsì—ì„œëŠ” ë¹„ê³µì‹ ë¹Œë“œ(`bitsandbytes-windows`)ë¥¼ ì‚¬ìš©í•´ì•¼ í•¨
> - ì•ˆì •ì„±ì´ ë³´ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ **ì»¤ë®¤ë‹ˆí‹° ì–‘ìí™” ëª¨ë¸ ì‚¬ìš©ì„ ê¶Œì¥**

```powershell
# Windowsì—ì„œ bitsandbytes ì„¤ì¹˜ (ë¹„ê³µì‹)
pip install bitsandbytes-windows

# ë˜ëŠ” ìµœì‹  bitsandbytes (Windows ì‹¤í—˜ì  ì§€ì›)
pip install bitsandbytes>=0.43.0
```

```python
from diffusers import QwenImageEditPlusPipeline
import torch

# âš ï¸ Windowsì—ì„œëŠ” ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ
try:
    from transformers import BitsAndBytesConfig
    
    # 4-bit ì–‘ìí™” ì„¤ì •
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True
    )

    # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
    pipeline = QwenImageEditPlusPipeline.from_pretrained(
        "Qwen/Qwen-Image-Edit-2511",
        quantization_config=quantization_config,
        torch_dtype=torch.bfloat16
    )
    print("âœ… 4-bit ì–‘ìí™” ë¡œë“œ ì„±ê³µ")
except Exception as e:
    print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {e}")
    print("   â†’ ì»¤ë®¤ë‹ˆí‹° ì–‘ìí™” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
```

#### ì»¤ë®¤ë‹ˆí‹° ì–‘ìí™” ëª¨ë¸ ì‚¬ìš© (Windows ê¶Œì¥ âœ…)

Windows í™˜ê²½ì—ì„œëŠ” ì´ë¯¸ ì–‘ìí™”ëœ ì»¤ë®¤ë‹ˆí‹° ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì •ì ì…ë‹ˆë‹¤.

##### ovedrive/Qwen-Image-Edit-2511-4bit (ê¶Œì¥)

[ovedrive/Qwen-Image-Edit-2511-4bit](https://huggingface.co/ovedrive/Qwen-Image-Edit-2511-4bit)ëŠ” **NF4 ì–‘ìí™”** ëª¨ë¸ë¡œ, **20GB ë¯¸ë§Œì˜ VRAM**ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

**íŠ¹ì§•:**
- âœ… diffusersì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥
- âœ… 16GB VRAMì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥
- âœ… ì¤‘ìš” ë ˆì´ì–´ëŠ” full precision ìœ ì§€ (í’ˆì§ˆ ë³´ì¥)
- âœ… 10 ìŠ¤í…ì—ì„œë„ ì‘ë™

**VRAM ìš”êµ¬ëŸ‰:**

| ì„¤ì • | VRAM ìš”êµ¬ëŸ‰ | ì„¤ëª… |
|------|-------------|------|
| `pipeline.to("cuda")` | ~20GB | ì „ì²´ GPU ë¡œë“œ |
| `enable_model_cpu_offload()` | ~16GB | CPU ì˜¤í”„ë¡œë”© |

##### ì‚¬ìš© ì˜ˆì œ ì½”ë“œ

```python
import os
from PIL import Image
import torch
from diffusers import QwenImageEditPlusPipeline

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NF4 ì–‘ìí™” ëª¨ë¸ ë¡œë“œ (20GB ë¯¸ë§Œ VRAMì—ì„œ ì‹¤í–‰ ê°€ëŠ¥)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
model_path = "ovedrive/Qwen-Image-Edit-2511-4bit"
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    model_path, 
    torch_dtype=torch.bfloat16
)
print("âœ… Pipeline loaded")

# VRAM ì„¤ì •
# ë°©ë²• 1: 20GB+ VRAMì´ ìˆëŠ” ê²½ìš°
# pipeline.to("cuda")

# ë°©ë²• 2: 16GB VRAM (ê¶Œì¥)
pipeline.enable_model_cpu_offload()

pipeline.set_progress_bar_config(disable=None)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ì´ë¯¸ì§€ í¸ì§‘
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
image = Image.open("./input.png").convert("RGB")
prompt = "Change the background to a sunset sky"

inputs = {
    "image": image,
    "prompt": prompt,
    "generator": torch.manual_seed(0),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 20,  # 10 ìŠ¤í…ë„ ê°€ëŠ¥
}

with torch.inference_mode():
    output = pipeline(**inputs)

output_image = output.images[0]
output_image.save("output_image.png")
print(f"âœ… Image saved at {os.path.abspath('output_image.png')}")
```

##### ë‹¤ì¤‘ ì´ë¯¸ì§€ ì…ë ¥ ì˜ˆì œ

```python
import os
from PIL import Image
import torch
from diffusers import QwenImageEditPlusPipeline

# ëª¨ë¸ ë¡œë“œ
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "ovedrive/Qwen-Image-Edit-2511-4bit", 
    torch_dtype=torch.bfloat16
)
pipeline.enable_model_cpu_offload()

# ë‘ ê°œì˜ ì´ë¯¸ì§€ ë¡œë“œ
image1 = Image.open("person1.png").convert("RGB")
image2 = Image.open("person2.png").convert("RGB")

prompt = "The person on the left and person on the right are sitting together in a coffee shop"

inputs = {
    "image": [image1, image2],
    "prompt": prompt,
    "generator": torch.manual_seed(42),
    "true_cfg_scale": 4.0,
    "negative_prompt": " ",
    "num_inference_steps": 20,
    "guidance_scale": 1.0,
    "num_images_per_prompt": 1,
}

with torch.inference_mode():
    output = pipeline(**inputs)
    output.images[0].save("combined_scene.png")
```

> ğŸ“– ëª¨ë¸ í˜ì´ì§€: [ovedrive/Qwen-Image-Edit-2511-4bit](https://huggingface.co/ovedrive/Qwen-Image-Edit-2511-4bit)

### 4. ğŸ“Š ì¶”ë¡  ìŠ¤í… ìµœì í™”

#### ìŠ¤í… ë””ìŠ¤í‹¸ë ˆì´ì…˜

ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•œ Lightning/Fast ë²„ì „ ì‚¬ìš©:

```python
# Fast ë²„ì „ (ì»¤ë®¤ë‹ˆí‹° ìŠ¤í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
# https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-2511-Fast

# ë” ì ì€ ìŠ¤í…ìœ¼ë¡œ ë¹ ë¥¸ ê²°ê³¼
inputs["num_inference_steps"] = 8  # Lightning ë²„ì „
# ë˜ëŠ”
inputs["num_inference_steps"] = 4  # Turbo ë²„ì „
```

### 5. ğŸ¯ ì¢…í•© ìµœì í™” ì˜ˆì œ (Windows)

```python
import torch
from diffusers import QwenImageEditPlusPipeline
from PIL import Image

def create_optimized_pipeline_windows():
    """Windows í™˜ê²½ì— ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ìƒì„±"""
    
    # 1. ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ë¡œë“œ
    pipeline = QwenImageEditPlusPipeline.from_pretrained(
        "Qwen/Qwen-Image-Edit-2511",
        torch_dtype=torch.bfloat16,
    )
    
    # 2. GPUë¡œ ì´ë™
    pipeline.to('cuda')
    
    # 3. ë©”ëª¨ë¦¬ ìµœì í™” (Windows í˜¸í™˜)
    pipeline.enable_attention_slicing("auto")
    pipeline.enable_vae_slicing()
    
    # 4. xFormers ì‚¬ìš© ì‹œë„ (ì„¤ì¹˜ëœ ê²½ìš°)
    try:
        pipeline.enable_xformers_memory_efficient_attention()
        print("âœ… xFormers í™œì„±í™”")
    except Exception as e:
        print(f"âš ï¸ xFormers ì‚¬ìš© ë¶ˆê°€: {e}")
        print("   â†’ Attention Slicingìœ¼ë¡œ ëŒ€ì²´ë¨")
    
    # âŒ Windowsì—ì„œ ì‚¬ìš© ë¶ˆê°€:
    # - Flash Attention 2
    # - torch.compile (ì œí•œì )
    
    return pipeline

def edit_image_optimized(pipeline, image_path, prompt, seed=42):
    """ìµœì í™”ëœ ì´ë¯¸ì§€ í¸ì§‘"""
    
    image = Image.open(image_path)
    
    inputs = {
        "image": [image],
        "prompt": prompt,
        "generator": torch.manual_seed(seed),
        "true_cfg_scale": 4.0,
        "negative_prompt": " ",
        "num_inference_steps": 30,  # í’ˆì§ˆê³¼ ì†ë„ì˜ ê· í˜•
        "guidance_scale": 1.0,
        "num_images_per_prompt": 1,
    }
    
    with torch.inference_mode():
        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
            output = pipeline(**inputs)
    
    return output.images[0]

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    pipeline = create_optimized_pipeline_windows()
    result = edit_image_optimized(
        pipeline,
        "input.png",
        "Change the background to outer space"
    )
    result.save("output.png")
    print("âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ!")
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµí‘œ (Windows)

| ìµœì í™” ë°©ë²• | VRAM ì‚¬ìš©ëŸ‰ | ì†ë„ ì˜í–¥ | í’ˆì§ˆ ì˜í–¥ | Windows ì§€ì› |
|-------------|-------------|-----------|-----------|--------------|
| ê¸°ë³¸ (BF16) | ~24GB | ê¸°ì¤€ | ê¸°ì¤€ | âœ… |
| CPU Offload | ~16GB | -30% | ì—†ìŒ | âœ… |
| Sequential Offload | ~8GB | -60% | ì—†ìŒ | âœ… |
| Attention Slicing | ~20GB | -10% | ì—†ìŒ | âœ… |
| VAE Slicing | ~22GB | -5% | ì—†ìŒ | âœ… |
| VAE Tiling | ~20GB | -15% | ì—†ìŒ | âœ… |
| NF4 ì–‘ìí™” (ovedrive) | ~16GB | -10% | ì•½ê°„ ì €í•˜ | âœ… |
| xFormers | ~22GB | +20% | ì—†ìŒ | âš ï¸ ì„¤ì¹˜ ì£¼ì˜ |
| Flash Attention 2 | ~20GB | +30% | ì—†ìŒ | âŒ ë¯¸ì§€ì› |
| torch.compile | - | +15~40% | ì—†ìŒ | âš ï¸ ì œí•œì  |

> ğŸ’¡ **Windows ê¶Œì¥ ì¡°í•©**: CPU Offload + Attention Slicing + VAE Slicing

---

## LoRA í™œìš© ê°€ì´ë“œ

### ë‚´ì¥ LoRA ê¸°ëŠ¥

Qwen-Image-Edit-2511ì—ëŠ” ì¸ê¸° ìˆëŠ” ì»¤ë®¤ë‹ˆí‹° LoRAê°€ ê¸°ë³¸ í†µí•©ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

#### ì¡°ëª… í–¥ìƒ (Lighting Enhancement)

```python
# ì¡°ëª… ê´€ë ¨ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ ì ìš©ë¨
prompt = "Add dramatic golden lighting to this photo"
prompt = "Add dramatic rim lighting to the subject"
prompt = "Studio lighting setup with soft boxes"
```

#### ë·°í¬ì¸íŠ¸ ë³€ê²½ (Viewpoint Generation)

```python
# ë‹¤ë¥¸ ê°ë„ì—ì„œ ë³¸ ì´ë¯¸ì§€ ìƒì„±
prompt = "Show this same scene from a bird's eye view"
prompt = "Show this scene from a bird's eye view"
prompt = "Render this object from a 45-degree angle"
```

### ì»¤ìŠ¤í…€ LoRA ì ìš©

```python
from diffusers import QwenImageEditPlusPipeline
import torch

pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.to('cuda')

# LoRA ê°€ì¤‘ì¹˜ ë¡œë“œ
pipeline.load_lora_weights(
    "path/to/lora",
    weight_name="custom_lora.safetensors"
)

# LoRA ìŠ¤ì¼€ì¼ ì¡°ì • (0.0 ~ 1.0)
pipeline.fuse_lora(lora_scale=0.8)

# ì´ë¯¸ì§€ í¸ì§‘ ìˆ˜í–‰
# ...
```

### ì¸ê¸° ìˆëŠ” ì»¤ë®¤ë‹ˆí‹° LoRA ëª©ë¡

| LoRA ì´ë¦„ | ìš©ë„ | ì¶œì²˜ |
|-----------|------|------|
| Lighting Enhancement | ì¡°ëª… ì œì–´ | ë‚´ì¥ |
| Viewpoint Generation | ì‹œì  ë³€ê²½ | ë‚´ì¥ |
| Anime Style | ì• ë‹ˆë©”ì´ì…˜ ìŠ¤íƒ€ì¼ | CivitAI |
| Realistic Skin | í”¼ë¶€ ì§ˆê° ê°œì„  | CivitAI |
| Product Photography | ì œí’ˆ ì‚¬ì§„ | HuggingFace |

---

## ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ë° í•´ê²° ë°©ë²•

#### 1. CUDA Out of Memory

```
RuntimeError: CUDA out of memory
```

**í•´ê²° ë°©ë²•:**

```python
# ë°©ë²• 1: CPU ì˜¤í”„ë¡œë”©
pipeline.enable_model_cpu_offload()

# ë°©ë²• 2: ë©”ëª¨ë¦¬ ì •ë¦¬
import torch
torch.cuda.empty_cache()

# ë°©ë²• 3: ë” ì‘ì€ ì´ë¯¸ì§€ ì‚¬ìš©
from PIL import Image
image = Image.open("large_image.png")
image = image.resize((512, 512))
```

#### 2. ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨

```
OSError: Can't load the model
```

**í•´ê²° ë°©ë²•:**

```powershell
# diffusers ìµœì‹  ë²„ì „ ì„¤ì¹˜
pip install --upgrade git+https://github.com/huggingface/diffusers

# Windowsì—ì„œ ìºì‹œ ì •ë¦¬ (PowerShell)
Remove-Item -Recurse -Force "$env:USERPROFILE\.cache\huggingface\hub\models--Qwen--Qwen-Image-Edit-2511"

# ë˜ëŠ” ëª…ë ¹ í”„ë¡¬í”„íŠ¸(CMD)ì—ì„œ
rmdir /s /q "%USERPROFILE%\.cache\huggingface\hub\models--Qwen--Qwen-Image-Edit-2511"
```

#### 3. í”„ë¡¬í”„íŠ¸ê°€ ì œëŒ€ë¡œ ë°˜ì˜ë˜ì§€ ì•ŠìŒ

**í•´ê²° ë°©ë²•:**

```python
# true_cfg_scale ê°’ ì¡°ì •
inputs["true_cfg_scale"] = 5.0  # ê¸°ë³¸ê°’ 4.0ì—ì„œ ì¦ê°€

# ë” êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
# Bad: "change color"
# Good: "Change the background color to light sky blue (#87CEEB) while keeping the person intact"
```

#### 4. ì´ë¯¸ì§€ í’ˆì§ˆ ì €í•˜

**í•´ê²° ë°©ë²•:**

```python
# ì¶”ë¡  ìŠ¤í… ì¦ê°€
inputs["num_inference_steps"] = 50  # ê¸°ë³¸ê°’ 40ì—ì„œ ì¦ê°€

# negative prompt ì¶”ê°€
inputs["negative_prompt"] = "blurry, low quality, pixelated, noisy, artifacts"
```

#### 5. Windowsì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œ

**ê¸´ ê²½ë¡œ ì˜¤ë¥˜ (Long Path):**

```powershell
# LongPathsEnabled ì„¤ì • (ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ PowerShell ì‹¤í–‰ í›„)
New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" -Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force
```

**DLL ë¡œë“œ ì˜¤ë¥˜:**

```
OSError: [WinError 126] ì§€ì •ëœ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤
```

```powershell
# Visual C++ Redistributable ì„¤ì¹˜ í•„ìš”
# https://aka.ms/vs/17/release/vc_redist.x64.exe ë‹¤ìš´ë¡œë“œ í›„ ì„¤ì¹˜

# ë˜ëŠ” wingetìœ¼ë¡œ ì„¤ì¹˜
winget install Microsoft.VCRedist.2015+.x64
```

**torch.compile ì˜¤ë¥˜:**

```
Triton not found, skipping compilation
```

```python
# Windowsì—ì„œëŠ” torch.compile ìƒëµ
# pipeline.transformer = torch.compile(...)  # ì£¼ì„ ì²˜ë¦¬

# ëŒ€ì‹  ë‹¤ë¥¸ ìµœì í™” ë°©ë²• ì‚¬ìš©
pipeline.enable_attention_slicing("auto")
```

#### 6. Flash Attention 2 ê´€ë ¨ ì˜¤ë¥˜

```
RuntimeError: FlashAttention only supports NVIDIA GPUs or ROCm
ModuleNotFoundError: No module named 'flash_attn'
```

**í•´ê²° ë°©ë²•:**

```python
# Flash Attention 2ëŠ” Windowsì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# use_flash_attention_2=True ì˜µì…˜ì„ ì œê±°í•˜ì„¸ìš”.

# âŒ ì˜ëª»ëœ ì½”ë“œ (Windows)
# pipeline = QwenImageEditPlusPipeline.from_pretrained(
#     "Qwen/Qwen-Image-Edit-2511",
#     use_flash_attention_2=True  # Windowsì—ì„œ ì˜¤ë¥˜ ë°œìƒ
# )

# âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ (Windows)
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "Qwen/Qwen-Image-Edit-2511",
    torch_dtype=torch.bfloat16
)
pipeline.enable_attention_slicing("auto")  # ëŒ€ì•ˆ
```

#### 7. bitsandbytes ì˜¤ë¥˜ (Windows)

```
RuntimeError: CUDA Setup failed despite GPU being available
```

**í•´ê²° ë°©ë²•:**

```powershell
# bitsandbytes ì¬ì„¤ì¹˜
pip uninstall bitsandbytes
pip install bitsandbytes>=0.43.0

# ê·¸ë˜ë„ ì•ˆ ë˜ë©´ â†’ ì‚¬ì „ ì–‘ìí™” ëª¨ë¸ ì‚¬ìš© (ê¶Œì¥)
```

**ê¶Œì¥ ëŒ€ì•ˆ - ovedrive NF4 ëª¨ë¸ ì‚¬ìš©:**

Windowsì—ì„œ ì–‘ìí™” ë¬¸ì œê°€ ê³„ì†ëœë‹¤ë©´ [ovedrive/Qwen-Image-Edit-2511-4bit](https://huggingface.co/ovedrive/Qwen-Image-Edit-2511-4bit)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì´ ëª¨ë¸ì€ ì´ë¯¸ ì–‘ìí™”ë˜ì–´ ìˆì–´ bitsandbytes ì„¤ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

```python
from diffusers import QwenImageEditPlusPipeline
import torch

# ì‚¬ì „ ì–‘ìí™”ëœ ëª¨ë¸ - bitsandbytes ë¶ˆí•„ìš”
pipeline = QwenImageEditPlusPipeline.from_pretrained(
    "ovedrive/Qwen-Image-Edit-2511-4bit",
    torch_dtype=torch.bfloat16
)
pipeline.enable_model_cpu_offload()  # 16GB VRAM
```

| GPU VRAM | ê¶Œì¥ ì„¤ì • |
|----------|-----------|
| 20GB+ | `pipeline.to("cuda")` |
| 16-20GB | `pipeline.enable_model_cpu_offload()` |

---

## ë¼ì´ì„ ìŠ¤ ë° ì¸ìš©

### ë¼ì´ì„ ìŠ¤

Qwen-Image-Edit-2511ì€ **Apache 2.0** ë¼ì´ì„ ìŠ¤ í•˜ì— ì œê³µë©ë‹ˆë‹¤.

- âœ… ìƒì—…ì  ì‚¬ìš© ê°€ëŠ¥
- âœ… ìˆ˜ì • ë° ë°°í¬ ê°€ëŠ¥
- âœ… íŠ¹í—ˆê¶Œ ë¶€ì—¬
- âš ï¸ ë¼ì´ì„ ìŠ¤ ë° ì €ì‘ê¶Œ ê³ ì§€ í•„ìˆ˜
- âš ï¸ ë³´ì¦ ì—†ìŒ

### ì¸ìš© (Citation)

ì—°êµ¬ë‚˜ í”„ë¡œì íŠ¸ì—ì„œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš°, ë‹¤ìŒê³¼ ê°™ì´ ì¸ìš©í•´ ì£¼ì„¸ìš”:

```bibtex
@misc{wu2025qwenimagetechnicalreport,
      title={Qwen-Image Technical Report}, 
      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and 
              Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and 
              Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and 
              Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and 
              Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and 
              Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and 
              Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and 
              Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and 
              Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and 
              Zenan Liu},
      year={2025},
      eprint={2508.02324},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.02324}, 
}
```

---

## ì°¸ê³  ìë£Œ

### ê³µì‹ ë§í¬

| ë¦¬ì†ŒìŠ¤ | URL |
|--------|-----|
| ğŸ¤— Hugging Face | https://huggingface.co/Qwen/Qwen-Image-Edit-2511 |
| ğŸ“„ ê¸°ìˆ  ë³´ê³ ì„œ (arXiv) | https://arxiv.org/abs/2508.02324 |
| ğŸ’¬ Qwen Chat | https://chat.qwen.ai |
| ğŸ–¥ï¸ ì˜¨ë¼ì¸ ë°ëª¨ | https://huggingface.co/spaces/Qwen/Qwen-Image-Edit-2511 |
| ğŸ™ GitHub | https://github.com/QwenLM |

### ì»¤ë®¤ë‹ˆí‹° ë¦¬ì†ŒìŠ¤

| ë¦¬ì†ŒìŠ¤ | ì„¤ëª… | URL |
|--------|------|-----|
| **NF4 ì–‘ìí™” (ovedrive)** | 16GB VRAMìš© NF4 ì–‘ìí™” ëª¨ë¸ (ê¶Œì¥) | [ovedrive/Qwen-Image-Edit-2511-4bit](https://huggingface.co/ovedrive/Qwen-Image-Edit-2511-4bit) |
| Fast ë²„ì „ | ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•œ ìµœì í™” ë²„ì „ | [linoyts/Qwen-Image-Edit-2511-Fast](https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-2511-Fast) |
| Lightning ë²„ì „ | 4-step ì¶”ë¡  ë²„ì „ | [akhaliq/Qwen-Image-Edit-2511-Lightning](https://huggingface.co/spaces/akhaliq/Qwen-Image-Edit-2511-Lightning) |
| AnyPose ë²„ì „ | í¬ì¦ˆ ì œì–´ ê¸°ëŠ¥ ì¶”ê°€ | [linoyts/Qwen-Image-Edit-2511-AnyPose](https://huggingface.co/spaces/linoyts/Qwen-Image-Edit-2511-AnyPose) |
| RunComfy API | API í†µí•© | [runcomfy.com](https://www.runcomfy.com/models/qwen/qwen-image/qwen-image-edit-2511) |
| WaveSpeed API | ë¹ ë¥¸ ì¶”ë¡  API | [wavespeed.ai](https://wavespeed.ai/models/wavespeed-ai/qwen-image/edit-2511) |

### ê´€ë ¨ ë¬¸ì„œ

- [Diffusers Documentation](https://huggingface.co/docs/diffusers)
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [Hugging Face Hub](https://huggingface.co/docs/hub)

---

## ë²„ì „ ì´ë ¥

| ë²„ì „ | ë‚ ì§œ | ì£¼ìš” ë³€ê²½ ì‚¬í•­ |
|------|------|----------------|
| 2511 | 2025ë…„ 11ì›” | ìºë¦­í„° ì¼ê´€ì„± í–¥ìƒ, LoRA í†µí•©, ê¸°í•˜í•™ì  ì¶”ë¡  ê°•í™” |
| 2509 | 2025ë…„ 9ì›” | ì´ˆê¸° ë¦´ë¦¬ìŠ¤ |

---

<div align="center">

**Made with â¤ï¸ by Qwen Team**

[Hugging Face](https://huggingface.co/Qwen) â€¢ [GitHub](https://github.com/QwenLM) â€¢ [Discord](https://discord.gg/qwen)

</div>

